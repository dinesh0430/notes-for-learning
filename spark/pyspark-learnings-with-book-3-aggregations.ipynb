{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8669894,"sourceType":"datasetVersion","datasetId":5195760}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n\nfrom pyspark.sql import SparkSession\n# Create a SparkSession (without a specified name)\nspark = SparkSession.builder.getOrCreate()\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True) #for simple calls and better display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-28T12:37:39.020210Z","iopub.execute_input":"2024-07-28T12:37:39.020824Z","iopub.status.idle":"2024-07-28T12:38:37.084237Z","shell.execute_reply.started":"2024-07-28T12:37:39.020758Z","shell.execute_reply":"2024-07-28T12:38:37.082867Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=c0aa904e3e80738adfde4281b6d8494c539914c917526d0a55b6252dc0db3e0a\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\n","output_type":"stream"},{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/07/28 12:38:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"processed_folder_path = '/kaggle/input/formula1-processed-hope'\n\n\ndrivers_df = spark.read.parquet(f\"{processed_folder_path}/drivers\")\n# constructors_df = spark.read.parquet(f\"{processed_folder_path}/constructors\") \n# circuits_df = spark.read.parquet(f\"{processed_folder_path}/circuits\") \n# races_df = spark.read.parquet(f\"{processed_folder_path}/races\") \n# results_df = spark.read.parquet(f\"{processed_folder_path}/results\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:37.087294Z","iopub.execute_input":"2024-07-28T12:38:37.088105Z","iopub.status.idle":"2024-07-28T12:38:40.135474Z","shell.execute_reply.started":"2024-07-28T12:38:37.088056Z","shell.execute_reply":"2024-07-28T12:38:40.133975Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"drivers_df.schema","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:40.140145Z","iopub.execute_input":"2024-07-28T12:38:40.140592Z","iopub.status.idle":"2024-07-28T12:38:40.188879Z","shell.execute_reply.started":"2024-07-28T12:38:40.140545Z","shell.execute_reply":"2024-07-28T12:38:40.187595Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"StructType([StructField('driver_id', IntegerType(), True), StructField('driver_ref', StringType(), True), StructField('number', IntegerType(), True), StructField('code', StringType(), True), StructField('name', StringType(), True), StructField('dob', DateType(), True), StructField('nationality', StringType(), True), StructField('ingestion_date', TimestampType(), True)])"},"metadata":{}}]},{"cell_type":"code","source":"df = drivers_df","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:40.191895Z","iopub.execute_input":"2024-07-28T12:38:40.192945Z","iopub.status.idle":"2024-07-28T12:38:40.197722Z","shell.execute_reply.started":"2024-07-28T12:38:40.192901Z","shell.execute_reply":"2024-07-28T12:38:40.196440Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import count\n# df.selectExpr(\"count(*)\")\ndf.selectExpr(\"count(number)\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:40.199841Z","iopub.execute_input":"2024-07-28T12:38:40.200275Z","iopub.status.idle":"2024-07-28T12:38:44.715932Z","shell.execute_reply.started":"2024-07-28T12:38:40.200240Z","shell.execute_reply":"2024-07-28T12:38:44.714573Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"+-------------+\n|count(number)|\n+-------------+\n|           47|\n+-------------+","text/html":"<table border='1'>\n<tr><th>count(number)</th></tr>\n<tr><td>47</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import countDistinct\ndf.select(countDistinct(\"number\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:44.717249Z","iopub.execute_input":"2024-07-28T12:38:44.717992Z","iopub.status.idle":"2024-07-28T12:38:46.134012Z","shell.execute_reply.started":"2024-07-28T12:38:44.717950Z","shell.execute_reply":"2024-07-28T12:38:46.132836Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"+----------------------+\n|count(DISTINCT number)|\n+----------------------+\n|                    41|\n+----------------------+","text/html":"<table border='1'>\n<tr><th>count(DISTINCT number)</th></tr>\n<tr><td>41</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import first,last\ndf.select(first(\"name\"),last(\"name\")) # first and last values from the dataframe, not first and last by sorting","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:46.135286Z","iopub.execute_input":"2024-07-28T12:38:46.135728Z","iopub.status.idle":"2024-07-28T12:38:46.894207Z","shell.execute_reply.started":"2024-07-28T12:38:46.135688Z","shell.execute_reply":"2024-07-28T12:38:46.893107Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"+--------------+---------------+\n|   first(name)|     last(name)|\n+--------------+---------------+\n|Lewis Hamilton|Mick Schumacher|\n+--------------+---------------+","text/html":"<table border='1'>\n<tr><th>first(name)</th><th>last(name)</th></tr>\n<tr><td>Lewis Hamilton</td><td>Mick Schumacher</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import max,min\ndf.select(min(\"dob\"),max(\"dob\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:46.895339Z","iopub.execute_input":"2024-07-28T12:38:46.895755Z","iopub.status.idle":"2024-07-28T12:38:47.584258Z","shell.execute_reply.started":"2024-07-28T12:38:46.895718Z","shell.execute_reply":"2024-07-28T12:38:47.583012Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"+----------+----------+\n|  min(dob)|  max(dob)|\n+----------+----------+\n|1896-12-28|2000-05-11|\n+----------+----------+","text/html":"<table border='1'>\n<tr><th>min(dob)</th><th>max(dob)</th></tr>\n<tr><td>1896-12-28</td><td>2000-05-11</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import sum\ndf.select(sum(\"driver_id\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:47.585588Z","iopub.execute_input":"2024-07-28T12:38:47.588365Z","iopub.status.idle":"2024-07-28T12:38:48.126428Z","shell.execute_reply.started":"2024-07-28T12:38:47.588320Z","shell.execute_reply":"2024-07-28T12:38:48.125108Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"+--------------+\n|sum(driver_id)|\n+--------------+\n|        364276|\n+--------------+","text/html":"<table border='1'>\n<tr><th>sum(driver_id)</th></tr>\n<tr><td>364276</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"collect_set ensures no duplicates or it ensures uniqueness and it doesn't ensure order, collect_list will contain duplicates as well, it ensures the physical order","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import collect_list, collect_set\ndf.filter(\"nationality IN ('Indian','Belgian','Spanish')\").select(collect_set(\"nationality\")).show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:48.131288Z","iopub.execute_input":"2024-07-28T12:38:48.131811Z","iopub.status.idle":"2024-07-28T12:38:48.719896Z","shell.execute_reply.started":"2024-07-28T12:38:48.131759Z","shell.execute_reply":"2024-07-28T12:38:48.718624Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"+--------------------------+\n|collect_set(nationality)  |\n+--------------------------+\n|[Indian, Spanish, Belgian]|\n+--------------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import collect_list, collect_set\ndf.filter(\"nationality IN ('Indian','Belgian','Spanish')\").select(collect_list(\"nationality\")).show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:48.721431Z","iopub.execute_input":"2024-07-28T12:38:48.721919Z","iopub.status.idle":"2024-07-28T12:38:48.990801Z","shell.execute_reply.started":"2024-07-28T12:38:48.721875Z","shell.execute_reply":"2024-07-28T12:38:48.989504Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|collect_list(nationality)                                                                                                                                                                                                                                                                                                                                             |\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|[Spanish, Spanish, Indian, Spanish, Belgian, Belgian, Belgian, Belgian, Spanish, Spanish, Spanish, Spanish, Belgian, Belgian, Belgian, Belgian, Spanish, Spanish, Belgian, Belgian, Belgian, Belgian, Spanish, Belgian, Spanish, Belgian, Spanish, Belgian, Belgian, Belgian, Belgian, Belgian, Belgian, Belgian, Spanish, Indian, Belgian, Spanish, Spanish, Belgian]|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import desc,year\ndf.groupBy(\"nationality\",year(\"dob\")).count().orderBy(desc(\"count\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:48.992074Z","iopub.execute_input":"2024-07-28T12:38:48.992497Z","iopub.status.idle":"2024-07-28T12:38:50.224234Z","shell.execute_reply.started":"2024-07-28T12:38:48.992463Z","shell.execute_reply":"2024-07-28T12:38:50.223066Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"+-----------+---------+-----+\n|nationality|year(dob)|count|\n+-----------+---------+-----+\n|   American|     1926|   13|\n|   American|     1927|    8|\n|   American|     1918|    8|\n|   American|     1920|    8|\n|   American|     1925|    7|\n|   American|     1928|    7|\n|    British|     1931|    7|\n|   American|     1919|    7|\n|    British|     1942|    6|\n|    British|     1929|    6|\n|   American|     1913|    6|\n|    British|     1933|    6|\n|   American|     1921|    6|\n|    British|     1936|    6|\n|    British|     1930|    6|\n|   American|     1914|    5|\n|   American|     1923|    5|\n|   American|     1932|    5|\n|    British|     1934|    5|\n|    British|     1932|    5|\n+-----------+---------+-----+\nonly showing top 20 rows","text/html":"<table border='1'>\n<tr><th>nationality</th><th>year(dob)</th><th>count</th></tr>\n<tr><td>American</td><td>1926</td><td>13</td></tr>\n<tr><td>American</td><td>1927</td><td>8</td></tr>\n<tr><td>American</td><td>1918</td><td>8</td></tr>\n<tr><td>American</td><td>1920</td><td>8</td></tr>\n<tr><td>American</td><td>1925</td><td>7</td></tr>\n<tr><td>American</td><td>1928</td><td>7</td></tr>\n<tr><td>British</td><td>1931</td><td>7</td></tr>\n<tr><td>American</td><td>1919</td><td>7</td></tr>\n<tr><td>British</td><td>1942</td><td>6</td></tr>\n<tr><td>British</td><td>1929</td><td>6</td></tr>\n<tr><td>American</td><td>1913</td><td>6</td></tr>\n<tr><td>British</td><td>1933</td><td>6</td></tr>\n<tr><td>American</td><td>1921</td><td>6</td></tr>\n<tr><td>British</td><td>1936</td><td>6</td></tr>\n<tr><td>British</td><td>1930</td><td>6</td></tr>\n<tr><td>American</td><td>1914</td><td>5</td></tr>\n<tr><td>American</td><td>1923</td><td>5</td></tr>\n<tr><td>American</td><td>1932</td><td>5</td></tr>\n<tr><td>British</td><td>1934</td><td>5</td></tr>\n<tr><td>British</td><td>1932</td><td>5</td></tr>\n</table>\nonly showing top 20 rows\n"},"metadata":{}}]},{"cell_type":"markdown","source":"Grouping data using agg for better control on the data","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:38:50.225457Z","iopub.execute_input":"2024-07-28T12:38:50.225904Z","iopub.status.idle":"2024-07-28T12:38:50.243316Z","shell.execute_reply.started":"2024-07-28T12:38:50.225866Z","shell.execute_reply":"2024-07-28T12:38:50.239383Z"}}},{"cell_type":"code","source":"from pyspark.sql.functions import expr\n#both give the same output\n# df.groupBy(\"nationality\").agg(count(\"driver_id\").alias(\"count_driver\")).orderBy(desc(\"count_driver\"))\ndf.groupBy(\"nationality\").agg(expr(\"count(driver_id)\").alias(\"count_driver\")).orderBy(desc(\"count_driver\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:43:43.005075Z","iopub.execute_input":"2024-07-28T12:43:43.005876Z","iopub.status.idle":"2024-07-28T12:43:43.757861Z","shell.execute_reply.started":"2024-07-28T12:43:43.005833Z","shell.execute_reply":"2024-07-28T12:43:43.756810Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"+-------------+------------+\n|  nationality|count_driver|\n+-------------+------------+\n|      British|         165|\n|     American|         157|\n|      Italian|          99|\n|       French|          73|\n|       German|          50|\n|    Brazilian|          32|\n|    Argentine|          24|\n|        Swiss|          23|\n|South African|          23|\n|      Belgian|          23|\n|     Japanese|          20|\n|   Australian|          17|\n|        Dutch|          17|\n|      Spanish|          15|\n|     Austrian|          15|\n|     Canadian|          14|\n|      Swedish|          10|\n|      Finnish|           9|\n|New Zealander|           9|\n|      Mexican|           6|\n+-------------+------------+\nonly showing top 20 rows","text/html":"<table border='1'>\n<tr><th>nationality</th><th>count_driver</th></tr>\n<tr><td>British</td><td>165</td></tr>\n<tr><td>American</td><td>157</td></tr>\n<tr><td>Italian</td><td>99</td></tr>\n<tr><td>French</td><td>73</td></tr>\n<tr><td>German</td><td>50</td></tr>\n<tr><td>Brazilian</td><td>32</td></tr>\n<tr><td>Argentine</td><td>24</td></tr>\n<tr><td>Swiss</td><td>23</td></tr>\n<tr><td>South African</td><td>23</td></tr>\n<tr><td>Belgian</td><td>23</td></tr>\n<tr><td>Japanese</td><td>20</td></tr>\n<tr><td>Australian</td><td>17</td></tr>\n<tr><td>Dutch</td><td>17</td></tr>\n<tr><td>Spanish</td><td>15</td></tr>\n<tr><td>Austrian</td><td>15</td></tr>\n<tr><td>Canadian</td><td>14</td></tr>\n<tr><td>Swedish</td><td>10</td></tr>\n<tr><td>Finnish</td><td>9</td></tr>\n<tr><td>New Zealander</td><td>9</td></tr>\n<tr><td>Mexican</td><td>6</td></tr>\n</table>\nonly showing top 20 rows\n"},"metadata":{}}]},{"cell_type":"code","source":"df.schema","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:43:43.759744Z","iopub.execute_input":"2024-07-28T12:43:43.760485Z","iopub.status.idle":"2024-07-28T12:43:43.767742Z","shell.execute_reply.started":"2024-07-28T12:43:43.760439Z","shell.execute_reply":"2024-07-28T12:43:43.766611Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"StructType([StructField('driver_id', IntegerType(), True), StructField('driver_ref', StringType(), True), StructField('number', IntegerType(), True), StructField('code', StringType(), True), StructField('name', StringType(), True), StructField('dob', DateType(), True), StructField('nationality', StringType(), True), StructField('ingestion_date', TimestampType(), True)])"},"metadata":{}}]},{"cell_type":"markdown","source":"Window functions","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc,asc,col,max,lag,lead,months_between\n\nwindowSpec = Window.partitionBy(col(\"nationality\")).orderBy(\"dob\")\n\ndf.select(\"name\",\"nationality\",\"dob\",lag(\"dob\",1).over(windowSpec).alias(\"prev\"),months_between(\"dob\",lag(\"dob\",1).over(windowSpec)).alias(\"months_btwn\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:43:43.769701Z","iopub.execute_input":"2024-07-28T12:43:43.770525Z","iopub.status.idle":"2024-07-28T12:43:44.759638Z","shell.execute_reply.started":"2024-07-28T12:43:43.770468Z","shell.execute_reply":"2024-07-28T12:43:44.758538Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"+---------------+-----------+----------+----------+-----------+\n|           name|nationality|       dob|      prev|months_btwn|\n+---------------+-----------+----------+----------+-----------+\n|    Chet Miller|   American|1902-07-19|      NULL|       NULL|\n|     Mauri Rose|   American|1906-05-26|1902-07-19|46.22580645|\n|  George Connor|   American|1906-08-16|1906-05-26| 2.67741935|\n|   Bill Holland|   American|1907-12-18|1906-08-16|16.06451613|\n|  Bill Cantrell|   American|1908-01-31|1907-12-18| 1.41935484|\n| Robert O'Brien|   American|1908-04-11|1908-01-31| 2.35483871|\n| Bill Schindler|   American|1909-03-06|1908-04-11|10.83870968|\n|  Jimmy Jackson|   American|1910-07-25|1909-03-06|16.61290323|\n|    Lee Wallard|   American|1910-09-07|1910-07-25| 1.41935484|\n|    Travis Webb|   American|1910-10-08|1910-09-07| 1.03225806|\n|   Carl Forberg|   American|1911-03-04|1910-10-08| 4.87096774|\n|     Len Duncan|   American|1911-07-25|1911-03-04| 4.67741935|\n|     Walt Brown|   American|1911-12-30|1911-07-25| 5.16129032|\n|  Joie Chitwood|   American|1912-04-14|1911-12-30| 3.48387097|\n|     Myron Fohr|   American|1912-06-17|1912-04-14| 2.09677419|\n|     Duke Nalon|   American|1913-03-02|1912-06-17| 8.51612903|\n|  Duke Dinsmore|   American|1913-04-10|1913-03-02| 1.25806452|\n|   Duane Carter|   American|1913-05-05|1913-04-10| 0.83870968|\n|    Henry Banks|   American|1913-06-14|1913-05-05| 1.29032258|\n|Fred Agabashian|   American|1913-08-21|1913-06-14| 2.22580645|\n+---------------+-----------+----------+----------+-----------+\nonly showing top 20 rows","text/html":"<table border='1'>\n<tr><th>name</th><th>nationality</th><th>dob</th><th>prev</th><th>months_btwn</th></tr>\n<tr><td>Chet Miller</td><td>American</td><td>1902-07-19</td><td>NULL</td><td>NULL</td></tr>\n<tr><td>Mauri Rose</td><td>American</td><td>1906-05-26</td><td>1902-07-19</td><td>46.22580645</td></tr>\n<tr><td>George Connor</td><td>American</td><td>1906-08-16</td><td>1906-05-26</td><td>2.67741935</td></tr>\n<tr><td>Bill Holland</td><td>American</td><td>1907-12-18</td><td>1906-08-16</td><td>16.06451613</td></tr>\n<tr><td>Bill Cantrell</td><td>American</td><td>1908-01-31</td><td>1907-12-18</td><td>1.41935484</td></tr>\n<tr><td>Robert O'Brien</td><td>American</td><td>1908-04-11</td><td>1908-01-31</td><td>2.35483871</td></tr>\n<tr><td>Bill Schindler</td><td>American</td><td>1909-03-06</td><td>1908-04-11</td><td>10.83870968</td></tr>\n<tr><td>Jimmy Jackson</td><td>American</td><td>1910-07-25</td><td>1909-03-06</td><td>16.61290323</td></tr>\n<tr><td>Lee Wallard</td><td>American</td><td>1910-09-07</td><td>1910-07-25</td><td>1.41935484</td></tr>\n<tr><td>Travis Webb</td><td>American</td><td>1910-10-08</td><td>1910-09-07</td><td>1.03225806</td></tr>\n<tr><td>Carl Forberg</td><td>American</td><td>1911-03-04</td><td>1910-10-08</td><td>4.87096774</td></tr>\n<tr><td>Len Duncan</td><td>American</td><td>1911-07-25</td><td>1911-03-04</td><td>4.67741935</td></tr>\n<tr><td>Walt Brown</td><td>American</td><td>1911-12-30</td><td>1911-07-25</td><td>5.16129032</td></tr>\n<tr><td>Joie Chitwood</td><td>American</td><td>1912-04-14</td><td>1911-12-30</td><td>3.48387097</td></tr>\n<tr><td>Myron Fohr</td><td>American</td><td>1912-06-17</td><td>1912-04-14</td><td>2.09677419</td></tr>\n<tr><td>Duke Nalon</td><td>American</td><td>1913-03-02</td><td>1912-06-17</td><td>8.51612903</td></tr>\n<tr><td>Duke Dinsmore</td><td>American</td><td>1913-04-10</td><td>1913-03-02</td><td>1.25806452</td></tr>\n<tr><td>Duane Carter</td><td>American</td><td>1913-05-05</td><td>1913-04-10</td><td>0.83870968</td></tr>\n<tr><td>Henry Banks</td><td>American</td><td>1913-06-14</td><td>1913-05-05</td><td>1.29032258</td></tr>\n<tr><td>Fred Agabashian</td><td>American</td><td>1913-08-21</td><td>1913-06-14</td><td>2.22580645</td></tr>\n</table>\nonly showing top 20 rows\n"},"metadata":{}}]},{"cell_type":"markdown","source":"### Rollup\n`Rollup` in PySpark creates subtotals and grand totals along a hierarchical grouping of columns, providing a detailed summary at different levels of granularity. Ex: (1,2) equals (1,2),(1),()\n\n### Cube\n`Cube` in PySpark generates multi-dimensional aggregates across all possible combinations of specified grouping columns, offering comprehensive summaries including all possible subtotals and grand totals. Ex: (1,2) equals (1,2),(1),(2),()","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import lit,desc\ndf.where(\"nationality = 'American' and year(dob) < 1910\").cube(\"nationality\",year(\"dob\")).agg(count(lit(1)).alias(\"cnt\"))\\\n.orderBy(\"nationality\",\"year(dob)\").show(500)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:46:36.233109Z","iopub.execute_input":"2024-07-28T12:46:36.233646Z","iopub.status.idle":"2024-07-28T12:46:36.599813Z","shell.execute_reply.started":"2024-07-28T12:46:36.233606Z","shell.execute_reply":"2024-07-28T12:46:36.598619Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"+-----------+---------+---+\n|nationality|year(dob)|cnt|\n+-----------+---------+---+\n|       NULL|     NULL|  7|\n|       NULL|     1902|  1|\n|       NULL|     1906|  2|\n|       NULL|     1907|  1|\n|       NULL|     1908|  2|\n|       NULL|     1909|  1|\n|   American|     NULL|  7|\n|   American|     1902|  1|\n|   American|     1906|  2|\n|   American|     1907|  1|\n|   American|     1908|  2|\n|   American|     1909|  1|\n+-----------+---------+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import lit,desc\ndf.where(\"nationality = 'American' and year(dob) < 1910\").rollup(\"nationality\",year(\"dob\")).agg(count(lit(1)).alias(\"cnt\"))\\\n.orderBy(\"nationality\",\"year(dob)\").show(500)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:46:36.602279Z","iopub.execute_input":"2024-07-28T12:46:36.602824Z","iopub.status.idle":"2024-07-28T12:46:36.891264Z","shell.execute_reply.started":"2024-07-28T12:46:36.602774Z","shell.execute_reply":"2024-07-28T12:46:36.890029Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"+-----------+---------+---+\n|nationality|year(dob)|cnt|\n+-----------+---------+---+\n|       NULL|     NULL|  7|\n|   American|     NULL|  7|\n|   American|     1902|  1|\n|   American|     1906|  2|\n|   American|     1907|  1|\n|   American|     1908|  2|\n|   American|     1909|  1|\n+-----------+---------+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import lit,desc,grouping_id\ndf.where(\"nationality = 'American' and year(dob) < 1910\").rollup(\"nationality\",year(\"dob\")).agg(grouping_id(),count(lit(1)).alias(\"cnt\"))\\\n.orderBy(\"nationality\",\"year(dob)\").show(500)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:46:36.892670Z","iopub.execute_input":"2024-07-28T12:46:36.893078Z","iopub.status.idle":"2024-07-28T12:46:37.260227Z","shell.execute_reply.started":"2024-07-28T12:46:36.893038Z","shell.execute_reply":"2024-07-28T12:46:37.258869Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"+-----------+---------+-------------+---+\n|nationality|year(dob)|grouping_id()|cnt|\n+-----------+---------+-------------+---+\n|       NULL|     NULL|            3|  7|\n|   American|     NULL|            1|  7|\n|   American|     1902|            0|  1|\n|   American|     1906|            0|  2|\n|   American|     1907|            0|  1|\n|   American|     1908|            0|  2|\n|   American|     1909|            0|  1|\n+-----------+---------+-------------+---+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import lit,desc,grouping_id\ndf.where(\"nationality = 'American' and year(dob) < 1910\").cube(\"nationality\",year(\"dob\")).agg(grouping_id(),count(lit(1)).alias(\"cnt\"))\\\n.orderBy(\"nationality\",\"year(dob)\").show(500)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T12:47:02.255079Z","iopub.execute_input":"2024-07-28T12:47:02.255536Z","iopub.status.idle":"2024-07-28T12:47:02.545132Z","shell.execute_reply.started":"2024-07-28T12:47:02.255479Z","shell.execute_reply":"2024-07-28T12:47:02.543777Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"+-----------+---------+-------------+---+\n|nationality|year(dob)|grouping_id()|cnt|\n+-----------+---------+-------------+---+\n|       NULL|     NULL|            3|  7|\n|       NULL|     1902|            2|  1|\n|       NULL|     1906|            2|  2|\n|       NULL|     1907|            2|  1|\n|       NULL|     1908|            2|  2|\n|       NULL|     1909|            2|  1|\n|   American|     NULL|            1|  7|\n|   American|     1902|            0|  1|\n|   American|     1906|            0|  2|\n|   American|     1907|            0|  1|\n|   American|     1908|            0|  2|\n|   American|     1909|            0|  1|\n+-----------+---------+-------------+---+\n\n","output_type":"stream"}]}]}