{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8669894,"sourceType":"datasetVersion","datasetId":5195760}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n\nfrom pyspark.sql import SparkSession\n# Create a SparkSession (without a specified name)\nspark = SparkSession.builder.getOrCreate()\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True) #for simple calls and better display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-23T02:35:49.087742Z","iopub.execute_input":"2024-07-23T02:35:49.088286Z","iopub.status.idle":"2024-07-23T02:36:56.007509Z","shell.execute_reply.started":"2024-07-23T02:35:49.088239Z","shell.execute_reply":"2024-07-23T02:36:56.005951Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=9840637063a31fda3d2bbc337a10e6eea1db5d037cb4f492189b3d695cf7c063\n  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.1\n","output_type":"stream"},{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/07/23 02:36:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"code","source":"processed_folder_path = '/kaggle/input/formula1-processed-hope'\n\n\ndrivers_df = spark.read.parquet(f\"{processed_folder_path}/drivers\")\n# constructors_df = spark.read.parquet(f\"{processed_folder_path}/constructors\") \n# circuits_df = spark.read.parquet(f\"{processed_folder_path}/circuits\") \n# races_df = spark.read.parquet(f\"{processed_folder_path}/races\") \n# results_df = spark.read.parquet(f\"{processed_folder_path}/results\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:36:56.010781Z","iopub.execute_input":"2024-07-23T02:36:56.011679Z","iopub.status.idle":"2024-07-23T02:36:59.416275Z","shell.execute_reply.started":"2024-07-23T02:36:56.011608Z","shell.execute_reply":"2024-07-23T02:36:59.414574Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"drivers_df.schema","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:36:59.417982Z","iopub.execute_input":"2024-07-23T02:36:59.418388Z","iopub.status.idle":"2024-07-23T02:36:59.463236Z","shell.execute_reply.started":"2024-07-23T02:36:59.418355Z","shell.execute_reply":"2024-07-23T02:36:59.462067Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"StructType([StructField('driver_id', IntegerType(), True), StructField('driver_ref', StringType(), True), StructField('number', IntegerType(), True), StructField('code', StringType(), True), StructField('name', StringType(), True), StructField('dob', DateType(), True), StructField('nationality', StringType(), True), StructField('ingestion_date', TimestampType(), True)])"},"metadata":{}}]},{"cell_type":"code","source":"df = drivers_df","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:36:59.466318Z","iopub.execute_input":"2024-07-23T02:36:59.466724Z","iopub.status.idle":"2024-07-23T02:36:59.473110Z","shell.execute_reply.started":"2024-07-23T02:36:59.466690Z","shell.execute_reply":"2024-07-23T02:36:59.470858Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql.functions import lit\ndf.select(lit(5)).limit(2)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:36:59.475176Z","iopub.execute_input":"2024-07-23T02:36:59.475618Z","iopub.status.idle":"2024-07-23T02:37:03.115091Z","shell.execute_reply.started":"2024-07-23T02:36:59.475579Z","shell.execute_reply":"2024-07-23T02:37:03.113904Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"+---+\n|  5|\n+---+\n|  5|\n|  5|\n+---+","text/html":"<table border='1'>\n<tr><th>5</th></tr>\n<tr><td>5</td></tr>\n<tr><td>5</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import col\n\n# Both the below statements give the same results\n\ndf.where(col(\"driver_id\")<7)\n# df.where(\"driver_id<7\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:03.116383Z","iopub.execute_input":"2024-07-23T02:37:03.116806Z","iopub.status.idle":"2024-07-23T02:37:04.507149Z","shell.execute_reply.started":"2024-07-23T02:37:03.116771Z","shell.execute_reply":"2024-07-23T02:37:04.506056Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"+---------+----------+------+----+-----------------+----------+-----------+--------------------+\n|driver_id|driver_ref|number|code|             name|       dob|nationality|      ingestion_date|\n+---------+----------+------+----+-----------------+----------+-----------+--------------------+\n|        1|  hamilton|    44| HAM|   Lewis Hamilton|1985-01-07|    British|2024-06-11 01:24:...|\n|        2|  heidfeld|  NULL| HEI|    Nick Heidfeld|1977-05-10|     German|2024-06-11 01:24:...|\n|        3|   rosberg|     6| ROS|     Nico Rosberg|1985-06-27|     German|2024-06-11 01:24:...|\n|        4|    alonso|    14| ALO|  Fernando Alonso|1981-07-29|    Spanish|2024-06-11 01:24:...|\n|        5|kovalainen|  NULL| KOV|Heikki Kovalainen|1981-10-19|    Finnish|2024-06-11 01:24:...|\n|        6|  nakajima|  NULL| NAK|  Kazuki Nakajima|1985-01-11|   Japanese|2024-06-11 01:24:...|\n+---------+----------+------+----+-----------------+----------+-----------+--------------------+","text/html":"<table border='1'>\n<tr><th>driver_id</th><th>driver_ref</th><th>number</th><th>code</th><th>name</th><th>dob</th><th>nationality</th><th>ingestion_date</th></tr>\n<tr><td>1</td><td>hamilton</td><td>44</td><td>HAM</td><td>Lewis Hamilton</td><td>1985-01-07</td><td>British</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>2</td><td>heidfeld</td><td>NULL</td><td>HEI</td><td>Nick Heidfeld</td><td>1977-05-10</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>3</td><td>rosberg</td><td>6</td><td>ROS</td><td>Nico Rosberg</td><td>1985-06-27</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>4</td><td>alonso</td><td>14</td><td>ALO</td><td>Fernando Alonso</td><td>1981-07-29</td><td>Spanish</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>5</td><td>kovalainen</td><td>NULL</td><td>KOV</td><td>Heikki Kovalainen</td><td>1981-10-19</td><td>Finnish</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>6</td><td>nakajima</td><td>NULL</td><td>NAK</td><td>Kazuki Nakajima</td><td>1985-01-11</td><td>Japanese</td><td>2024-06-11 01:24:...</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"df.where(\"nationality = 'German'\").limit(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:04.509103Z","iopub.execute_input":"2024-07-23T02:37:04.509522Z","iopub.status.idle":"2024-07-23T02:37:05.138375Z","shell.execute_reply.started":"2024-07-23T02:37:04.509487Z","shell.execute_reply":"2024-07-23T02:37:05.137238Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"+---------+----------+------+----+----------------+----------+-----------+--------------------+\n|driver_id|driver_ref|number|code|            name|       dob|nationality|      ingestion_date|\n+---------+----------+------+----+----------------+----------+-----------+--------------------+\n|        2|  heidfeld|  NULL| HEI|   Nick Heidfeld|1977-05-10|     German|2024-06-11 01:24:...|\n|        3|   rosberg|     6| ROS|    Nico Rosberg|1985-06-27|     German|2024-06-11 01:24:...|\n|       10|     glock|  NULL| GLO|      Timo Glock|1982-03-18|     German|2024-06-11 01:24:...|\n|       16|     sutil|    99| SUT|    Adrian Sutil|1983-01-11|     German|2024-06-11 01:24:...|\n|       20|    vettel|     5| VET|Sebastian Vettel|1987-07-03|     German|2024-06-11 01:24:...|\n+---------+----------+------+----+----------------+----------+-----------+--------------------+","text/html":"<table border='1'>\n<tr><th>driver_id</th><th>driver_ref</th><th>number</th><th>code</th><th>name</th><th>dob</th><th>nationality</th><th>ingestion_date</th></tr>\n<tr><td>2</td><td>heidfeld</td><td>NULL</td><td>HEI</td><td>Nick Heidfeld</td><td>1977-05-10</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>3</td><td>rosberg</td><td>6</td><td>ROS</td><td>Nico Rosberg</td><td>1985-06-27</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>10</td><td>glock</td><td>NULL</td><td>GLO</td><td>Timo Glock</td><td>1982-03-18</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>16</td><td>sutil</td><td>99</td><td>SUT</td><td>Adrian Sutil</td><td>1983-01-11</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>20</td><td>vettel</td><td>5</td><td>VET</td><td>Sebastian Vettel</td><td>1987-07-03</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"col(\"driver_id\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:05.139593Z","iopub.execute_input":"2024-07-23T02:37:05.140034Z","iopub.status.idle":"2024-07-23T02:37:05.164024Z","shell.execute_reply.started":"2024-07-23T02:37:05.139997Z","shell.execute_reply":"2024-07-23T02:37:05.162784Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Column<'driver_id'>"},"metadata":{}}]},{"cell_type":"code","source":"df.where(df.driver_id.isin(2,3))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:05.166539Z","iopub.execute_input":"2024-07-23T02:37:05.167091Z","iopub.status.idle":"2024-07-23T02:37:05.741275Z","shell.execute_reply.started":"2024-07-23T02:37:05.167056Z","shell.execute_reply":"2024-07-23T02:37:05.740108Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"+---------+----------+------+----+-------------+----------+-----------+--------------------+\n|driver_id|driver_ref|number|code|         name|       dob|nationality|      ingestion_date|\n+---------+----------+------+----+-------------+----------+-----------+--------------------+\n|        2|  heidfeld|  NULL| HEI|Nick Heidfeld|1977-05-10|     German|2024-06-11 01:24:...|\n|        3|   rosberg|     6| ROS| Nico Rosberg|1985-06-27|     German|2024-06-11 01:24:...|\n+---------+----------+------+----+-------------+----------+-----------+--------------------+","text/html":"<table border='1'>\n<tr><th>driver_id</th><th>driver_ref</th><th>number</th><th>code</th><th>name</th><th>dob</th><th>nationality</th><th>ingestion_date</th></tr>\n<tr><td>2</td><td>heidfeld</td><td>NULL</td><td>HEI</td><td>Nick Heidfeld</td><td>1977-05-10</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n<tr><td>3</td><td>rosberg</td><td>6</td><td>ROS</td><td>Nico Rosberg</td><td>1985-06-27</td><td>German</td><td>2024-06-11 01:24:...</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import expr\ndf.withColumn(\"fromBritain\",expr(\"nationality = 'British'\")).where(df.nationality.isin(\"British\",\"German\"))\\\n.select(\"driver_id\",\"name\",\"nationality\",\"fromBritain\").show(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:05.747987Z","iopub.execute_input":"2024-07-23T02:37:05.749172Z","iopub.status.idle":"2024-07-23T02:37:06.091410Z","shell.execute_reply.started":"2024-07-23T02:37:05.749126Z","shell.execute_reply":"2024-07-23T02:37:06.090191Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"+---------+--------------+-----------+-----------+\n|driver_id|          name|nationality|fromBritain|\n+---------+--------------+-----------+-----------+\n|        1|Lewis Hamilton|    British|       true|\n|        2| Nick Heidfeld|     German|      false|\n|        3|  Nico Rosberg|     German|      false|\n+---------+--------------+-----------+-----------+\nonly showing top 3 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import expr, pow\n\n#Both statements give the same result\n\nsquaredDriverID = pow(col(\"driver_id\"),2)\n# df.select(expr(\"driver_id\"),squaredDriverID.alias(\"DriverID_Squared\"))\ndf.select(\"driver_id\",squaredDriverID.alias(\"DriverID_Squared\")).show(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:06.092737Z","iopub.execute_input":"2024-07-23T02:37:06.093205Z","iopub.status.idle":"2024-07-23T02:37:06.349419Z","shell.execute_reply.started":"2024-07-23T02:37:06.093165Z","shell.execute_reply":"2024-07-23T02:37:06.348103Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"+---------+----------------+\n|driver_id|DriverID_Squared|\n+---------+----------------+\n|        1|             1.0|\n|        2|             4.0|\n|        3|             9.0|\n+---------+----------------+\nonly showing top 3 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import round,bround,lit\ndf.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:06.350766Z","iopub.execute_input":"2024-07-23T02:37:06.351223Z","iopub.status.idle":"2024-07-23T02:37:06.558201Z","shell.execute_reply.started":"2024-07-23T02:37:06.351181Z","shell.execute_reply":"2024-07-23T02:37:06.556892Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"+-------------+--------------+\n|round(2.5, 0)|bround(2.5, 0)|\n+-------------+--------------+\n|          3.0|           2.0|\n|          3.0|           2.0|\n+-------------+--------------+\nonly showing top 2 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import current_date, current_timestamp\n\ndateDF = spark.range(10).withColumn(\"today\",current_date()).withColumn(\"now\",current_timestamp())\ndateDF.show(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:06.559953Z","iopub.execute_input":"2024-07-23T02:37:06.560409Z","iopub.status.idle":"2024-07-23T02:37:06.947763Z","shell.execute_reply.started":"2024-07-23T02:37:06.560367Z","shell.execute_reply":"2024-07-23T02:37:06.946397Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"+---+----------+--------------------+\n| id|     today|                 now|\n+---+----------+--------------------+\n|  0|2024-07-23|2024-07-23 02:37:...|\n|  1|2024-07-23|2024-07-23 02:37:...|\n|  2|2024-07-23|2024-07-23 02:37:...|\n+---+----------+--------------------+\nonly showing top 3 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import date_add, date_sub\ndateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:06.949323Z","iopub.execute_input":"2024-07-23T02:37:06.949738Z","iopub.status.idle":"2024-07-23T02:37:07.148202Z","shell.execute_reply.started":"2024-07-23T02:37:06.949699Z","shell.execute_reply":"2024-07-23T02:37:07.146901Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"+------------------+------------------+\n|date_sub(today, 5)|date_add(today, 5)|\n+------------------+------------------+\n|        2024-07-18|        2024-07-28|\n+------------------+------------------+\nonly showing top 1 row\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import datediff, months_between, to_date\ndateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n.select(datediff(\"week_ago\",\"today\")).show(1)\n# .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)  #both work\n\ndateDF.select(\nto_date(lit(\"2016-01-01\")).alias(\"start\"),\nto_date(lit(\"2017-01-01\")).alias(\"end\"))\\\n.select(months_between(col(\"start\"), col(\"end\"))).show(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:07.149778Z","iopub.execute_input":"2024-07-23T02:37:07.150229Z","iopub.status.idle":"2024-07-23T02:37:07.548838Z","shell.execute_reply.started":"2024-07-23T02:37:07.150192Z","shell.execute_reply":"2024-07-23T02:37:07.547455Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"+-------------------------+\n|datediff(week_ago, today)|\n+-------------------------+\n|                       -7|\n+-------------------------+\nonly showing top 1 row\n\n+--------------------------------+\n|months_between(start, end, true)|\n+--------------------------------+\n|                           -12.0|\n+--------------------------------+\nonly showing top 1 row\n\n","output_type":"stream"}]},{"cell_type":"code","source":"dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:07.550358Z","iopub.execute_input":"2024-07-23T02:37:07.551592Z","iopub.status.idle":"2024-07-23T02:37:07.682213Z","shell.execute_reply.started":"2024-07-23T02:37:07.551550Z","shell.execute_reply":"2024-07-23T02:37:07.680989Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"+-------------------+-------------------+\n|to_date(2016-20-12)|to_date(2017-12-11)|\n+-------------------+-------------------+\n|               NULL|         2017-12-11|\n+-------------------+-------------------+\nonly showing top 1 row\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import to_date\n\ndateFormat = \"yyyy-dd-MM\"\n\ncleanDateDF = spark.range(1).select(\nto_date(lit(\"2016-12-11\"),dateFormat).alias(\"date\"),\nto_date(lit(\"2016-10-21\"),dateFormat).alias(\"date2\"), # this shows a NULL as there is no 20th Month, but it also doesn't throw any error\n)\n\ncleanDateDF.show(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:07.683582Z","iopub.execute_input":"2024-07-23T02:37:07.684979Z","iopub.status.idle":"2024-07-23T02:37:07.960895Z","shell.execute_reply.started":"2024-07-23T02:37:07.684922Z","shell.execute_reply":"2024-07-23T02:37:07.959500Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"+----------+-----+\n|      date|date2|\n+----------+-----+\n|2016-11-12| NULL|\n+----------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import to_timestamp\n\ncleanDateDF.select(to_timestamp(col(\"date\"),dateFormat)).show(1) # to_timestamp the dateFormat is necessary","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:07.962275Z","iopub.execute_input":"2024-07-23T02:37:07.962749Z","iopub.status.idle":"2024-07-23T02:37:08.174704Z","shell.execute_reply.started":"2024-07-23T02:37:07.962706Z","shell.execute_reply":"2024-07-23T02:37:08.173569Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"+------------------------------+\n|to_timestamp(date, yyyy-dd-MM)|\n+------------------------------+\n|           2016-11-12 00:00:00|\n+------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"cleanDateDF.filter(col(\"date\")<lit(\"2020-01-01\")).show(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:08.176013Z","iopub.execute_input":"2024-07-23T02:37:08.176414Z","iopub.status.idle":"2024-07-23T02:37:08.317286Z","shell.execute_reply.started":"2024-07-23T02:37:08.176378Z","shell.execute_reply":"2024-07-23T02:37:08.315799Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"+----------+-----+\n|      date|date2|\n+----------+-----+\n|2016-11-12| NULL|\n+----------+-----+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import coalesce\n\ncleanDateDF.select(coalesce(\"date2\",\"date\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:08.318594Z","iopub.execute_input":"2024-07-23T02:37:08.319090Z","iopub.status.idle":"2024-07-23T02:37:08.546975Z","shell.execute_reply.started":"2024-07-23T02:37:08.319045Z","shell.execute_reply":"2024-07-23T02:37:08.545850Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"+---------------------+\n|coalesce(date2, date)|\n+---------------------+\n|           2016-11-12|\n+---------------------+","text/html":"<table border='1'>\n<tr><th>coalesce(date2, date)</th></tr>\n<tr><td>2016-11-12</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import ifnull\ndf.select(ifnull(lit(None),lit(\"happy\"))).show(1)   #to give a NULL manually in python use 'None'","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:08.548190Z","iopub.execute_input":"2024-07-23T02:37:08.548595Z","iopub.status.idle":"2024-07-23T02:37:08.708038Z","shell.execute_reply.started":"2024-07-23T02:37:08.548560Z","shell.execute_reply":"2024-07-23T02:37:08.706854Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"+-------------------+\n|ifnull(NULL, happy)|\n+-------------------+\n|              happy|\n+-------------------+\nonly showing top 1 row\n\n","output_type":"stream"}]},{"cell_type":"code","source":"cleanDateDF.na.drop(\"all\") ","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:08.709332Z","iopub.execute_input":"2024-07-23T02:37:08.709774Z","iopub.status.idle":"2024-07-23T02:37:08.933143Z","shell.execute_reply.started":"2024-07-23T02:37:08.709734Z","shell.execute_reply":"2024-07-23T02:37:08.931728Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"+----------+-----+\n|      date|date2|\n+----------+-----+\n|2016-11-12| NULL|\n+----------+-----+","text/html":"<table border='1'>\n<tr><th>date</th><th>date2</th></tr>\n<tr><td>2016-11-12</td><td>NULL</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import struct\ncomplexDF = df.select(struct(\"number\",\"code\").alias(\"complexColumn\"))\n# complexDF.select(\"complexColumn.code\")\ncomplexDF.select(\"complexColumn.*\").show(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:08.935674Z","iopub.execute_input":"2024-07-23T02:37:08.936208Z","iopub.status.idle":"2024-07-23T02:37:09.335791Z","shell.execute_reply.started":"2024-07-23T02:37:08.936163Z","shell.execute_reply":"2024-07-23T02:37:09.334687Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"+------+----+\n|number|code|\n+------+----+\n|    44| HAM|\n|  NULL| HEI|\n|     6| ROS|\n+------+----+\nonly showing top 3 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import split\ndf.select(split(\"name\",\" \").alias(\"split_col\")).selectExpr(\"split_col[0]\").show(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:09.337122Z","iopub.execute_input":"2024-07-23T02:37:09.337542Z","iopub.status.idle":"2024-07-23T02:37:09.541023Z","shell.execute_reply.started":"2024-07-23T02:37:09.337508Z","shell.execute_reply":"2024-07-23T02:37:09.539336Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"+------------+\n|split_col[0]|\n+------------+\n|       Lewis|\n|        Nick|\n|        Nico|\n+------------+\nonly showing top 3 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import split,explode\n# df.select(split(\"name\",\" \").alias(\"split_col\")).selectExpr(\"split_col[0]\")\nsplit_df = df.select(\"driver_id\",split(\"name\",\" \").alias(\"split_col\"))\nsplit_df.select(\"driver_id\",explode(\"split_col\")).show(3)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:09.542591Z","iopub.execute_input":"2024-07-23T02:37:09.543082Z","iopub.status.idle":"2024-07-23T02:37:09.899029Z","shell.execute_reply.started":"2024-07-23T02:37:09.543036Z","shell.execute_reply":"2024-07-23T02:37:09.897805Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"+---------+--------+\n|driver_id|     col|\n+---------+--------+\n|        1|   Lewis|\n|        1|Hamilton|\n|        2|    Nick|\n+---------+--------+\nonly showing top 3 rows\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import create_map\n\n#Maps are created by using the map function and key-value pairs of columns. \n#You then can select them just like you might select from an array, but using the key value\n\nmap_df = df.select(\"driver_id\",\"code\",create_map(col(\"driver_id\"),col(\"name\")).alias(\"map_col\")).limit(3)\nmap_df.show()\ntype(map_df)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:09.900375Z","iopub.execute_input":"2024-07-23T02:37:09.900848Z","iopub.status.idle":"2024-07-23T02:37:10.151381Z","shell.execute_reply.started":"2024-07-23T02:37:09.900787Z","shell.execute_reply":"2024-07-23T02:37:10.150236Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"+---------+----+--------------------+\n|driver_id|code|             map_col|\n+---------+----+--------------------+\n|        1| HAM|{1 -> Lewis Hamil...|\n|        2| HEI|{2 -> Nick Heidfeld}|\n|        3| ROS| {3 -> Nico Rosberg}|\n+---------+----+--------------------+\n\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"pyspark.sql.dataframe.DataFrame"},"metadata":{}}]},{"cell_type":"code","source":"map_df.select(map_df.map_col[1])  #displays for all the 3 rows","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:10.153400Z","iopub.execute_input":"2024-07-23T02:37:10.153844Z","iopub.status.idle":"2024-07-23T02:37:10.450717Z","shell.execute_reply.started":"2024-07-23T02:37:10.153785Z","shell.execute_reply":"2024-07-23T02:37:10.449405Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"+--------------+\n|    map_col[1]|\n+--------------+\n|Lewis Hamilton|\n|          NULL|\n|          NULL|\n+--------------+","text/html":"<table border='1'>\n<tr><th>map_col[1]</th></tr>\n<tr><td>Lewis Hamilton</td></tr>\n<tr><td>NULL</td></tr>\n<tr><td>NULL</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"map_df.select(map_df.map_col[1]).where(\"map_col[1] is NOT NULL\") #filtering for only rows where it is NOT NULL","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:10.458864Z","iopub.execute_input":"2024-07-23T02:37:10.459993Z","iopub.status.idle":"2024-07-23T02:37:11.380721Z","shell.execute_reply.started":"2024-07-23T02:37:10.459938Z","shell.execute_reply":"2024-07-23T02:37:11.379542Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"+--------------+\n|    map_col[1]|\n+--------------+\n|Lewis Hamilton|\n+--------------+","text/html":"<table border='1'>\n<tr><th>map_col[1]</th></tr>\n<tr><td>Lewis Hamilton</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"map_df.select(\"code\",explode(\"map_col\"))  # we can also explode the map columns, the \"key\" and \"value\" column names are automatically assigned","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:11.381998Z","iopub.execute_input":"2024-07-23T02:37:11.383053Z","iopub.status.idle":"2024-07-23T02:37:11.922236Z","shell.execute_reply.started":"2024-07-23T02:37:11.383009Z","shell.execute_reply":"2024-07-23T02:37:11.920918Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"+----+---+--------------+\n|code|key|         value|\n+----+---+--------------+\n| HAM|  1|Lewis Hamilton|\n| HEI|  2| Nick Heidfeld|\n| ROS|  3|  Nico Rosberg|\n+----+---+--------------+","text/html":"<table border='1'>\n<tr><th>code</th><th>key</th><th>value</th></tr>\n<tr><td>HAM</td><td>1</td><td>Lewis Hamilton</td></tr>\n<tr><td>HEI</td><td>2</td><td>Nick Heidfeld</td></tr>\n<tr><td>ROS</td><td>3</td><td>Nico Rosberg</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"spark.range(1).selectExpr(\"1\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:11.924974Z","iopub.execute_input":"2024-07-23T02:37:11.925432Z","iopub.status.idle":"2024-07-23T02:37:12.156234Z","shell.execute_reply.started":"2024-07-23T02:37:11.925393Z","shell.execute_reply":"2024-07-23T02:37:12.154891Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"+---+\n|  1|\n+---+\n|  1|\n+---+","text/html":"<table border='1'>\n<tr><th>1</th></tr>\n<tr><td>1</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"jsonDF = spark.range(1).selectExpr(\"'col_text' as col_name\",\"\"\"\n'{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")\njsonDF.show(truncate=False) #to show the complete column without truncation","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:12.157766Z","iopub.execute_input":"2024-07-23T02:37:12.158274Z","iopub.status.idle":"2024-07-23T02:37:12.320980Z","shell.execute_reply.started":"2024-07-23T02:37:12.158234Z","shell.execute_reply":"2024-07-23T02:37:12.319764Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"+--------+-------------------------------------------+\n|col_name|jsonString                                 |\n+--------+-------------------------------------------+\n|col_text|{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}|\n+--------+-------------------------------------------+\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyspark.sql.functions import json_tuple, get_json_object\n\n# get_json_object to inline query a JSON object, be it a dictionary or array. You can use json_tuple if this object has only one level of nesting\n\n# jsonDF.select(get_json_object(\"jsonString\",\"$.myJSONKey.myJSONValue\")).show(truncate=False)\njsonDF\\\n    .select(\\\n            get_json_object(\"jsonString\",\"$.myJSONKey.myJSONValue[1]\").alias(\"col1\"),\n            json_tuple(\"jsonString\",\"myJSONKey\").alias(\"col2\")\\\n           )\\\n    .show(truncate=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:12.322261Z","iopub.execute_input":"2024-07-23T02:37:12.322723Z","iopub.status.idle":"2024-07-23T02:37:12.746896Z","shell.execute_reply.started":"2024-07-23T02:37:12.322675Z","shell.execute_reply":"2024-07-23T02:37:12.745709Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"+----+-----------------------+\n|col1|col2                   |\n+----+-----------------------+\n|2   |{\"myJSONValue\":[1,2,3]}|\n+----+-----------------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### User Defined functions","metadata":{}},{"cell_type":"code","source":"udfDF = spark.range(5).toDF(\"num\")\n\ndef power3(value):\n    return value**3\n\npower3(2)","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:12.748152Z","iopub.execute_input":"2024-07-23T02:37:12.748563Z","iopub.status.idle":"2024-07-23T02:37:12.798824Z","shell.execute_reply.started":"2024-07-23T02:37:12.748526Z","shell.execute_reply":"2024-07-23T02:37:12.797723Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"8"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import udf\n\npower3udf = udf(power3)\nudfDF.select(power3udf(\"num\"))","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:12.800746Z","iopub.execute_input":"2024-07-23T02:37:12.801268Z","iopub.status.idle":"2024-07-23T02:37:15.676674Z","shell.execute_reply.started":"2024-07-23T02:37:12.801228Z","shell.execute_reply":"2024-07-23T02:37:15.675520Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"+-----------+\n|power3(num)|\n+-----------+\n|          0|\n|          1|\n|          8|\n|         27|\n|         64|\n+-----------+","text/html":"<table border='1'>\n<tr><th>power3(num)</th></tr>\n<tr><td>0</td></tr>\n<tr><td>1</td></tr>\n<tr><td>8</td></tr>\n<tr><td>27</td></tr>\n<tr><td>64</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"#registering the udf to be used with SQL expressions, which was not possible previously\n\nfrom pyspark.sql.types import StringType\nspark.udf.register(\"power3udfSQL\",power3,StringType())\nudfDF.selectExpr(\"power3udfSQL(num)\")","metadata":{"execution":{"iopub.status.busy":"2024-07-23T02:37:15.678648Z","iopub.execute_input":"2024-07-23T02:37:15.679106Z","iopub.status.idle":"2024-07-23T02:37:17.366056Z","shell.execute_reply.started":"2024-07-23T02:37:15.679067Z","shell.execute_reply":"2024-07-23T02:37:17.364767Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"+-----------------+\n|power3udfSQL(num)|\n+-----------------+\n|                0|\n|                1|\n|                8|\n|               27|\n|               64|\n+-----------------+","text/html":"<table border='1'>\n<tr><th>power3udfSQL(num)</th></tr>\n<tr><td>0</td></tr>\n<tr><td>1</td></tr>\n<tr><td>8</td></tr>\n<tr><td>27</td></tr>\n<tr><td>64</td></tr>\n</table>\n"},"metadata":{}}]}]}