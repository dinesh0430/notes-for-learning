{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n\nfrom pyspark.sql import SparkSession\n# Create a SparkSession (without a specified name)\nspark = SparkSession.builder.getOrCreate()\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True) #for simple calls and better display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-10-10T02:45:23.090362Z","iopub.execute_input":"2024-10-10T02:45:23.090873Z","iopub.status.idle":"2024-10-10T02:46:27.945867Z","shell.execute_reply.started":"2024-10-10T02:45:23.090802Z","shell.execute_reply":"2024-10-10T02:46:27.944509Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=7246848b2a26e0708be21cef9f171b04d48639ddea401e896522ec1e38bf99aa\n  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.3\n","output_type":"stream"},{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/10/10 02:46:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Problem 1\n\nWrite a solution to find the ids of products that are both low fat and recyclable.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize a Spark session\nspark = SparkSession.builder.appName(\"ProductsDataFrame\").getOrCreate()\n\n# Define the schema for the Products table\nschema = StructType([\n    StructField(\"product_id\", IntegerType(), False),  # product_id as int\n    StructField(\"low_fats\", StringType(), False),     # low_fats as enum (treated as String)\n    StructField(\"recyclable\", StringType(), False)    # recyclable as enum (treated as String)\n])\n\n# Create the DataFrame using the sample data\ndata = [\n    (0, \"Y\", \"N\"),\n    (1, \"Y\", \"Y\"),\n    (2, \"N\", \"Y\"),\n    (3, \"Y\", \"Y\"),\n    (4, \"N\", \"N\")\n]\n\n# Create the DataFrame\nproducts_df = spark.createDataFrame(data, schema)\n\n# Show the original DataFrame (optional)\nproducts_df\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:27.950803Z","iopub.execute_input":"2024-10-10T02:46:27.951493Z","iopub.status.idle":"2024-10-10T02:46:36.779426Z","shell.execute_reply.started":"2024-10-10T02:46:27.951444Z","shell.execute_reply":"2024-10-10T02:46:36.778121Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"24/10/10 02:46:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"+----------+--------+----------+\n|product_id|low_fats|recyclable|\n+----------+--------+----------+\n|         0|       Y|         N|\n|         1|       Y|         Y|\n|         2|       N|         Y|\n|         3|       Y|         Y|\n|         4|       N|         N|\n+----------+--------+----------+","text/html":"<table border='1'>\n<tr><th>product_id</th><th>low_fats</th><th>recyclable</th></tr>\n<tr><td>0</td><td>Y</td><td>N</td></tr>\n<tr><td>1</td><td>Y</td><td>Y</td></tr>\n<tr><td>2</td><td>N</td><td>Y</td></tr>\n<tr><td>3</td><td>Y</td><td>Y</td></tr>\n<tr><td>4</td><td>N</td><td>N</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"# solution 1\ndf = products_df \ndf.filter(\"low_fats == 'Y' and recyclable == 'Y'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:36.781197Z","iopub.execute_input":"2024-10-10T02:46:36.781922Z","iopub.status.idle":"2024-10-10T02:46:38.798271Z","shell.execute_reply.started":"2024-10-10T02:46:36.781862Z","shell.execute_reply":"2024-10-10T02:46:38.797089Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"+----------+--------+----------+\n|product_id|low_fats|recyclable|\n+----------+--------+----------+\n|         1|       Y|         Y|\n|         3|       Y|         Y|\n+----------+--------+----------+","text/html":"<table border='1'>\n<tr><th>product_id</th><th>low_fats</th><th>recyclable</th></tr>\n<tr><td>1</td><td>Y</td><td>Y</td></tr>\n<tr><td>3</td><td>Y</td><td>Y</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"# solution 2, the most preferred with less overhead processing as it uses the pyspark column api\nfrom pyspark.sql.functions import col\n\n# df.filter((col(\"low_fats\")=='Y') and (col(\"recyclable\")=='Y'))  # we can't use \"and\" else we will get a PySparkValueError\ndf.filter((col(\"low_fats\")=='Y') & (col(\"recyclable\")=='Y'))","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:38.801614Z","iopub.execute_input":"2024-10-10T02:46:38.802417Z","iopub.status.idle":"2024-10-10T02:46:40.583061Z","shell.execute_reply.started":"2024-10-10T02:46:38.802358Z","shell.execute_reply":"2024-10-10T02:46:40.581827Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"+----------+--------+----------+\n|product_id|low_fats|recyclable|\n+----------+--------+----------+\n|         1|       Y|         Y|\n|         3|       Y|         Y|\n+----------+--------+----------+","text/html":"<table border='1'>\n<tr><th>product_id</th><th>low_fats</th><th>recyclable</th></tr>\n<tr><td>1</td><td>Y</td><td>Y</td></tr>\n<tr><td>3</td><td>Y</td><td>Y</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"# solution 3\n\ndf.createOrReplaceTempView(\"products\")\n\ndf3 = spark.sql(\"select * from products where low_fats = 'Y' and recyclable = 'Y'\")\ndf3","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:40.588901Z","iopub.execute_input":"2024-10-10T02:46:40.589495Z","iopub.status.idle":"2024-10-10T02:46:42.518211Z","shell.execute_reply.started":"2024-10-10T02:46:40.589428Z","shell.execute_reply":"2024-10-10T02:46:42.517060Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"+----------+--------+----------+\n|product_id|low_fats|recyclable|\n+----------+--------+----------+\n|         1|       Y|         Y|\n|         3|       Y|         Y|\n+----------+--------+----------+","text/html":"<table border='1'>\n<tr><th>product_id</th><th>low_fats</th><th>recyclable</th></tr>\n<tr><td>1</td><td>Y</td><td>Y</td></tr>\n<tr><td>3</td><td>Y</td><td>Y</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\npddf = df.toPandas()\npddf","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:42.519546Z","iopub.execute_input":"2024-10-10T02:46:42.520032Z","iopub.status.idle":"2024-10-10T02:46:43.411669Z","shell.execute_reply.started":"2024-10-10T02:46:42.519975Z","shell.execute_reply":"2024-10-10T02:46:43.410445Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   product_id low_fats recyclable\n0           0        Y          N\n1           1        Y          Y\n2           2        N          Y\n3           3        Y          Y\n4           4        N          N","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>low_fats</th>\n      <th>recyclable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Y</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>N</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>N</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# solution 1\npddf_1 = pddf[ (pddf[\"low_fats\"]=='Y') & (pddf[\"recyclable\"]=='Y') ]\npddf_1","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:43.413137Z","iopub.execute_input":"2024-10-10T02:46:43.413632Z","iopub.status.idle":"2024-10-10T02:46:43.449044Z","shell.execute_reply.started":"2024-10-10T02:46:43.413575Z","shell.execute_reply":"2024-10-10T02:46:43.447505Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   product_id low_fats recyclable\n1           1        Y          Y\n3           3        Y          Y","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>low_fats</th>\n      <th>recyclable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# solution 2\npddf_2 = pddf.loc[(pddf[\"low_fats\"] == 'Y') & (pddf[\"recyclable\"]=='Y')]\npddf_2","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:43.450501Z","iopub.execute_input":"2024-10-10T02:46:43.451029Z","iopub.status.idle":"2024-10-10T02:46:43.478537Z","shell.execute_reply.started":"2024-10-10T02:46:43.450969Z","shell.execute_reply":"2024-10-10T02:46:43.477234Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   product_id low_fats recyclable\n1           1        Y          Y\n3           3        Y          Y","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>low_fats</th>\n      <th>recyclable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# solution 3\npddf_3 = pddf.query(\"low_fats == 'Y' and recyclable == 'Y'\")\npddf_3","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:43.484985Z","iopub.execute_input":"2024-10-10T02:46:43.488753Z","iopub.status.idle":"2024-10-10T02:46:43.525198Z","shell.execute_reply.started":"2024-10-10T02:46:43.488693Z","shell.execute_reply":"2024-10-10T02:46:43.523767Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   product_id low_fats recyclable\n1           1        Y          Y\n3           3        Y          Y","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>low_fats</th>\n      <th>recyclable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# solution 4\npddf_4 = pddf[ (pddf[\"low_fats\"].isin([\"Y\"])) & (pddf[\"recyclable\"].isin([\"Y\"])) ]\npddf_4","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:43.530397Z","iopub.execute_input":"2024-10-10T02:46:43.531296Z","iopub.status.idle":"2024-10-10T02:46:43.547039Z","shell.execute_reply.started":"2024-10-10T02:46:43.531236Z","shell.execute_reply":"2024-10-10T02:46:43.545790Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   product_id low_fats recyclable\n1           1        Y          Y\n3           3        Y          Y","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_id</th>\n      <th>low_fats</th>\n      <th>recyclable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Y</td>\n      <td>Y</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 2\n\nFind the names of the customer that are not referred by the customer with id = 2.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\nspark = SparkSession.builder.appName(\"Problem2\").getOrCreate()\n\ncustomer_schema = StructType([StructField(\"id\",IntegerType()),StructField(\"name\",StringType()),StructField(\"referee_id\",IntegerType())])\n\ncustomer_data = [\n    (1, \"Will\", None),\n    (2, \"Jane\", None),\n    (3, \"Alex\", 2),\n    (4, \"Bill\", None),\n    (5, \"Zack\", 1),\n    (6, \"Mark\", 2)\n]\n\ncustomer_df = spark.createDataFrame(customer_data,schema=customer_schema)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:43.548723Z","iopub.execute_input":"2024-10-10T02:46:43.549963Z","iopub.status.idle":"2024-10-10T02:46:43.607618Z","shell.execute_reply.started":"2024-10-10T02:46:43.549903Z","shell.execute_reply":"2024-10-10T02:46:43.606428Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"24/10/10 02:46:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"customer_df.filter(\"referee_id <> 2 or referee_id is NULL\").select(\"name\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:43.609143Z","iopub.execute_input":"2024-10-10T02:46:43.609666Z","iopub.status.idle":"2024-10-10T02:46:45.670649Z","shell.execute_reply.started":"2024-10-10T02:46:43.609599Z","shell.execute_reply":"2024-10-10T02:46:45.668994Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"+----+\n|name|\n+----+\n|Will|\n|Jane|\n|Bill|\n|Zack|\n+----+","text/html":"<table border='1'>\n<tr><th>name</th></tr>\n<tr><td>Will</td></tr>\n<tr><td>Jane</td></tr>\n<tr><td>Bill</td></tr>\n<tr><td>Zack</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"# here OR is a bitwise operator and we should use the two ORed conditions in the paranthesis\nfrom pyspark.sql.functions import col\ncustomer_df.filter( ( col(\"referee_id\").isNull()) | (col(\"referee_id\") != 2 )).select(\"name\") ","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:45.676923Z","iopub.execute_input":"2024-10-10T02:46:45.677593Z","iopub.status.idle":"2024-10-10T02:46:47.810826Z","shell.execute_reply.started":"2024-10-10T02:46:45.677524Z","shell.execute_reply":"2024-10-10T02:46:47.809604Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"+----+\n|name|\n+----+\n|Will|\n|Jane|\n|Bill|\n|Zack|\n+----+","text/html":"<table border='1'>\n<tr><th>name</th></tr>\n<tr><td>Will</td></tr>\n<tr><td>Jane</td></tr>\n<tr><td>Bill</td></tr>\n<tr><td>Zack</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"customer_df.filter((col(\"referee_id\").isNull()) | ( ~ (col(\"referee_id\") == 2))).select(\"name\")\n\n# Using isin() to match a range of values and handling NULL separately\ncustomer_df.filter((col(\"referee_id\").isNull()) | (~col(\"referee_id\").isin(2))).select(\"name\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:47.816559Z","iopub.execute_input":"2024-10-10T02:46:47.817444Z","iopub.status.idle":"2024-10-10T02:46:49.591157Z","shell.execute_reply.started":"2024-10-10T02:46:47.817351Z","shell.execute_reply":"2024-10-10T02:46:49.589883Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"+----+\n|name|\n+----+\n|Will|\n|Jane|\n|Bill|\n|Zack|\n+----+","text/html":"<table border='1'>\n<tr><th>name</th></tr>\n<tr><td>Will</td></tr>\n<tr><td>Jane</td></tr>\n<tr><td>Bill</td></tr>\n<tr><td>Zack</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"import numpy as np\npndf = customer_df.toPandas()\ndf = pndf.replace({np.nan: None})\n\n# val = df.loc[df[\"id\"]==1,\"referee_id\"].values[0]\ndf1 = df.loc[(df[\"referee_id\"]!=2) | (df[\"referee_id\"].isnull()), \"name\"]\ndf1\nprint(df1.to_string(index=False)) # this will just remove the default row labels (0,1,2..) assigned to pandas dataframe by default","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:49.596012Z","iopub.execute_input":"2024-10-10T02:46:49.596635Z","iopub.status.idle":"2024-10-10T02:46:50.366524Z","shell.execute_reply.started":"2024-10-10T02:46:49.596571Z","shell.execute_reply":"2024-10-10T02:46:50.365062Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Will\nJane\nBill\nZack\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"df[(df[\"referee_id\"]!=2)|(df[\"referee_id\"].isnull())][\"name\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:50.367906Z","iopub.execute_input":"2024-10-10T02:46:50.368465Z","iopub.status.idle":"2024-10-10T02:46:50.992379Z","shell.execute_reply.started":"2024-10-10T02:46:50.368407Z","shell.execute_reply":"2024-10-10T02:46:50.990945Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0    Will\n1    Jane\n3    Bill\n4    Zack\nName: name, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"# Using the query method\ndf.query('referee_id != 2 or referee_id.isnull()')['name']","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:50.993976Z","iopub.execute_input":"2024-10-10T02:46:50.994479Z","iopub.status.idle":"2024-10-10T02:46:51.014282Z","shell.execute_reply.started":"2024-10-10T02:46:50.994424Z","shell.execute_reply":"2024-10-10T02:46:51.013021Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0    Will\n1    Jane\n3    Bill\n4    Zack\nName: name, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 3\n\nA country is big if:\n\nit has an area of at least three million (i.e., 3000000 km2), or\nit has a population of at least twenty-five million (i.e., 25000000).\nWrite a solution to find the name, population, and area of the big countries.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType,StructField,IntegerType,StringType,LongType\n\nspark = SparkSession.builder.appName(\"prblm3\").getOrCreate()\n\ndf_schema = StructType([\n    StructField(\"name\",StringType(),True),\n    StructField(\"continent\",StringType(),True),\n    StructField(\"area\",IntegerType(),True),\n    StructField(\"population\",IntegerType(),True),\n    StructField(\"gdp\",LongType(),True)\n])\n\ndata = [\n    [\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000000],\n    [\"Albania\", \"Europe\", 28748, 2831741, 12960000000],\n    [\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000],\n    [\"Andorra\", \"Europe\", 468, 78115, 3712000000],\n    [\"Angola\", \"Africa\", 1246700, 20609294, 100990000000]\n]\n\ndf = spark.createDataFrame(data,df_schema)\ndf","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:51.016193Z","iopub.execute_input":"2024-10-10T02:46:51.016978Z","iopub.status.idle":"2024-10-10T02:46:52.566405Z","shell.execute_reply.started":"2024-10-10T02:46:51.016931Z","shell.execute_reply":"2024-10-10T02:46:52.565072Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"24/10/10 02:46:51 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"+-----------+---------+-------+----------+------------+\n|       name|continent|   area|population|         gdp|\n+-----------+---------+-------+----------+------------+\n|Afghanistan|     Asia| 652230|  25500100| 20343000000|\n|    Albania|   Europe|  28748|   2831741| 12960000000|\n|    Algeria|   Africa|2381741|  37100000|188681000000|\n|    Andorra|   Europe|    468|     78115|  3712000000|\n|     Angola|   Africa|1246700|  20609294|100990000000|\n+-----------+---------+-------+----------+------------+","text/html":"<table border='1'>\n<tr><th>name</th><th>continent</th><th>area</th><th>population</th><th>gdp</th></tr>\n<tr><td>Afghanistan</td><td>Asia</td><td>652230</td><td>25500100</td><td>20343000000</td></tr>\n<tr><td>Albania</td><td>Europe</td><td>28748</td><td>2831741</td><td>12960000000</td></tr>\n<tr><td>Algeria</td><td>Africa</td><td>2381741</td><td>37100000</td><td>188681000000</td></tr>\n<tr><td>Andorra</td><td>Europe</td><td>468</td><td>78115</td><td>3712000000</td></tr>\n<tr><td>Angola</td><td>Africa</td><td>1246700</td><td>20609294</td><td>100990000000</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import col\n\ndf.where( (col(\"area\")>=3000000) | (col(\"population\")>=25000000) ).select(\"name\",\"population\",\"area\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:52.572140Z","iopub.execute_input":"2024-10-10T02:46:52.572697Z","iopub.status.idle":"2024-10-10T02:46:53.994050Z","shell.execute_reply.started":"2024-10-10T02:46:52.572639Z","shell.execute_reply":"2024-10-10T02:46:53.992678Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"+-----------+----------+-------+\n|       name|population|   area|\n+-----------+----------+-------+\n|Afghanistan|  25500100| 652230|\n|    Algeria|  37100000|2381741|\n+-----------+----------+-------+","text/html":"<table border='1'>\n<tr><th>name</th><th>population</th><th>area</th></tr>\n<tr><td>Afghanistan</td><td>25500100</td><td>652230</td></tr>\n<tr><td>Algeria</td><td>37100000</td><td>2381741</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pddf = df.toPandas()\npddf_filtered = pddf[(pddf[\"population\"]>=25000000) | (pddf[\"area\"]>=3000000) ]\npddf_filtered_formatted = pddf_filtered[[\"name\",\"population\",\"area\"]].to_string(index=False)\nprint(pddf_filtered_formatted)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:53.998867Z","iopub.execute_input":"2024-10-10T02:46:53.999430Z","iopub.status.idle":"2024-10-10T02:46:54.597056Z","shell.execute_reply.started":"2024-10-10T02:46:53.999370Z","shell.execute_reply":"2024-10-10T02:46:54.595634Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"       name  population    area\nAfghanistan    25500100  652230\n    Algeria    37100000 2381741\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Problem 4\n\nWrite a solution to find all the authors that viewed at least one of their own articles.\nReturn the result table sorted by id in ascending order","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DateType\nfrom datetime import date\n\nspark = SparkSession.builder.appName(\"P4\").getOrCreate()\n\nschema = StructType([\n    StructField(\"article_id\", IntegerType(), True),\n    StructField(\"author_id\", IntegerType(), True),\n    StructField(\"viewer_id\", IntegerType(), True),\n    StructField(\"view_date\", DateType(), True)\n])\n\n# Data for the Views table (based on the input sample)\ndata = [\n    (1, 3, 5, date(2019, 8, 1)),\n    (1, 3, 6, date(2019, 8, 2)),\n    (2, 7, 7, date(2019, 8, 1)),\n    (2, 7, 6, date(2019, 8, 2)),\n    (4, 7, 1, date(2019, 7, 22)),\n    (3, 4, 4, date(2019, 7, 21)),\n    (3, 4, 4, date(2019, 7, 21))\n]\n\n# Create DataFrame\ndf_views = spark.createDataFrame(data, schema)\n\n# Show the DataFrame content\ndf_views.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:54.599459Z","iopub.execute_input":"2024-10-10T02:46:54.600087Z","iopub.status.idle":"2024-10-10T02:46:55.466268Z","shell.execute_reply.started":"2024-10-10T02:46:54.600025Z","shell.execute_reply":"2024-10-10T02:46:55.465027Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"24/10/10 02:46:54 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+----------+---------+---------+----------+\n|article_id|author_id|viewer_id| view_date|\n+----------+---------+---------+----------+\n|         1|        3|        5|2019-08-01|\n|         1|        3|        6|2019-08-02|\n|         2|        7|        7|2019-08-01|\n|         2|        7|        6|2019-08-02|\n|         4|        7|        1|2019-07-22|\n|         3|        4|        4|2019-07-21|\n|         3|        4|        4|2019-07-21|\n+----------+---------+---------+----------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import col\n\ndf_views.filter(col(\"author_id\")==col(\"viewer_id\")).select(\"author_id\").distinct().sort(\"author_id\",ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:55.467680Z","iopub.execute_input":"2024-10-10T02:46:55.468152Z","iopub.status.idle":"2024-10-10T02:46:57.995193Z","shell.execute_reply.started":"2024-10-10T02:46:55.468095Z","shell.execute_reply":"2024-10-10T02:46:57.994022Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"+---------+\n|author_id|\n+---------+\n|        7|\n|        4|\n+---------+","text/html":"<table border='1'>\n<tr><th>author_id</th></tr>\n<tr><td>7</td></tr>\n<tr><td>4</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pddf = df_views.toPandas()\npddf = pddf[pddf[\"author_id\"]==pddf[\"viewer_id\"]][\"author_id\"].drop_duplicates() # Output is pandas series\npddf_df = pddf.to_frame() # pandas data frame \n# pddf = pddf[pddf[\"author_id\"]==pddf[\"viewer_id\"]][\"author_id\"].unique() # Output is numpy.ndarray\ntype(pddf_df) # pandas.core.frame.DataFrame\npddf_df","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:57.996501Z","iopub.execute_input":"2024-10-10T02:46:57.996950Z","iopub.status.idle":"2024-10-10T02:46:58.573157Z","shell.execute_reply.started":"2024-10-10T02:46:57.996899Z","shell.execute_reply":"2024-10-10T02:46:58.572128Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   author_id\n2          7\n5          4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>author_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 5\n\nWrite a solution to find the IDs of the invalid tweets. The tweet is invalid if the number of characters used in the content of the tweet is strictly greater than 15.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"TweetsTable\").getOrCreate()\n\n# Define schema for the Tweets table\nschema = StructType([\n    StructField(\"tweet_id\", IntegerType(), True),\n    StructField(\"content\", StringType(), True)\n])\n\n# Data for the Tweets table (based on the input sample)\ndata = [\n    (1, \"Let us Code\"),\n    (2, \"More than fifteen chars are here!\")\n]\n\n# Create DataFrame\ndf_tweets = spark.createDataFrame(data, schema)\n\n# Show the DataFrame content\ndf_tweets","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:58.575026Z","iopub.execute_input":"2024-10-10T02:46:58.575897Z","iopub.status.idle":"2024-10-10T02:46:59.919594Z","shell.execute_reply.started":"2024-10-10T02:46:58.575842Z","shell.execute_reply":"2024-10-10T02:46:59.918388Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"24/10/10 02:46:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"+--------+--------------------+\n|tweet_id|             content|\n+--------+--------------------+\n|       1|         Let us Code|\n|       2|More than fifteen...|\n+--------+--------------------+","text/html":"<table border='1'>\n<tr><th>tweet_id</th><th>content</th></tr>\n<tr><td>1</td><td>Let us Code</td></tr>\n<tr><td>2</td><td>More than fifteen...</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import col,length\n\ndf = df_tweets \ndf.where(length(col(\"content\"))>15).select(\"tweet_id\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:46:59.921342Z","iopub.execute_input":"2024-10-10T02:46:59.922111Z","iopub.status.idle":"2024-10-10T02:47:01.259973Z","shell.execute_reply.started":"2024-10-10T02:46:59.922054Z","shell.execute_reply":"2024-10-10T02:47:01.258506Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"+--------+\n|tweet_id|\n+--------+\n|       2|\n+--------+","text/html":"<table border='1'>\n<tr><th>tweet_id</th></tr>\n<tr><td>2</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"df.where(\"length(content)>15\").select(\"tweet_id\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:01.261431Z","iopub.execute_input":"2024-10-10T02:47:01.266797Z","iopub.status.idle":"2024-10-10T02:47:02.856067Z","shell.execute_reply.started":"2024-10-10T02:47:01.266710Z","shell.execute_reply":"2024-10-10T02:47:02.854878Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"+--------+\n|tweet_id|\n+--------+\n|       2|\n+--------+","text/html":"<table border='1'>\n<tr><th>tweet_id</th></tr>\n<tr><td>2</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pddf = df.toPandas()\npddf[pddf[\"content\"].str.len()>15][\"tweet_id\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:02.857497Z","iopub.execute_input":"2024-10-10T02:47:02.861707Z","iopub.status.idle":"2024-10-10T02:47:03.331062Z","shell.execute_reply.started":"2024-10-10T02:47:02.861641Z","shell.execute_reply":"2024-10-10T02:47:03.329725Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"1    2\nName: tweet_id, dtype: int32"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 6 \n\nWrite a solution to show the unique ID of each user, If a user does not have a unique ID replace just show null.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"EmployeeTables\").getOrCreate()\n\n# Define schema for Employees table\nemployees_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True)\n])\n\n# Create data for Employees table\nemployees_data = [\n    (1, \"Alice\"),\n    (7, \"Bob\"),\n    (11, \"Meir\"),\n    (90, \"Winston\"),\n    (3, \"Jonathan\")\n]\n\n# Create Employees DataFrame\ndf_employees = spark.createDataFrame(employees_data, schema=employees_schema)\n\ndf_employees\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:03.339436Z","iopub.execute_input":"2024-10-10T02:47:03.340007Z","iopub.status.idle":"2024-10-10T02:47:04.613602Z","shell.execute_reply.started":"2024-10-10T02:47:03.339964Z","shell.execute_reply":"2024-10-10T02:47:04.612356Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"24/10/10 02:47:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"+---+--------+\n| id|    name|\n+---+--------+\n|  1|   Alice|\n|  7|     Bob|\n| 11|    Meir|\n| 90| Winston|\n|  3|Jonathan|\n+---+--------+","text/html":"<table border='1'>\n<tr><th>id</th><th>name</th></tr>\n<tr><td>1</td><td>Alice</td></tr>\n<tr><td>7</td><td>Bob</td></tr>\n<tr><td>11</td><td>Meir</td></tr>\n<tr><td>90</td><td>Winston</td></tr>\n<tr><td>3</td><td>Jonathan</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"# Define schema for EmployeeUNI table\nemployeeuni_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"unique_id\", IntegerType(), True)\n])\n\n# Create data for EmployeeUNI table\nemployeeuni_data = [\n    (3, 1),\n    (11, 2),\n    (90, 3)\n]\n\n# Create EmployeeUNI DataFrame\ndf_employeeuni = spark.createDataFrame(employeeuni_data, schema=employeeuni_schema)\ndf_employeeuni\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:04.618134Z","iopub.execute_input":"2024-10-10T02:47:04.618594Z","iopub.status.idle":"2024-10-10T02:47:06.092784Z","shell.execute_reply.started":"2024-10-10T02:47:04.618551Z","shell.execute_reply":"2024-10-10T02:47:06.091430Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"+---+---------+\n| id|unique_id|\n+---+---------+\n|  3|        1|\n| 11|        2|\n| 90|        3|\n+---+---------+","text/html":"<table border='1'>\n<tr><th>id</th><th>unique_id</th></tr>\n<tr><td>3</td><td>1</td></tr>\n<tr><td>11</td><td>2</td></tr>\n<tr><td>90</td><td>3</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## PySpark\n\nIn PySpark, you can access columns of a DataFrame using both dot notation (e.g., df_employees.id) and bracket notation (e.g., df_employees[\"id\"])\n\nDot Notation (df_employees.id):\n- Easier to read and type for simple use cases.\n- Cannot be used if the column name contains spaces or special characters, or if the column name conflicts with a built-in method or attribute of the ---- DataFrame (like count, join, etc.).\n\n\nBracket Notation (df_employees[\"id\"]):\n- More flexible and can be used for any valid column name, including those with spaces or special characters.\n- It allows for the use of variables to specify the column name dynamically.","metadata":{}},{"cell_type":"code","source":"df_employees.join(df_employeeuni,df_employees[\"id\"]==df_employeeuni[\"id\"],\"left_outer\").select(df_employees[\"id\"],df_employees[\"name\"],df_employeeuni[\"unique_id\"])","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:06.096672Z","iopub.execute_input":"2024-10-10T02:47:06.097218Z","iopub.status.idle":"2024-10-10T02:47:08.956446Z","shell.execute_reply.started":"2024-10-10T02:47:06.097161Z","shell.execute_reply":"2024-10-10T02:47:08.955061Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"+---+--------+---------+\n| id|    name|unique_id|\n+---+--------+---------+\n|  1|   Alice|     NULL|\n|  7|     Bob|     NULL|\n| 11|    Meir|        2|\n|  3|Jonathan|        1|\n| 90| Winston|        3|\n+---+--------+---------+","text/html":"<table border='1'>\n<tr><th>id</th><th>name</th><th>unique_id</th></tr>\n<tr><td>1</td><td>Alice</td><td>NULL</td></tr>\n<tr><td>7</td><td>Bob</td><td>NULL</td></tr>\n<tr><td>11</td><td>Meir</td><td>2</td></tr>\n<tr><td>3</td><td>Jonathan</td><td>1</td></tr>\n<tr><td>90</td><td>Winston</td><td>3</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"\ndf_employees.join(df_employeeuni,df_employees.id==df_employeeuni.id,\"left_outer\").select(df_employees.id,df_employees.name,df_employeeuni.unique_id)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:08.958130Z","iopub.execute_input":"2024-10-10T02:47:08.958531Z","iopub.status.idle":"2024-10-10T02:47:11.424849Z","shell.execute_reply.started":"2024-10-10T02:47:08.958489Z","shell.execute_reply":"2024-10-10T02:47:11.423410Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"+---+--------+---------+\n| id|    name|unique_id|\n+---+--------+---------+\n|  1|   Alice|     NULL|\n|  7|     Bob|     NULL|\n| 11|    Meir|        2|\n|  3|Jonathan|        1|\n| 90| Winston|        3|\n+---+--------+---------+","text/html":"<table border='1'>\n<tr><th>id</th><th>name</th><th>unique_id</th></tr>\n<tr><td>1</td><td>Alice</td><td>NULL</td></tr>\n<tr><td>7</td><td>Bob</td><td>NULL</td></tr>\n<tr><td>11</td><td>Meir</td><td>2</td></tr>\n<tr><td>3</td><td>Jonathan</td><td>1</td></tr>\n<tr><td>90</td><td>Winston</td><td>3</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pdf_emp = df_employees.toPandas()\npdf_emp_uni = df_employeeuni.toPandas()\n\nresult = pd.merge(pdf_emp,pdf_emp_uni,left_on=\"id\",right_on=\"id\",how=\"left\")\nresult[[\"name\",\"unique_id\"]]","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:11.426383Z","iopub.execute_input":"2024-10-10T02:47:11.426956Z","iopub.status.idle":"2024-10-10T02:47:12.347113Z","shell.execute_reply.started":"2024-10-10T02:47:11.426890Z","shell.execute_reply":"2024-10-10T02:47:12.345867Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"       name  unique_id\n0     Alice        NaN\n1       Bob        NaN\n2      Meir        2.0\n3   Winston        3.0\n4  Jonathan        1.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>unique_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Alice</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Bob</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Meir</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Winston</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Jonathan</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 7\n\nWrite a solution to report the product_name, year, and price for each sale_id in the Sales table","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize SparkSession\nspark = SparkSession.builder.appName(\"SalesProduct\").getOrCreate()\n\n# Define schema for Sales DataFrame\nsales_schema = StructType([\n    StructField(\"sale_id\", IntegerType(), False),\n    StructField(\"product_id\", IntegerType(), False),\n    StructField(\"year\", IntegerType(), False),\n    StructField(\"quantity\", IntegerType(), True),\n    StructField(\"price\", IntegerType(), True)\n])\n\n# Create Sales DataFrame\nsales_data = [\n    (1, 100, 2008, 10, 5000),\n    (2, 100, 2009, 12, 5000),\n    (7, 200, 2011, 15, 9000)\n]\n\ndf_sales = spark.createDataFrame(sales_data, schema=sales_schema)\n\n# Define schema for Product DataFrame\nproduct_schema = StructType([\n    StructField(\"product_id\", IntegerType(), False),\n    StructField(\"product_name\", StringType(), True)\n])\n\n# Create Product DataFrame\nproduct_data = [\n    (100, \"Nokia\"),\n    (200, \"Apple\"),\n    (300, \"Samsung\")\n]\n\ndf_product = spark.createDataFrame(product_data, schema=product_schema)\n\n# Show the data in both DataFrames\ndf_sales.show()\ndf_product.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:12.348798Z","iopub.execute_input":"2024-10-10T02:47:12.349352Z","iopub.status.idle":"2024-10-10T02:47:13.675595Z","shell.execute_reply.started":"2024-10-10T02:47:12.349271Z","shell.execute_reply":"2024-10-10T02:47:13.674259Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"24/10/10 02:47:12 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+-------+----------+----+--------+-----+\n|sale_id|product_id|year|quantity|price|\n+-------+----------+----+--------+-----+\n|      1|       100|2008|      10| 5000|\n|      2|       100|2009|      12| 5000|\n|      7|       200|2011|      15| 9000|\n+-------+----------+----+--------+-----+\n\n+----------+------------+\n|product_id|product_name|\n+----------+------------+\n|       100|       Nokia|\n|       200|       Apple|\n|       300|     Samsung|\n+----------+------------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"df_sales.join(df_product,df_product.product_id == df_sales.product_id,\"inner\").select(\"product_name\", \"year\",\"price\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:13.677239Z","iopub.execute_input":"2024-10-10T02:47:13.677781Z","iopub.status.idle":"2024-10-10T02:47:16.431590Z","shell.execute_reply.started":"2024-10-10T02:47:13.677725Z","shell.execute_reply":"2024-10-10T02:47:16.430385Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"+------------+----+-----+\n|product_name|year|price|\n+------------+----+-----+\n|       Nokia|2008| 5000|\n|       Nokia|2009| 5000|\n|       Apple|2011| 9000|\n+------------+----+-----+","text/html":"<table border='1'>\n<tr><th>product_name</th><th>year</th><th>price</th></tr>\n<tr><td>Nokia</td><td>2008</td><td>5000</td></tr>\n<tr><td>Nokia</td><td>2009</td><td>5000</td></tr>\n<tr><td>Apple</td><td>2011</td><td>9000</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql.functions import col\n\ndf_sales.alias(\"s\").join(df_product.alias(\"p\"), col(\"s.product_id\") == col(\"p.product_id\"), \"inner\") \\\n    .select(col(\"p.product_name\"), col(\"s.year\"), col(\"s.price\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:16.432994Z","iopub.execute_input":"2024-10-10T02:47:16.433628Z","iopub.status.idle":"2024-10-10T02:47:18.747827Z","shell.execute_reply.started":"2024-10-10T02:47:16.433566Z","shell.execute_reply":"2024-10-10T02:47:18.746366Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"+------------+----+-----+\n|product_name|year|price|\n+------------+----+-----+\n|       Nokia|2008| 5000|\n|       Nokia|2009| 5000|\n|       Apple|2011| 9000|\n+------------+----+-----+","text/html":"<table border='1'>\n<tr><th>product_name</th><th>year</th><th>price</th></tr>\n<tr><td>Nokia</td><td>2008</td><td>5000</td></tr>\n<tr><td>Nokia</td><td>2009</td><td>5000</td></tr>\n<tr><td>Apple</td><td>2011</td><td>9000</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pdf_sales = df_sales.toPandas()\npdf_product = df_product.toPandas()\n\nresult = pd.merge(left=pdf_sales,right=pdf_product,left_on=\"product_id\",right_on=\"product_id\",how=\"inner\")\nresult[[\"product_name\",\"year\",\"price\"]]","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:18.749934Z","iopub.execute_input":"2024-10-10T02:47:18.750444Z","iopub.status.idle":"2024-10-10T02:47:20.187658Z","shell.execute_reply.started":"2024-10-10T02:47:18.750388Z","shell.execute_reply":"2024-10-10T02:47:20.186319Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"  product_name  year  price\n0        Nokia  2008   5000\n1        Nokia  2009   5000\n2        Apple  2011   9000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>product_name</th>\n      <th>year</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Nokia</td>\n      <td>2008</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Nokia</td>\n      <td>2009</td>\n      <td>5000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Apple</td>\n      <td>2011</td>\n      <td>9000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 8\n\nWrite a solution to find the IDs of the users who visited without making any transactions and the number of times they made these types of visits.","metadata":{}},{"cell_type":"code","source":"# Import necessary modules\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"VisitsTransactions\").getOrCreate()\n\n# Define the schema for the Visits table\nvisits_schema = StructType([\n    StructField(\"visit_id\", IntegerType(), True),\n    StructField(\"customer_id\", IntegerType(), True)\n])\n\n# Data for Visits table\nvisits_data = [\n    (1, 23),\n    (2, 9),\n    (4, 30),\n    (5, 54),\n    (6, 96),\n    (7, 54),\n    (8, 54)\n]\n\n# Create DataFrame for Visits table\ndf_visits = spark.createDataFrame(visits_data, schema=visits_schema)\n\n# Define the schema for the Transactions table\ntransactions_schema = StructType([\n    StructField(\"transaction_id\", IntegerType(), True),\n    StructField(\"visit_id\", IntegerType(), True),\n    StructField(\"amount\", IntegerType(), True)\n])\n\n# Data for Transactions table\ntransactions_data = [\n    (2, 5, 310),\n    (3, 5, 300),\n    (9, 5, 200),\n    (12, 1, 910),\n    (13, 2, 970)\n]\n\n# Create DataFrame for Transactions table\ndf_transactions = spark.createDataFrame(transactions_data, schema=transactions_schema)\n\n# Show the created DataFrames\ndf_visits.show()\ndf_transactions.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:20.189341Z","iopub.execute_input":"2024-10-10T02:47:20.189848Z","iopub.status.idle":"2024-10-10T02:47:21.487894Z","shell.execute_reply.started":"2024-10-10T02:47:20.189777Z","shell.execute_reply":"2024-10-10T02:47:21.486538Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"24/10/10 02:47:20 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+--------+-----------+\n|visit_id|customer_id|\n+--------+-----------+\n|       1|         23|\n|       2|          9|\n|       4|         30|\n|       5|         54|\n|       6|         96|\n|       7|         54|\n|       8|         54|\n+--------+-----------+\n\n+--------------+--------+------+\n|transaction_id|visit_id|amount|\n+--------------+--------+------+\n|             2|       5|   310|\n|             3|       5|   300|\n|             9|       5|   200|\n|            12|       1|   910|\n|            13|       2|   970|\n+--------------+--------+------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"df_visits.join(df_transactions,df_visits.visit_id == df_transactions.visit_id,\"left_anti\").groupBy(col(\"customer_id\")).count()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:21.489269Z","iopub.execute_input":"2024-10-10T02:47:21.489783Z","iopub.status.idle":"2024-10-10T02:47:24.156725Z","shell.execute_reply.started":"2024-10-10T02:47:21.489696Z","shell.execute_reply":"2024-10-10T02:47:24.155563Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"+-----------+-----+\n|customer_id|count|\n+-----------+-----+\n|         54|    2|\n|         96|    1|\n|         30|    1|\n+-----------+-----+","text/html":"<table border='1'>\n<tr><th>customer_id</th><th>count</th></tr>\n<tr><td>54</td><td>2</td></tr>\n<tr><td>96</td><td>1</td></tr>\n<tr><td>30</td><td>1</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"df_visits.join(df_transactions,df_visits.visit_id == df_transactions.visit_id,\"left_anti\").groupBy(df_visits[\"customer_id\"]).count().withColumnRenamed('count','count_no_of_trans')\n# just a small change to test that [] notation works and renaming the output column name","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:24.158433Z","iopub.execute_input":"2024-10-10T02:47:24.159167Z","iopub.status.idle":"2024-10-10T02:47:26.575765Z","shell.execute_reply.started":"2024-10-10T02:47:24.159091Z","shell.execute_reply":"2024-10-10T02:47:26.574541Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"+-----------+-----------------+\n|customer_id|count_no_of_trans|\n+-----------+-----------------+\n|         54|                2|\n|         96|                1|\n|         30|                1|\n+-----------+-----------------+","text/html":"<table border='1'>\n<tr><th>customer_id</th><th>count_no_of_trans</th></tr>\n<tr><td>54</td><td>2</td></tr>\n<tr><td>96</td><td>1</td></tr>\n<tr><td>30</td><td>1</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pdf_visits = df_visits.toPandas()\npdf_transactions = df_transactions.toPandas()\nmerged_df = pdf_visits.merge(pdf_transactions,on=\"visit_id\",how=\"left\",indicator=True)\nmerged_df = merged_df[merged_df[\"_merge\"]==\"left_only\"]\n\nmerged_df_no_reset_index = merged_df.groupby(\"customer_id\").count()\nprint(merged_df_no_reset_index)\n\n# without reset_index, the groupby will put the customer_id as the rowindex effectively removing the column\nmerged_df = merged_df.groupby(\"customer_id\").count().reset_index()\nmerged_df.rename(columns={\"visit_id\":\"count_no_of_trans\"},inplace=True)\nprint(merged_df[[\"customer_id\",\"count_no_of_trans\"]])","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:26.577106Z","iopub.execute_input":"2024-10-10T02:47:26.577627Z","iopub.status.idle":"2024-10-10T02:47:27.521576Z","shell.execute_reply.started":"2024-10-10T02:47:26.577561Z","shell.execute_reply":"2024-10-10T02:47:27.520093Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"             visit_id  transaction_id  amount  _merge\ncustomer_id                                          \n30                  1               0       0       1\n54                  2               0       0       2\n96                  1               0       0       1\n   customer_id  count_no_of_trans\n0           30                  1\n1           54                  2\n2           96                  1\n","output_type":"stream"}]},{"cell_type":"code","source":"pdf_visits = df_visits.toPandas()\npdf_transactions = df_transactions.toPandas()\n\nmerged_df = pdf_visits.merge(pdf_transactions,on=\"visit_id\",how=\"left\")\nmerged_df[merged_df[\"transaction_id\"].isna()].groupby(\"customer_id\")['visit_id'].count().rename(\"count_no_trans\").reset_index()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:27.524871Z","iopub.execute_input":"2024-10-10T02:47:27.525369Z","iopub.status.idle":"2024-10-10T02:47:28.342981Z","shell.execute_reply.started":"2024-10-10T02:47:27.525290Z","shell.execute_reply":"2024-10-10T02:47:28.341509Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"   customer_id  count_no_trans\n0           30               1\n1           54               2\n2           96               1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>customer_id</th>\n      <th>count_no_trans</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>30</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>54</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>96</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 9\n\nWrite a solution to find all dates' id with higher temperatures compared to its previous dates","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DateType\nfrom datetime import datetime\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"WeatherData\").getOrCreate()\n\n# Define the schema for the Weather table\nweather_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"recordDate\", DateType(), True),\n    StructField(\"temperature\", IntegerType(), True)\n])\n\n# Create a list of data with proper date objects\nweather_data = [\n    (1, datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\"), 10),\n    (2, datetime.strptime(\"2015-01-02\", \"%Y-%m-%d\"), 25),\n    (3, datetime.strptime(\"2015-01-03\", \"%Y-%m-%d\"), 20),\n    (4, datetime.strptime(\"2015-01-04\", \"%Y-%m-%d\"), 30)\n]\n\n# Convert the list to a DataFrame\ndf_weather = spark.createDataFrame(weather_data, schema=weather_schema)\n\n# Show the DataFrame\ndf_weather.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:28.344805Z","iopub.execute_input":"2024-10-10T02:47:28.345387Z","iopub.status.idle":"2024-10-10T02:47:29.035690Z","shell.execute_reply.started":"2024-10-10T02:47:28.345324Z","shell.execute_reply":"2024-10-10T02:47:29.034390Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"24/10/10 02:47:28 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+---+----------+-----------+\n| id|recordDate|temperature|\n+---+----------+-----------+\n|  1|2015-01-01|         10|\n|  2|2015-01-02|         25|\n|  3|2015-01-03|         20|\n|  4|2015-01-04|         30|\n+---+----------+-----------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import max,col\n\ndf_weather.alias(\"a\").join(df_weather.alias(\"b\"),col(\"a.recordDate\") > col(\"b.recordDate\"),\"inner\")\\\n    .groupBy(\"a.recordDate\",\"a.id\",\"a.temperature\").agg(max(col(\"b.recordDate\")).alias(\"prevDate\")).alias(\"d\")\\\n    .join(df_weather.alias(\"c\"),col(\"d.prevDate\") == col(\"c.recordDate\"),\"inner\")\\\n    .filter(\"c.temperature < d.temperature\").withColumn(\"prevDateID\", col(\"c.id\")).drop(\"c.id\") ","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:29.037104Z","iopub.execute_input":"2024-10-10T02:47:29.037602Z","iopub.status.idle":"2024-10-10T02:47:37.796547Z","shell.execute_reply.started":"2024-10-10T02:47:29.037548Z","shell.execute_reply":"2024-10-10T02:47:37.792757Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"+----------+---+-----------+----------+---+----------+-----------+----------+\n|recordDate| id|temperature|  prevDate| id|recordDate|temperature|prevDateID|\n+----------+---+-----------+----------+---+----------+-----------+----------+\n|2015-01-02|  2|         25|2015-01-01|  1|2015-01-01|         10|         1|\n|2015-01-04|  4|         30|2015-01-03|  3|2015-01-03|         20|         3|\n+----------+---+-----------+----------+---+----------+-----------+----------+","text/html":"<table border='1'>\n<tr><th>recordDate</th><th>id</th><th>temperature</th><th>prevDate</th><th>id</th><th>recordDate</th><th>temperature</th><th>prevDateID</th></tr>\n<tr><td>2015-01-02</td><td>2</td><td>25</td><td>2015-01-01</td><td>1</td><td>2015-01-01</td><td>10</td><td>1</td></tr>\n<tr><td>2015-01-04</td><td>4</td><td>30</td><td>2015-01-03</td><td>3</td><td>2015-01-03</td><td>20</td><td>3</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql import Window\nfrom pyspark.sql.functions import max,lag,col\n\nwin_spec = Window.orderBy(\"recordDate\")\n\ndf_weather.withColumn(\"prevDate\",lag(\"recordDate\").over(win_spec))\\\n        .withColumn(\"prevTemp\",lag(\"temperature\").over(win_spec))\\\n        .filter(\"prevTemp < temperature\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:37.804853Z","iopub.execute_input":"2024-10-10T02:47:37.805420Z","iopub.status.idle":"2024-10-10T02:47:39.176800Z","shell.execute_reply.started":"2024-10-10T02:47:37.805365Z","shell.execute_reply":"2024-10-10T02:47:39.175668Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"24/10/10 02:47:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n24/10/10 02:47:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"+---+----------+-----------+----------+--------+\n| id|recordDate|temperature|  prevDate|prevTemp|\n+---+----------+-----------+----------+--------+\n|  2|2015-01-02|         25|2015-01-01|      10|\n|  4|2015-01-04|         30|2015-01-03|      20|\n+---+----------+-----------+----------+--------+","text/html":"<table border='1'>\n<tr><th>id</th><th>recordDate</th><th>temperature</th><th>prevDate</th><th>prevTemp</th></tr>\n<tr><td>2</td><td>2015-01-02</td><td>25</td><td>2015-01-01</td><td>10</td></tr>\n<tr><td>4</td><td>2015-01-04</td><td>30</td><td>2015-01-03</td><td>20</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pdf = df_weather.toPandas()\npdf_a = pdf.copy()\npdf_b = pdf.copy()\npdf_cross = pd.merge(pdf_a,pdf_b,how=\"cross\")\npdf_filt = pdf_cross[pdf_cross[\"recordDate_x\"]>pdf_cross[\"recordDate_y\"]].sort_values(\"recordDate_x\")\npdf_filt = pdf_filt.groupby([\"recordDate_x\",\"temperature_x\",\"id_x\"]).agg({\"recordDate_y\":\"max\"}).reset_index()\n\npdf_filt = pd.merge(pdf_filt,pdf,left_on=\"recordDate_y\",right_on=\"recordDate\",how=\"inner\")\n\npdf_filt[pdf_filt[\"temperature_x\"]>pdf_filt[\"temperature\"]]","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:39.178388Z","iopub.execute_input":"2024-10-10T02:47:39.179559Z","iopub.status.idle":"2024-10-10T02:47:39.761957Z","shell.execute_reply.started":"2024-10-10T02:47:39.179504Z","shell.execute_reply":"2024-10-10T02:47:39.760772Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"  recordDate_x  temperature_x  id_x recordDate_y  id  recordDate  temperature\n0   2015-01-02             25     2   2015-01-01   1  2015-01-01           10\n2   2015-01-04             30     4   2015-01-03   3  2015-01-03           20","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>recordDate_x</th>\n      <th>temperature_x</th>\n      <th>id_x</th>\n      <th>recordDate_y</th>\n      <th>id</th>\n      <th>recordDate</th>\n      <th>temperature</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-02</td>\n      <td>25</td>\n      <td>2</td>\n      <td>2015-01-01</td>\n      <td>1</td>\n      <td>2015-01-01</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2015-01-04</td>\n      <td>30</td>\n      <td>4</td>\n      <td>2015-01-03</td>\n      <td>3</td>\n      <td>2015-01-03</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pdf = df_weather.toPandas()\n\npdf = pdf.sort_values(\"recordDate\")\n\npdf[\"prevTemp\"] = pdf[\"temperature\"].shift(1)\npdf[\"prevDate\"] = pdf[\"recordDate\"].shift(1)\n\npdf[pdf[\"temperature\"]>pdf[\"prevTemp\"]]","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:39.764443Z","iopub.execute_input":"2024-10-10T02:47:39.765137Z","iopub.status.idle":"2024-10-10T02:47:40.289463Z","shell.execute_reply.started":"2024-10-10T02:47:39.765092Z","shell.execute_reply":"2024-10-10T02:47:40.288103Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"   id  recordDate  temperature  prevTemp    prevDate\n1   2  2015-01-02           25      10.0  2015-01-01\n3   4  2015-01-04           30      20.0  2015-01-03","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>recordDate</th>\n      <th>temperature</th>\n      <th>prevTemp</th>\n      <th>prevDate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2015-01-02</td>\n      <td>25</td>\n      <td>10.0</td>\n      <td>2015-01-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2015-01-04</td>\n      <td>30</td>\n      <td>20.0</td>\n      <td>2015-01-03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Problem 10\n\nThere is a factory website that has several machines each running the same number of processes. Write a solution to find the average time each machine takes to complete a process.\n\nThe time to complete a process is the 'end' timestamp minus the 'start' timestamp. The average time is calculated by the total time to complete every process on the machine divided by the number of processes that were run.\n\nThe resulting table should have the machine_id along with the average time as processing_time, which should be rounded to 3 decimal places.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"Activity Table\").getOrCreate()\n\n# Define schema for the Activity table\nschema = StructType([\n    StructField(\"machine_id\", IntegerType(), True),\n    StructField(\"process_id\", IntegerType(), True),\n    StructField(\"activity_type\", StringType(), True),  # enum can be treated as StringType in PySpark\n    StructField(\"timestamp\", FloatType(), True)\n])\n\n# Create data for the DataFrame\ndata = [\n    (0, 0, 'start', 0.712),\n    (0, 0, 'end', 1.520),\n    (0, 1, 'start', 3.140),\n    (0, 1, 'end', 4.120),\n    (1, 0, 'start', 0.550),\n    (1, 0, 'end', 1.550),\n    (1, 1, 'start', 0.430),\n    (1, 1, 'end', 1.420),\n    (2, 0, 'start', 4.100),\n    (2, 0, 'end', 4.512),\n    (2, 1, 'start', 2.500),\n    (2, 1, 'end', 5.000)\n]\n\n# Create DataFrame\nactivity_df = spark.createDataFrame(data, schema)\n\n# Show DataFrame content\nactivity_df.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:40.291096Z","iopub.execute_input":"2024-10-10T02:47:40.291592Z","iopub.status.idle":"2024-10-10T02:47:40.978600Z","shell.execute_reply.started":"2024-10-10T02:47:40.291535Z","shell.execute_reply":"2024-10-10T02:47:40.977410Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"24/10/10 02:47:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+----------+----------+-------------+---------+\n|machine_id|process_id|activity_type|timestamp|\n+----------+----------+-------------+---------+\n|         0|         0|        start|    0.712|\n|         0|         0|          end|     1.52|\n|         0|         1|        start|     3.14|\n|         0|         1|          end|     4.12|\n|         1|         0|        start|     0.55|\n|         1|         0|          end|     1.55|\n|         1|         1|        start|     0.43|\n|         1|         1|          end|     1.42|\n|         2|         0|        start|      4.1|\n|         2|         0|          end|    4.512|\n|         2|         1|        start|      2.5|\n|         2|         1|          end|      5.0|\n+----------+----------+-------------+---------+\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import avg,round\ndf = activity_df\n\ndf.alias('a')\\\n        .join(df.alias('b'),(col(\"a.machine_id\") == col(\"b.machine_id\") ) \\\n                & (col(\"a.process_id\") == col(\"b.process_id\") )\\\n                & (col(\"a.activity_type\") == 'start' )\\\n                & (col(\"b.activity_type\") == 'end' )\\\n            ,\"inner\")\\\n        .groupBy(\"a.machine_id\")\\\n        .agg(avg(col(\"b.timestamp\")-col(\"a.timestamp\")).alias(\"processing_time\"))\\\n        .select(\"a.machine_id\",round(col(\"processing_time\"),3).alias(\"processing_time\"))","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:40.979910Z","iopub.execute_input":"2024-10-10T02:47:40.980508Z","iopub.status.idle":"2024-10-10T02:47:44.016013Z","shell.execute_reply.started":"2024-10-10T02:47:40.980452Z","shell.execute_reply":"2024-10-10T02:47:44.014595Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"+----------+---------------+\n|machine_id|processing_time|\n+----------+---------------+\n|         1|          0.995|\n|         2|          1.456|\n|         0|          0.894|\n+----------+---------------+","text/html":"<table border='1'>\n<tr><th>machine_id</th><th>processing_time</th></tr>\n<tr><td>1</td><td>0.995</td></tr>\n<tr><td>2</td><td>1.456</td></tr>\n<tr><td>0</td><td>0.894</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"code","source":"from pyspark.sql import Window\nfrom pyspark.sql.functions import lead\n\nwin_spec = Window.partitionBy(\"machine_id\",\"process_id\").orderBy(\"timestamp\")\ndf.withColumn(\"end_timestamp\",lead(\"timestamp\").over(win_spec)).orderBy(\"machine_id\",\"process_id\")\\\n    .filter(\"activity_type == 'start'\")\\\n    .groupBy(\"machine_id\")\\\n    .agg(round(avg(col(\"end_timestamp\")-col(\"timestamp\")),3).alias(\"process_time\"))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T02:47:44.017856Z","iopub.execute_input":"2024-10-10T02:47:44.018390Z","iopub.status.idle":"2024-10-10T02:47:46.040318Z","shell.execute_reply.started":"2024-10-10T02:47:44.018333Z","shell.execute_reply":"2024-10-10T02:47:46.039125Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"+----------+------------+\n|machine_id|process_time|\n+----------+------------+\n|         1|       0.995|\n|         2|       1.456|\n|         0|       0.894|\n+----------+------------+","text/html":"<table border='1'>\n<tr><th>machine_id</th><th>process_time</th></tr>\n<tr><td>1</td><td>0.995</td></tr>\n<tr><td>2</td><td>1.456</td></tr>\n<tr><td>0</td><td>0.894</td></tr>\n</table>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pdf = df.toPandas()\npdf_merge = pdf.merge(pdf,on=['machine_id','process_id'],how='inner',suffixes=('_left','_right')) # to set custom suffixes\npdf_filt = pdf_merge[ (pdf_merge[\"activity_type_left\"]=='start') & (pdf_merge[\"activity_type_right\"]=='end') ].copy()\npdf_filt[\"diff_timestamp\"] = pdf_filt[\"timestamp_right\"] - pdf_filt[\"timestamp_left\"]\npdf_filt = pdf_filt.groupby(\"machine_id\").agg({\"diff_timestamp\":\"mean\"}).reset_index()\npdf_filt[\"diff_timestamp\"] = pdf_filt[\"diff_timestamp\"].round(2)\npdf_filt","metadata":{"execution":{"iopub.status.busy":"2024-10-10T03:45:23.900110Z","iopub.execute_input":"2024-10-10T03:45:23.900640Z","iopub.status.idle":"2024-10-10T03:45:24.342484Z","shell.execute_reply.started":"2024-10-10T03:45:23.900594Z","shell.execute_reply":"2024-10-10T03:45:24.341170Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"   machine_id  diff_timestamp\n0           0            0.89\n1           1            0.99\n2           2            1.46","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>machine_id</th>\n      <th>diff_timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.89</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0.99</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.46</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}