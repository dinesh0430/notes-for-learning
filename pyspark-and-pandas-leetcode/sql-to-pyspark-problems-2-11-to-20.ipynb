{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n!pip install pyspark\n\nfrom pyspark.sql import SparkSession\n# Create a SparkSession (without a specified name)\nspark = SparkSession.builder.getOrCreate()\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True) #for simple calls and better display\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-01-31T17:17:16.878519Z","iopub.execute_input":"2025-01-31T17:17:16.878888Z","iopub.status.idle":"2025-01-31T17:18:10.840212Z","shell.execute_reply.started":"2025-01-31T17:17:16.878855Z","shell.execute_reply":"2025-01-31T17:18:10.837993Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\nBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849765 sha256=637049e986747eafb73f130b715dcf2c1198c83c08408793d14221e382b4cd50\n  Stored in directory: /root/.cache/pip/wheels/d9/1c/98/31e395a42d1735d18d42124971ecbbade844b50bb9845b6f4a\nSuccessfully built pyspark\nInstalling collected packages: pyspark\nSuccessfully installed pyspark-3.5.4\n","output_type":"stream"},{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/31 17:18:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Understanding some aspects of Pandas groupby","metadata":{}},{"cell_type":"code","source":"# testing pandas groupby functionality\n\nimport pandas as pd\ntechnologies   = ({\n    'Courses':[\"Spark\",\"PySpark\",\"Hadoop\",\"Python\",\"Pandas\",\"Hadoop\",\"Spark\",\"Python\",\"NA\"],\n    'Fee' :[22000,25000,23000,24000,26000,25000,25000,22000,1500],\n    'Duration':['30days','50days','55days','40days','60days','35days','30days','50days','40days'],\n    'Discount':[1000,2300,1000,1200,2500,None,1400,1600,0]\n          })\ndf = pd.DataFrame(technologies)\nprint(\"Create DataFrame:\\n\", df)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:10.843081Z","iopub.execute_input":"2025-01-31T17:18:10.845597Z","iopub.status.idle":"2025-01-31T17:18:10.878749Z","shell.execute_reply.started":"2025-01-31T17:18:10.845533Z","shell.execute_reply":"2025-01-31T17:18:10.877360Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Create DataFrame:\n    Courses    Fee Duration  Discount\n0    Spark  22000   30days    1000.0\n1  PySpark  25000   50days    2300.0\n2   Hadoop  23000   55days    1000.0\n3   Python  24000   40days    1200.0\n4   Pandas  26000   60days    2500.0\n5   Hadoop  25000   35days       NaN\n6    Spark  25000   30days    1400.0\n7   Python  22000   50days    1600.0\n8       NA   1500   40days       0.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df.groupby(\"Courses\").sum().reset_index() # here the sum is applied on the all the remaining cols","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:10.880137Z","iopub.execute_input":"2025-01-31T17:18:10.881741Z","iopub.status.idle":"2025-01-31T17:18:10.924713Z","shell.execute_reply.started":"2025-01-31T17:18:10.881669Z","shell.execute_reply":"2025-01-31T17:18:10.923168Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   Courses    Fee      Duration  Discount\n0   Hadoop  48000  55days35days    1000.0\n1       NA   1500        40days       0.0\n2   Pandas  26000        60days    2500.0\n3  PySpark  25000        50days    2300.0\n4   Python  46000  40days50days    2800.0\n5    Spark  47000  30days30days    2400.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Courses</th>\n      <th>Fee</th>\n      <th>Duration</th>\n      <th>Discount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hadoop</td>\n      <td>48000</td>\n      <td>55days35days</td>\n      <td>1000.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NA</td>\n      <td>1500</td>\n      <td>40days</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pandas</td>\n      <td>26000</td>\n      <td>60days</td>\n      <td>2500.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PySpark</td>\n      <td>25000</td>\n      <td>50days</td>\n      <td>2300.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Python</td>\n      <td>46000</td>\n      <td>40days50days</td>\n      <td>2800.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Spark</td>\n      <td>47000</td>\n      <td>30days30days</td>\n      <td>2400.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df.groupby(\"Courses\")[\"Fee\"].sum().reset_index() # here the sum is only applied on the column mentioned in []","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:10.927855Z","iopub.execute_input":"2025-01-31T17:18:10.928326Z","iopub.status.idle":"2025-01-31T17:18:10.943326Z","shell.execute_reply.started":"2025-01-31T17:18:10.928280Z","shell.execute_reply":"2025-01-31T17:18:10.942086Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"   Courses    Fee\n0   Hadoop  48000\n1       NA   1500\n2   Pandas  26000\n3  PySpark  25000\n4   Python  46000\n5    Spark  47000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Courses</th>\n      <th>Fee</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hadoop</td>\n      <td>48000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NA</td>\n      <td>1500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pandas</td>\n      <td>26000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PySpark</td>\n      <td>25000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Python</td>\n      <td>46000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Spark</td>\n      <td>47000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.groupby(\"Courses\")[[\"Fee\",\"Discount\"]].sum().reset_index() # here the sum is only applied on the columns mentioned in []","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:10.944924Z","iopub.execute_input":"2025-01-31T17:18:10.945622Z","iopub.status.idle":"2025-01-31T17:18:10.967194Z","shell.execute_reply.started":"2025-01-31T17:18:10.945574Z","shell.execute_reply":"2025-01-31T17:18:10.966212Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   Courses    Fee  Discount\n0   Hadoop  48000    1000.0\n1       NA   1500       0.0\n2   Pandas  26000    2500.0\n3  PySpark  25000    2300.0\n4   Python  46000    2800.0\n5    Spark  47000    2400.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Courses</th>\n      <th>Fee</th>\n      <th>Discount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hadoop</td>\n      <td>48000</td>\n      <td>1000.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NA</td>\n      <td>1500</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pandas</td>\n      <td>26000</td>\n      <td>2500.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PySpark</td>\n      <td>25000</td>\n      <td>2300.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Python</td>\n      <td>46000</td>\n      <td>2800.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Spark</td>\n      <td>47000</td>\n      <td>2400.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df.groupby(\"Courses\").agg({\"Fee\":\"sum\"}).reset_index() \n# note the use of dict's {} in the agg function","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:10.968296Z","iopub.execute_input":"2025-01-31T17:18:10.968675Z","iopub.status.idle":"2025-01-31T17:18:10.993846Z","shell.execute_reply.started":"2025-01-31T17:18:10.968634Z","shell.execute_reply":"2025-01-31T17:18:10.992619Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Courses    Fee\n0   Hadoop  48000\n1       NA   1500\n2   Pandas  26000\n3  PySpark  25000\n4   Python  46000\n5    Spark  47000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Courses</th>\n      <th>Fee</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hadoop</td>\n      <td>48000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NA</td>\n      <td>1500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pandas</td>\n      <td>26000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PySpark</td>\n      <td>25000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Python</td>\n      <td>46000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Spark</td>\n      <td>47000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df.groupby(\"Courses\").agg(cust_col_name=(\"Fee\",\"sum\")).reset_index() \n# note the use of () in the agg function to set the name","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:10.995293Z","iopub.execute_input":"2025-01-31T17:18:10.995674Z","iopub.status.idle":"2025-01-31T17:18:11.023804Z","shell.execute_reply.started":"2025-01-31T17:18:10.995633Z","shell.execute_reply":"2025-01-31T17:18:11.022337Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   Courses  cust_col_name\n0   Hadoop          48000\n1       NA           1500\n2   Pandas          26000\n3  PySpark          25000\n4   Python          46000\n5    Spark          47000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Courses</th>\n      <th>cust_col_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hadoop</td>\n      <td>48000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NA</td>\n      <td>1500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Pandas</td>\n      <td>26000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PySpark</td>\n      <td>25000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Python</td>\n      <td>46000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Spark</td>\n      <td>47000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"# Problem 11","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"EmployeeBonus\").getOrCreate()\n\n# Define schema for Employee DataFrame\nemployee_schema = StructType([\n    StructField(\"empId\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"supervisor\", IntegerType(), True),\n    StructField(\"salary\", IntegerType(), True)\n])\n\n# Sample data for Employee\nemployee_data = [\n    (3, \"Brad\", None, 4000),\n    (1, \"John\", 3, 1000),\n    (2, \"Dan\", 3, 2000),\n    (4, \"Thomas\", 3, 4000)\n]\n\n# Create Employee DataFrame\nemployee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n\nbonus_schema = StructType([\n    StructField(\"empId\", IntegerType(), True),\n    StructField(\"bonus\", IntegerType(), True)\n])\n\n# Sample data for Bonus\nbonus_data = [\n    (2, 500),\n    (4, 2000)\n]\n\n# Create Bonus DataFrame\nbonus_df = spark.createDataFrame(bonus_data, schema=bonus_schema)\nbonus_df.show()\nemployee_df.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:11.025365Z","iopub.execute_input":"2025-01-31T17:18:11.026400Z","iopub.status.idle":"2025-01-31T17:18:19.935766Z","shell.execute_reply.started":"2025-01-31T17:18:11.026352Z","shell.execute_reply":"2025-01-31T17:18:19.934207Z"},"trusted":true},"outputs":[{"name":"stderr","text":"25/01/31 17:18:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-----+-----+\n|empId|bonus|\n+-----+-----+\n|    2|  500|\n|    4| 2000|\n+-----+-----+\n\n+-----+------+----------+------+\n|empId|  name|supervisor|salary|\n+-----+------+----------+------+\n|    3|  Brad|      NULL|  4000|\n|    1|  John|         3|  1000|\n|    2|   Dan|         3|  2000|\n|    4|Thomas|         3|  4000|\n+-----+------+----------+------+\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import col\nemployee_df.join(bonus_df,bonus_df.empId == employee_df.empId,\"left\").filter( (col(\"bonus\") < 1000) | (col(\"bonus\").isNull()))","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:19.940308Z","iopub.execute_input":"2025-01-31T17:18:19.940777Z","iopub.status.idle":"2025-01-31T17:18:25.909172Z","shell.execute_reply.started":"2025-01-31T17:18:19.940729Z","shell.execute_reply":"2025-01-31T17:18:25.907086Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"+-----+----+----------+------+-----+-----+\n|empId|name|supervisor|salary|empId|bonus|\n+-----+----+----------+------+-----+-----+\n|    3|Brad|      NULL|  4000| NULL| NULL|\n|    1|John|         3|  1000| NULL| NULL|\n|    2| Dan|         3|  2000|    2|  500|\n+-----+----+----------+------+-----+-----+","text/html":"<table border='1'>\n<tr><th>empId</th><th>name</th><th>supervisor</th><th>salary</th><th>empId</th><th>bonus</th></tr>\n<tr><td>3</td><td>Brad</td><td>NULL</td><td>4000</td><td>NULL</td><td>NULL</td></tr>\n<tr><td>1</td><td>John</td><td>3</td><td>1000</td><td>NULL</td><td>NULL</td></tr>\n<tr><td>2</td><td>Dan</td><td>3</td><td>2000</td><td>2</td><td>500</td></tr>\n</table>\n"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\npd_emp = employee_df.toPandas()\npd_bonus = bonus_df.toPandas()\n\npd_res = pd_emp.merge(pd_bonus,on=\"empId\",how=\"left\")\npd_res[ (pd_res[\"bonus\"]<1000) | (pd_res[\"bonus\"].isna()) ]","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:25.938805Z","iopub.execute_input":"2025-01-31T17:18:25.944754Z","iopub.status.idle":"2025-01-31T17:18:28.986355Z","shell.execute_reply.started":"2025-01-31T17:18:25.944678Z","shell.execute_reply":"2025-01-31T17:18:28.983821Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   empId  name  supervisor  salary  bonus\n0      3  Brad         NaN    4000    NaN\n1      1  John         3.0    1000    NaN\n2      2   Dan         3.0    2000  500.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>empId</th>\n      <th>name</th>\n      <th>supervisor</th>\n      <th>salary</th>\n      <th>bonus</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>Brad</td>\n      <td>NaN</td>\n      <td>4000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>John</td>\n      <td>3.0</td>\n      <td>1000</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Dan</td>\n      <td>3.0</td>\n      <td>2000</td>\n      <td>500.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Problem 12\nEach student from the Students table takes every course from the Subjects table.\nEach row of this table indicates that a student with ID student_id attended the exam of subject_name. \nWrite a solution to find the number of times each student attended each exam.\nReturn the result table ordered by student_id and subject_name.\n\nThe result table should contain all students and all subjects.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"Students Example\").getOrCreate()\n\n# Create the Students DataFrame\nstudents_data = [\n    (1, \"Alice\"),\n    (2, \"Bob\"),\n    (13, \"John\"),\n    (6, \"Alex\")\n]\n\nstudents_columns = [\"student_id\", \"student_name\"]\nst_df = spark.createDataFrame(students_data, schema=students_columns)\n\n# Create the Subjects DataFrame\nsubjects_data = [\n    (\"Math\",),\n    (\"Physics\",),\n    (\"Programming\",)\n]\n\nsubjects_columns = [\"subject_name\"]\nsub_df = spark.createDataFrame(subjects_data, schema=subjects_columns)\n\n# Create the Examinations DataFrame\nexaminations_data = [\n    (1, \"Math\"),\n    (1, \"Physics\"),\n    (1, \"Programming\"),\n    (2, \"Programming\"),\n    (1, \"Physics\"),\n    (1, \"Math\"),\n    (13, \"Math\"),\n    (13, \"Programming\"),\n    (13, \"Physics\"),\n    (2, \"Math\"),\n    (1, \"Math\")\n]\n\nexaminations_columns = [\"student_id\", \"subject_name\"]\nexam_df = spark.createDataFrame(examinations_data, schema=examinations_columns)\n\n\nst_df\nsub_df\nexam_df","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:28.995538Z","iopub.execute_input":"2025-01-31T17:18:29.001082Z","iopub.status.idle":"2025-01-31T17:18:31.503776Z","shell.execute_reply.started":"2025-01-31T17:18:29.000872Z","shell.execute_reply":"2025-01-31T17:18:31.502068Z"},"trusted":true},"outputs":[{"name":"stderr","text":"25/01/31 17:18:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"+----------+------------+\n|student_id|subject_name|\n+----------+------------+\n|         1|        Math|\n|         1|     Physics|\n|         1| Programming|\n|         2| Programming|\n|         1|     Physics|\n|         1|        Math|\n|        13|        Math|\n|        13| Programming|\n|        13|     Physics|\n|         2|        Math|\n|         1|        Math|\n+----------+------------+","text/html":"<table border='1'>\n<tr><th>student_id</th><th>subject_name</th></tr>\n<tr><td>1</td><td>Math</td></tr>\n<tr><td>1</td><td>Physics</td></tr>\n<tr><td>1</td><td>Programming</td></tr>\n<tr><td>2</td><td>Programming</td></tr>\n<tr><td>1</td><td>Physics</td></tr>\n<tr><td>1</td><td>Math</td></tr>\n<tr><td>13</td><td>Math</td></tr>\n<tr><td>13</td><td>Programming</td></tr>\n<tr><td>13</td><td>Physics</td></tr>\n<tr><td>2</td><td>Math</td></tr>\n<tr><td>1</td><td>Math</td></tr>\n</table>\n"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"#### TESTING MYSELF after long time, since I don't completely remember all the topics","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import count\n# st_df\n# sub_df\n# exam_df\nres = st_df.join(sub_df,how=\"cross\")\nst_sub_cross = res.orderBy(\"student_id\",\"subject_name\")\nst_sub_cross.show()\n\n# Below both give the same result\n# exam_grp =exam_df.groupBy(\"student_id\",\"subject_name\").count()\nexam_grp =exam_df.groupBy(\"student_id\",\"subject_name\").agg(count(col(\"*\")).alias(\"cnt\"))\n\nexam_grp.show()\n\n# the below is wrong because col() does not support dot notation without alias\n# res = st_sub_cross.join(exam_grp,col(\"exam_grp.student_id\")==col(\"st_sub_cross.student_id\"),how='left')\n\n# the below is RIGHT and works because col() supports dot notation WITH alias\n# res = st_sub_cross.alias('st_sub_cross').join(exam_grp.alias('exam_grp'),col(\"exam_grp.student_id\")==col(\"st_sub_cross.student_id\"),how='left')\n\n# the below works fine without any issue, but we won't be able to have access to ambiguous column names using this method\n# res = st_sub_cross.join(exam_grp,\\\n#                         (exam_grp.student_id==st_sub_cross.student_id) & ( exam_grp.subject_name==st_sub_cross.subject_name ),\\\n#                         how='left')\n\n# The below won't work as we can't access ambiguous column names present in both sides of a join using the table name .(dot) prefix\n# If we want to access the ambiguous column names, then we HAVE to alias them before hand\n# # res = res.selectExpr(\"student_id as st_sub_student_id\",\"exam_grp.student_id\")\n\n\nres = st_sub_cross.alias('st_sub_cross').join(exam_grp.alias('exam_grp'),\\\n                        (col(\"exam_grp.student_id\")==col(\"st_sub_cross.student_id\")) & ( col(\"exam_grp.subject_name\")==col(\"st_sub_cross.subject_name\") ),\\\n                        how='left')\n\n# in this below command we can specifically see that using alias gives us special access to ambiguous column names\nres.selectExpr(\"st_sub_cross.*\",\"COALESCE(cnt, 0) AS cnt_1\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:18:31.505476Z","iopub.execute_input":"2025-01-31T17:18:31.506008Z","iopub.status.idle":"2025-01-31T17:18:40.916789Z","shell.execute_reply.started":"2025-01-31T17:18:31.505948Z","shell.execute_reply":"2025-01-31T17:18:40.915726Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+------------+------------+\n|student_id|student_name|subject_name|\n+----------+------------+------------+\n|         1|       Alice|        Math|\n|         1|       Alice|     Physics|\n|         1|       Alice| Programming|\n|         2|         Bob|        Math|\n|         2|         Bob|     Physics|\n|         2|         Bob| Programming|\n|         6|        Alex|        Math|\n|         6|        Alex|     Physics|\n|         6|        Alex| Programming|\n|        13|        John|        Math|\n|        13|        John|     Physics|\n|        13|        John| Programming|\n+----------+------------+------------+\n\n+----------+------------+---+\n|student_id|subject_name|cnt|\n+----------+------------+---+\n|         1|     Physics|  2|\n|         1|        Math|  3|\n|         1| Programming|  1|\n|         2| Programming|  1|\n|        13|        Math|  1|\n|        13| Programming|  1|\n|         2|        Math|  1|\n|        13|     Physics|  1|\n+----------+------------+---+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+------------+------------+-----+\n|student_id|student_name|subject_name|cnt_1|\n+----------+------------+------------+-----+\n|         1|       Alice|        Math|    3|\n|         1|       Alice|     Physics|    2|\n|         1|       Alice| Programming|    1|\n|         2|         Bob|        Math|    1|\n|         2|         Bob|     Physics|    0|\n|         2|         Bob| Programming|    1|\n|        13|        John|        Math|    1|\n|        13|        John|     Physics|    1|\n|        13|        John| Programming|    1|\n|         6|        Alex|        Math|    0|\n|         6|        Alex|     Physics|    0|\n|         6|        Alex| Programming|    0|\n+----------+------------+------------+-----+\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import when,sum\n\nst_sub = st_df.crossJoin(sub_df)\nst_sub.orderBy(\"student_id\",\"subject_name\")\n\ndf = st_sub.alias('a').join( exam_df.alias('b'), (exam_df.student_id == st_sub.student_id ) & ( exam_df.subject_name == st_sub.subject_name ),\"left\")\\\n    .drop(exam_df.student_id)\n\ndf_2 = df.select(\"*\", when(col(\"b.subject_name\").isNull(), 0).otherwise(1).alias(\"cnt\"))\ndf_2.show()\ndf_3 = df.withColumn(\"cnt\",when(col(\"b.subject_name\").isNull(),0).otherwise(1))\ndf_3.show()\ndf_3.groupBy(col(\"student_id\"),col(\"a.subject_name\")).agg(sum(\"cnt\")).orderBy(\"student_id\",\"subject_name\")","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:40.918038Z","iopub.execute_input":"2025-01-31T17:18:40.918459Z","iopub.status.idle":"2025-01-31T17:18:54.180725Z","shell.execute_reply.started":"2025-01-31T17:18:40.918414Z","shell.execute_reply":"2025-01-31T17:18:54.179499Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+------------+------------+------------+---+\n|student_id|student_name|subject_name|subject_name|cnt|\n+----------+------------+------------+------------+---+\n|         1|       Alice|        Math|        Math|  1|\n|         1|       Alice|        Math|        Math|  1|\n|         1|       Alice|        Math|        Math|  1|\n|         1|       Alice|     Physics|     Physics|  1|\n|         1|       Alice|     Physics|     Physics|  1|\n|         1|       Alice| Programming| Programming|  1|\n|         2|         Bob|        Math|        Math|  1|\n|         2|         Bob|     Physics|        NULL|  0|\n|         2|         Bob| Programming| Programming|  1|\n|        13|        John|        Math|        Math|  1|\n|        13|        John|     Physics|     Physics|  1|\n|        13|        John| Programming| Programming|  1|\n|         6|        Alex|        Math|        NULL|  0|\n|         6|        Alex|     Physics|        NULL|  0|\n|         6|        Alex| Programming|        NULL|  0|\n+----------+------------+------------+------------+---+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+------------+------------+------------+---+\n|student_id|student_name|subject_name|subject_name|cnt|\n+----------+------------+------------+------------+---+\n|         1|       Alice|        Math|        Math|  1|\n|         1|       Alice|        Math|        Math|  1|\n|         1|       Alice|        Math|        Math|  1|\n|         1|       Alice|     Physics|     Physics|  1|\n|         1|       Alice|     Physics|     Physics|  1|\n|         1|       Alice| Programming| Programming|  1|\n|         2|         Bob|        Math|        Math|  1|\n|         2|         Bob|     Physics|        NULL|  0|\n|         2|         Bob| Programming| Programming|  1|\n|        13|        John|        Math|        Math|  1|\n|        13|        John|     Physics|     Physics|  1|\n|        13|        John| Programming| Programming|  1|\n|         6|        Alex|        Math|        NULL|  0|\n|         6|        Alex|     Physics|        NULL|  0|\n|         6|        Alex| Programming|        NULL|  0|\n+----------+------------+------------+------------+---+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"+----------+------------+--------+\n|student_id|subject_name|sum(cnt)|\n+----------+------------+--------+\n|         1|        Math|       3|\n|         1|     Physics|       2|\n|         1| Programming|       1|\n|         2|        Math|       1|\n|         2|     Physics|       0|\n|         2| Programming|       1|\n|         6|        Math|       0|\n|         6|     Physics|       0|\n|         6| Programming|       0|\n|        13|        Math|       1|\n|        13|     Physics|       1|\n|        13| Programming|       1|\n+----------+------------+--------+","text/html":"<table border='1'>\n<tr><th>student_id</th><th>subject_name</th><th>sum(cnt)</th></tr>\n<tr><td>1</td><td>Math</td><td>3</td></tr>\n<tr><td>1</td><td>Physics</td><td>2</td></tr>\n<tr><td>1</td><td>Programming</td><td>1</td></tr>\n<tr><td>2</td><td>Math</td><td>1</td></tr>\n<tr><td>2</td><td>Physics</td><td>0</td></tr>\n<tr><td>2</td><td>Programming</td><td>1</td></tr>\n<tr><td>6</td><td>Math</td><td>0</td></tr>\n<tr><td>6</td><td>Physics</td><td>0</td></tr>\n<tr><td>6</td><td>Programming</td><td>0</td></tr>\n<tr><td>13</td><td>Math</td><td>1</td></tr>\n<tr><td>13</td><td>Physics</td><td>1</td></tr>\n<tr><td>13</td><td>Programming</td><td>1</td></tr>\n</table>\n"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from pyspark.sql.functions import col, count\n\nst_sub = st_df.crossJoin(sub_df).orderBy(\"student_id\", \"subject_name\")\n\ndf = st_sub.alias('a').join(exam_df.alias('b'),\n                            (col(\"a.student_id\") == col(\"b.student_id\")) & (col(\"a.subject_name\") == col(\"b.subject_name\")),\n                            \"left\")\n\nresult = df.groupBy(\"a.student_id\", \"a.subject_name\").agg(count(col(\"b.subject_name\")).alias(\"attendance_count\")).orderBy(\"student_id\", \"subject_name\")\nresult","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:18:54.182775Z","iopub.execute_input":"2025-01-31T17:18:54.183242Z","iopub.status.idle":"2025-01-31T17:19:00.585724Z","shell.execute_reply.started":"2025-01-31T17:18:54.183192Z","shell.execute_reply":"2025-01-31T17:19:00.582644Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"+----------+------------+----------------+\n|student_id|subject_name|attendance_count|\n+----------+------------+----------------+\n|         1|        Math|               3|\n|         1|     Physics|               2|\n|         1| Programming|               1|\n|         2|        Math|               1|\n|         2|     Physics|               0|\n|         2| Programming|               1|\n|         6|        Math|               0|\n|         6|     Physics|               0|\n|         6| Programming|               0|\n|        13|        Math|               1|\n|        13|     Physics|               1|\n|        13| Programming|               1|\n+----------+------------+----------------+","text/html":"<table border='1'>\n<tr><th>student_id</th><th>subject_name</th><th>attendance_count</th></tr>\n<tr><td>1</td><td>Math</td><td>3</td></tr>\n<tr><td>1</td><td>Physics</td><td>2</td></tr>\n<tr><td>1</td><td>Programming</td><td>1</td></tr>\n<tr><td>2</td><td>Math</td><td>1</td></tr>\n<tr><td>2</td><td>Physics</td><td>0</td></tr>\n<tr><td>2</td><td>Programming</td><td>1</td></tr>\n<tr><td>6</td><td>Math</td><td>0</td></tr>\n<tr><td>6</td><td>Physics</td><td>0</td></tr>\n<tr><td>6</td><td>Programming</td><td>0</td></tr>\n<tr><td>13</td><td>Math</td><td>1</td></tr>\n<tr><td>13</td><td>Physics</td><td>1</td></tr>\n<tr><td>13</td><td>Programming</td><td>1</td></tr>\n</table>\n"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"st_pdf = st_df.toPandas()\nsub_pdf = sub_df.toPandas()\nexam_pdf = exam_df.toPandas()\n\nst_sub_pdf = st_pdf.merge(sub_pdf,how=\"cross\")\n\nexam_pdf.rename(columns={\"student_id\":\"ex_student_id\",\"subject_name\":\"ex_subject_name\"},inplace=True)\n\npdf = pd.merge(st_sub_pdf,exam_pdf, \n                       left_on=[\"student_id\", \"subject_name\"], \n                       right_on=[\"ex_student_id\", \"ex_subject_name\"], \n                       how=\"left\")\n\npdf.groupby([\"student_id\",\"student_name\",\"subject_name\"]).agg({\"ex_subject_name\":\"count\"}).rename(columns={\"ex_subject_name\":\"attended_exams\"}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:19:00.590833Z","iopub.execute_input":"2025-01-31T17:19:00.593265Z","iopub.status.idle":"2025-01-31T17:19:02.084806Z","shell.execute_reply.started":"2025-01-31T17:19:00.593206Z","shell.execute_reply":"2025-01-31T17:19:02.083840Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"    student_id student_name subject_name  attended_exams\n0            1        Alice         Math               3\n1            1        Alice      Physics               2\n2            1        Alice  Programming               1\n3            2          Bob         Math               1\n4            2          Bob      Physics               0\n5            2          Bob  Programming               1\n6            6         Alex         Math               0\n7            6         Alex      Physics               0\n8            6         Alex  Programming               0\n9           13         John         Math               1\n10          13         John      Physics               1\n11          13         John  Programming               1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>student_id</th>\n      <th>student_name</th>\n      <th>subject_name</th>\n      <th>attended_exams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Alice</td>\n      <td>Math</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Alice</td>\n      <td>Physics</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Alice</td>\n      <td>Programming</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>Bob</td>\n      <td>Math</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Bob</td>\n      <td>Physics</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2</td>\n      <td>Bob</td>\n      <td>Programming</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>Alex</td>\n      <td>Math</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6</td>\n      <td>Alex</td>\n      <td>Physics</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>6</td>\n      <td>Alex</td>\n      <td>Programming</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>13</td>\n      <td>John</td>\n      <td>Math</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>13</td>\n      <td>John</td>\n      <td>Physics</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>13</td>\n      <td>John</td>\n      <td>Programming</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# Problem 13\nWrite a solution to find managers with at least five direct reports.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Create Spark session\nspark = SparkSession.builder \\\n    .appName(\"Employee Manager Example\") \\\n    .getOrCreate()\n\n# Define the schema for the Employee table\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"department\", StringType(), True),\n    StructField(\"managerId\", IntegerType(), True)\n])\n\n# Create the data for the Employee table\ndata = [\n    (101, \"John\", \"A\", None),\n    (102, \"Dan\", \"A\", 101),\n    (103, \"James\", \"A\", 101),\n    (104, \"Amy\", \"A\", 101),\n    (105, \"Anne\", \"A\", 101),\n    (106, \"Ron\", \"B\", 101)\n]\n\n# Create DataFrame using the schema and data\nemployee_df = spark.createDataFrame(data, schema)\n\n# Show the DataFrame (to see if it's created correctly)\nemployee_df.show()\n","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:19:02.087013Z","iopub.execute_input":"2025-01-31T17:19:02.087486Z","iopub.status.idle":"2025-01-31T17:19:02.586851Z","shell.execute_reply.started":"2025-01-31T17:19:02.087435Z","shell.execute_reply":"2025-01-31T17:19:02.583765Z"},"trusted":true},"outputs":[{"name":"stderr","text":"25/01/31 17:19:02 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+---+-----+----------+---------+\n| id| name|department|managerId|\n+---+-----+----------+---------+\n|101| John|         A|     NULL|\n|102|  Dan|         A|      101|\n|103|James|         A|      101|\n|104|  Amy|         A|      101|\n|105| Anne|         A|      101|\n|106|  Ron|         B|      101|\n+---+-----+----------+---------+\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"df = employee_df\ndf_gp = df.groupby(\"managerId\").count().filter(\"count >= 5\")\ndf.alias('a').join(df_gp.alias('b'),col(\"b.managerId\") == col(\"a.id\"),\"inner\").select(\"name\")","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:19:02.588406Z","iopub.execute_input":"2025-01-31T17:19:02.591155Z","iopub.status.idle":"2025-01-31T17:19:05.051150Z","shell.execute_reply.started":"2025-01-31T17:19:02.591078Z","shell.execute_reply":"2025-01-31T17:19:05.050078Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"+----+\n|name|\n+----+\n|John|\n+----+","text/html":"<table border='1'>\n<tr><th>name</th></tr>\n<tr><td>John</td></tr>\n</table>\n"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"pdf = employee_df.toPandas()\npdf_gp = pdf.groupby(\"managerId\").size().reset_index(name=\"cnt\") # this will create the new column with the given name\n# .rename(columns={\"0\":\"cnt\"})\npdf_gp = pdf_gp[pdf_gp[\"cnt\"]>=5]\nprint(pdf_gp.columns)\n\npdf.merge(pdf_gp,left_on=\"id\",right_on=\"managerId\",how='inner')[\"name\"].to_frame()","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:19:05.053077Z","iopub.execute_input":"2025-01-31T17:19:05.053559Z","iopub.status.idle":"2025-01-31T17:19:05.525853Z","shell.execute_reply.started":"2025-01-31T17:19:05.053512Z","shell.execute_reply":"2025-01-31T17:19:05.524904Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Index(['managerId', 'cnt'], dtype='object')\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   name\n0  John","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>John</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Problem 14\n\nThe confirmation rate of a user is the number of 'confirmed' messages divided by the total number of requested confirmation messages. The confirmation rate of a user that did not request any confirmation messages is 0. Round the confirmation rate to two decimal places.\n\nWrite a solution to find the confirmation rate of each user.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType\nfrom datetime import datetime\n\n# Create a Spark session\nspark = SparkSession.builder \\\n    .appName(\"Signup Confirmations\") \\\n    .getOrCreate()\n\n# Schema for the Signups table\nsignups_schema = StructType([\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"time_stamp\", TimestampType(), True)\n])\n\n# Data for the Signups table (converting strings to datetime objects)\nsignups_data = [\n    (3, datetime(2020, 3, 21, 10, 16, 13)),\n    (7, datetime(2020, 1, 4, 13, 57, 59)),\n    (2, datetime(2020, 7, 29, 23, 9, 44)),\n    (6, datetime(2020, 12, 9, 10, 39, 37))\n]\n\n# Create Signups DataFrame\nsignups_df = spark.createDataFrame(signups_data, schema=signups_schema)\n\n# Schema for the Confirmations table\nconfirmations_schema = StructType([\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"time_stamp\", TimestampType(), True),\n    StructField(\"action\", StringType(), True)  # ENUM types can be treated as strings\n])\n\n# Data for the Confirmations table (converting strings to datetime objects)\nconfirmations_data = [\n    (3, datetime(2021, 1, 6, 3, 30, 46), \"timeout\"),\n    (3, datetime(2021, 7, 14, 14, 0, 0), \"timeout\"),\n    (7, datetime(2021, 6, 12, 11, 57, 29), \"confirmed\"),\n    (7, datetime(2021, 6, 13, 12, 58, 28), \"confirmed\"),\n    (7, datetime(2021, 6, 14, 13, 59, 27), \"confirmed\"),\n    (2, datetime(2021, 1, 22, 0, 0, 0), \"confirmed\"),\n    (2, datetime(2021, 2, 28, 23, 59, 59), \"timeout\")\n]\n\n# Create Confirmations DataFrame\nconfirmations_df = spark.createDataFrame(confirmations_data, schema=confirmations_schema)\n\n# Show the data for Signups and Confirmations\nsignups_df.show(truncate=False)\nconfirmations_df.show(truncate=False)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:19:05.527907Z","iopub.execute_input":"2025-01-31T17:19:05.528683Z","iopub.status.idle":"2025-01-31T17:19:06.529752Z","shell.execute_reply.started":"2025-01-31T17:19:05.528628Z","shell.execute_reply":"2025-01-31T17:19:06.527661Z"},"trusted":true},"outputs":[{"name":"stderr","text":"25/01/31 17:19:05 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+-------+-------------------+\n|user_id|time_stamp         |\n+-------+-------------------+\n|3      |2020-03-21 10:16:13|\n|7      |2020-01-04 13:57:59|\n|2      |2020-07-29 23:09:44|\n|6      |2020-12-09 10:39:37|\n+-------+-------------------+\n\n+-------+-------------------+---------+\n|user_id|time_stamp         |action   |\n+-------+-------------------+---------+\n|3      |2021-01-06 03:30:46|timeout  |\n|3      |2021-07-14 14:00:00|timeout  |\n|7      |2021-06-12 11:57:29|confirmed|\n|7      |2021-06-13 12:58:28|confirmed|\n|7      |2021-06-14 13:59:27|confirmed|\n|2      |2021-01-22 00:00:00|confirmed|\n|2      |2021-02-28 23:59:59|timeout  |\n+-------+-------------------+---------+\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from pyspark.sql.functions import expr,when,round,format_number\ndfs = signups_df\ndfc = confirmations_df\n# dfc.show()\n\ndfc_cnf = dfc.filter(\"action = 'confirmed'\").groupby(\"user_id\").agg(count('*').alias(\"cnt\"))\ndfc_cnf.show()\n\ndfc_cnf_percent = dfc.alias(\"dfc\").groupby(\"user_id\").agg(count(\"*\").alias(\"cnt_total\"))\\\n    .join(dfc_cnf.alias(\"dfc_cnf\"),col(\"dfc_cnf.user_id\") == col(\"dfc.user_id\"),\"left\")\\\n    .select(\"dfc.user_id\",expr(\"COALESCE(cnt/cnt_total,0)\").alias(\"conf_percent\"))\n\ndfc_cnf_percent.show()\n\nres = dfs.alias(\"dfs\").join(dfc_cnf_percent.alias(\"dfc_cnf_percent\"),col(\"dfs.user_id\")==col(\"dfc_cnf_percent.user_id\"),\"left\")\nres = res.select(\"dfs.user_id\",round(when(col(\"conf_percent\").isNull(),0).otherwise(col(\"conf_percent\")),2).alias(\"cnf_percent\"))\nres.show()\n\n# to force the dataframe to show 2 decimals we can use the format_number function in pyspark\nres = res.select(\"dfs.user_id\",format_number(col(\"cnf_percent\"),2).alias(\"cnf_percent_forced_2_decimals\"))\nres.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:06.530962Z","iopub.execute_input":"2025-01-31T17:19:06.531377Z","iopub.status.idle":"2025-01-31T17:19:11.723991Z","shell.execute_reply.started":"2025-01-31T17:19:06.531333Z","shell.execute_reply":"2025-01-31T17:19:11.722165Z"}},"outputs":[{"name":"stdout","text":"+-------+---+\n|user_id|cnt|\n+-------+---+\n|      7|  3|\n|      2|  1|\n+-------+---+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-------+------------+\n|user_id|conf_percent|\n+-------+------------+\n|      3|         0.0|\n|      7|         1.0|\n|      2|         0.5|\n+-------+------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-------+-----------+\n|user_id|cnf_percent|\n+-------+-----------+\n|      2|        0.5|\n|      3|        0.0|\n|      6|        0.0|\n|      7|        1.0|\n+-------+-----------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+-------+-----------------------------+\n|user_id|cnf_percent_forced_2_decimals|\n+-------+-----------------------------+\n|      2|                         0.50|\n|      3|                         0.00|\n|      6|                         0.00|\n|      7|                         1.00|\n+-------+-----------------------------+\n\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## PySpark","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import max\ns_df = signups_df\nc_df = confirmations_df\n\ndf_cnfrm = c_df.filter(\"action == 'confirmed'\").groupBy(\"user_id\").agg(count(\"user_id\").alias(\"cnt_cnfrm\"))\n\ns_df.alias('a').join(c_df.alias('b'),col(\"a.user_id\") == col(\"b.user_id\"),\"left\")\\\n    .join(df_cnfrm.alias('c'),col(\"c.user_id\")==col(\"a.user_id\"),\"left\")\\\n    .groupBy(\"a.user_id\").agg(count(\"*\").alias(\"cnt_all\"),max(\"cnt_cnfrm\").alias(\"cnt_cnfrm\"))\\\n    .fillna(0,subset=['cnt_cnfrm'])\\\n    .select(\"user_id\",col(\"cnt_cnfrm\")/col(\"cnt_all\"))","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:19:11.725840Z","iopub.execute_input":"2025-01-31T17:19:11.726327Z","iopub.status.idle":"2025-01-31T17:19:14.816616Z","shell.execute_reply.started":"2025-01-31T17:19:11.726276Z","shell.execute_reply":"2025-01-31T17:19:14.813727Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"+-------+---------------------+\n|user_id|(cnt_cnfrm / cnt_all)|\n+-------+---------------------+\n|      3|                  0.0|\n|      7|                  1.0|\n|      2|                  0.5|\n|      6|                  0.0|\n+-------+---------------------+","text/html":"<table border='1'>\n<tr><th>user_id</th><th>(cnt_cnfrm / cnt_all)</th></tr>\n<tr><td>3</td><td>0.0</td></tr>\n<tr><td>7</td><td>1.0</td></tr>\n<tr><td>2</td><td>0.5</td></tr>\n<tr><td>6</td><td>0.0</td></tr>\n</table>\n"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## Pandas","metadata":{}},{"cell_type":"code","source":"signups = signups_df.toPandas()\nconfirmations = confirmations_df.toPandas()\n\ndf_total = confirmations.groupby('user_id')['action'].count().reset_index()\ndf_conf = confirmations[confirmations.action =='confirmed'].groupby('user_id')['action'].count().reset_index()\ndf = signups.merge(df_total, how = 'left').merge(df_conf , how = 'left', on = 'user_id')\ndf\n\ndf['confirmation_rate'] =  ((df.action_y)/ (df.action_x)).round(2)   \ndf.loc[:,[\"user_id\",\"confirmation_rate\"]].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2025-01-31T17:19:14.817971Z","iopub.execute_input":"2025-01-31T17:19:14.818442Z","iopub.status.idle":"2025-01-31T17:19:15.597844Z","shell.execute_reply.started":"2025-01-31T17:19:14.818390Z","shell.execute_reply":"2025-01-31T17:19:15.596711Z"},"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"   user_id  confirmation_rate\n0        3                0.0\n1        7                1.0\n2        2                0.5\n3        6                0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>confirmation_rate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"# Problem 15\n\nWrite a solution to report the movies with an odd-numbered ID and a description that is not \"boring\".\nReturn the result table ordered by rating in descending order.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.types import FloatType\n# Define the schema for the Cinema table\ncinema_schema = StructType([\n    StructField(\"id\", IntegerType(), nullable=False),\n    StructField(\"movie\", StringType(), nullable=False),\n    StructField(\"description\", StringType(), nullable=False),\n    StructField(\"rating\", FloatType(), nullable=False)\n])\n\n# Create the data for the Cinema table\ncinema_data = [\n    (1, \"War\", \"great 3D\", 8.9),\n    (2, \"Science\", \"fiction\", 8.5),\n    (3, \"irish\", \"boring\", 6.2),\n    (4, \"Ice song\", \"Fantacy\", 8.6),\n    (5, \"House card\", \"Interesting\", 9.1)\n]\n\n# Create the DataFrame\ncinema_df = spark.createDataFrame(data=cinema_data, schema=cinema_schema)\n\n# Show the DataFrame\ncinema_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:15.599370Z","iopub.execute_input":"2025-01-31T17:19:15.599711Z","iopub.status.idle":"2025-01-31T17:19:16.083511Z","shell.execute_reply.started":"2025-01-31T17:19:15.599670Z","shell.execute_reply":"2025-01-31T17:19:16.080582Z"}},"outputs":[{"name":"stdout","text":"+---+----------+-----------+------+\n| id|     movie|description|rating|\n+---+----------+-----------+------+\n|  1|       War|   great 3D|   8.9|\n|  2|   Science|    fiction|   8.5|\n|  3|     irish|     boring|   6.2|\n|  4|  Ice song|    Fantacy|   8.6|\n|  5|House card|Interesting|   9.1|\n+---+----------+-----------+------+\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"## Pyspark\n\nfrom pyspark.sql.functions import like\n\n# here the use of ~ is giving the inverse affect of LIKE and acting like \"NOT LIKE\"\ndfc = cinema_df\ndfc = dfc.filter(\\\n    (col(\"id\")%2==1) & ( ~col(\"description\").like(\"%bori%\") ) \\\n).orderBy(\"rating\",ascending=False)\ndfc.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:16.084769Z","iopub.execute_input":"2025-01-31T17:19:16.085227Z","iopub.status.idle":"2025-01-31T17:19:16.517581Z","shell.execute_reply.started":"2025-01-31T17:19:16.085178Z","shell.execute_reply":"2025-01-31T17:19:16.514169Z"}},"outputs":[{"name":"stdout","text":"+---+----------+-----------+------+\n| id|     movie|description|rating|\n+---+----------+-----------+------+\n|  5|House card|Interesting|   9.1|\n|  1|       War|   great 3D|   8.9|\n+---+----------+-----------+------+\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Problem 16\n\nWrite a solution to find the average selling price for each product. average_price should be rounded to 2 decimal places. If a product does not have any sold units, its average selling price is assumed to be 0.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DateType\nfrom datetime import datetime\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"Create DataFrames\") \\\n    .getOrCreate()\n\n# Define schema for Prices table\nprices_schema = StructType([\n    StructField(\"product_id\", IntegerType(), True),\n    StructField(\"start_date\", DateType(), True),\n    StructField(\"end_date\", DateType(), True),\n    StructField(\"price\", IntegerType(), True)\n])\n\n# Create Prices DataFrame\nprices_data = [\n    (1, datetime.strptime('2019-02-17', '%Y-%m-%d'), datetime.strptime('2019-02-28', '%Y-%m-%d'), 5),\n    (1, datetime.strptime('2019-03-01', '%Y-%m-%d'), datetime.strptime('2019-03-22', '%Y-%m-%d'), 20),\n    (2, datetime.strptime('2019-02-01', '%Y-%m-%d'), datetime.strptime('2019-02-20', '%Y-%m-%d'), 15),\n    (2, datetime.strptime('2019-02-21', '%Y-%m-%d'), datetime.strptime('2019-03-31', '%Y-%m-%d'), 30),\n    (3, datetime.strptime('2019-02-21', '%Y-%m-%d'), datetime.strptime('2019-03-31', '%Y-%m-%d'), 55)\n]\n\nprices_df = spark.createDataFrame(prices_data, schema=prices_schema)\n\n# Define schema for UnitsSold table\nunits_sold_schema = StructType([\n    StructField(\"product_id\", IntegerType(), True),\n    StructField(\"purchase_date\", DateType(), True),\n    StructField(\"units\", IntegerType(), True)\n])\n\n# Create UnitsSold DataFrame\nunits_sold_data = [\n    (1, datetime.strptime('2019-02-25', '%Y-%m-%d'), 100),\n    (1, datetime.strptime('2019-03-01', '%Y-%m-%d'), 15),\n    # (1, datetime.strptime('2019-03-20', '%Y-%m-%d'), 10),\n    (2, datetime.strptime('2019-02-10', '%Y-%m-%d'), 200),\n    (2, datetime.strptime('2019-03-22', '%Y-%m-%d'), 30)\n]\n\nunits_sold_df = spark.createDataFrame(units_sold_data, schema=units_sold_schema)\n\n# Show DataFrames\nprices_df.show()\nunits_sold_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:16.519339Z","iopub.execute_input":"2025-01-31T17:19:16.519817Z","iopub.status.idle":"2025-01-31T17:19:17.517414Z","shell.execute_reply.started":"2025-01-31T17:19:16.519767Z","shell.execute_reply":"2025-01-31T17:19:17.516195Z"}},"outputs":[{"name":"stderr","text":"25/01/31 17:19:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n","output_type":"stream"},{"name":"stdout","text":"+----------+----------+----------+-----+\n|product_id|start_date|  end_date|price|\n+----------+----------+----------+-----+\n|         1|2019-02-17|2019-02-28|    5|\n|         1|2019-03-01|2019-03-22|   20|\n|         2|2019-02-01|2019-02-20|   15|\n|         2|2019-02-21|2019-03-31|   30|\n|         3|2019-02-21|2019-03-31|   55|\n+----------+----------+----------+-----+\n\n+----------+-------------+-----+\n|product_id|purchase_date|units|\n+----------+-------------+-----+\n|         1|   2019-02-25|  100|\n|         1|   2019-03-01|   15|\n|         2|   2019-02-10|  200|\n|         2|   2019-03-22|   30|\n+----------+-------------+-----+\n\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"dfu = units_sold_df\ndfp = prices_df\n\nres = dfp.alias(\"dfp\").join(dfu.alias('dfu'),\\\n                            ( col(\"dfu.product_id\")==col(\"dfp.product_id\") ) &\\\n                            ( col(\"dfu.purchase_date\") >= col(\"dfp.start_date\") ) &\\\n                            ( col(\"dfu.purchase_date\") <= col(\"dfp.end_date\") )\\\n                            ,\"outer\")\nres = res.withColumn(\"sales_price\",col(\"units\")*col(\"price\")).groupby(col(\"dfp.product_id\")).agg(sum(col(\"sales_price\")).alias(\"sum_sales_price\"),sum(col(\"units\")).alias(\"sum_units\"))\nres = res.select(\"product_id\",expr(\"FORMAT_NUMBER (ROUND ( coalesce(sum_sales_price/sum_units,0) , 2),2) as avg_sales_price\")).orderBy(\"product_id\")\nres.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:17.520706Z","iopub.execute_input":"2025-01-31T17:19:17.521200Z","iopub.status.idle":"2025-01-31T17:19:18.619180Z","shell.execute_reply.started":"2025-01-31T17:19:17.521149Z","shell.execute_reply":"2025-01-31T17:19:18.618080Z"}},"outputs":[{"name":"stdout","text":"+----------+---------------+\n|product_id|avg_sales_price|\n+----------+---------------+\n|         1|           6.96|\n|         2|          16.96|\n|         3|           0.00|\n+----------+---------------+\n\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Problem 17\nWrite an SQL query that reports the average experience years of all the employees for each project, rounded to 2 digits.","metadata":{}},{"cell_type":"code","source":"\n# Define schema for Project table\nproject_schema = StructType([\n    StructField(\"project_id\", IntegerType(), True),\n    StructField(\"employee_id\", IntegerType(), True)\n])\n\n# Create Project DataFrame\nproject_data = [\n    (1, 1),\n    (1, 2),\n    (1, 3),\n    (2, 1),\n    (2, 4)\n]\n\nproject_df = spark.createDataFrame(project_data, schema=project_schema)\n\n# Define schema for Employee table\nemployee_schema = StructType([\n    StructField(\"employee_id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"experience_years\", IntegerType(), True)\n])\n\n# Create Employee DataFrame\nemployee_data = [\n    (1, \"Khaled\", 3),\n    (2, \"Ali\", 2),\n    (3, \"John\", 1),\n    (4, \"Doe\", 2)\n]\n\nemployee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n\n# Show DataFrames\nproject_df.show()\nemployee_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:18.621325Z","iopub.execute_input":"2025-01-31T17:19:18.622089Z","iopub.status.idle":"2025-01-31T17:19:19.542528Z","shell.execute_reply.started":"2025-01-31T17:19:18.622027Z","shell.execute_reply":"2025-01-31T17:19:19.541101Z"}},"outputs":[{"name":"stdout","text":"+----------+-----------+\n|project_id|employee_id|\n+----------+-----------+\n|         1|          1|\n|         1|          2|\n|         1|          3|\n|         2|          1|\n|         2|          4|\n+----------+-----------+\n\n+-----------+------+----------------+\n|employee_id|  name|experience_years|\n+-----------+------+----------------+\n|          1|Khaled|               3|\n|          2|   Ali|               2|\n|          3|  John|               1|\n|          4|   Doe|               2|\n+-----------+------+----------------+\n\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"from pyspark.sql.functions import avg\ndfp = project_df\ndfe = employee_df\n\ndfp.alias(\"dfp\").join(dfe.alias(\"dfe\"),col(\"dfe.employee_id\")==col(\"dfp.employee_id\"),\"inner\")\\\n    .groupby(col(\"dfp.project_id\")).agg(avg(col(\"dfe.experience_years\"))).show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:19.549868Z","iopub.execute_input":"2025-01-31T17:19:19.550740Z","iopub.status.idle":"2025-01-31T17:19:20.703883Z","shell.execute_reply.started":"2025-01-31T17:19:19.550689Z","shell.execute_reply":"2025-01-31T17:19:20.702634Z"}},"outputs":[{"name":"stderr","text":"[Stage 156:============================>                            (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+----------+-------------------------+\n|project_id|avg(dfe.experience_years)|\n+----------+-------------------------+\n|         1|                      2.0|\n|         2|                      2.5|\n+----------+-------------------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Problem 18\n\nWrite a solution to find the percentage of the users registered in each contest rounded to two decimals.\n\nReturn the result table ordered by percentage in descending order. In case of a tie, order it by contest_id in ascending order.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n\n# Define schema for Users table\nusers_schema = StructType([\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"user_name\", StringType(), True)\n])\n\n# Create Users DataFrame\nusers_data = [\n    (6, \"Alice\"),\n    (2, \"Bob\"),\n    (7, \"Alex\")\n]\n\nusers_df = spark.createDataFrame(users_data, schema=users_schema)\n\n# Define schema for Register table\nregister_schema = StructType([\n    StructField(\"contest_id\", IntegerType(), True),\n    StructField(\"user_id\", IntegerType(), True)\n])\n\n# Create Register DataFrame\nregister_data = [\n    (215, 6),\n    (215, 6),\n    (209, 2),\n    (208, 2),\n    (210, 6),\n    (208, 6),\n    (209, 7),\n    (209, 6),\n    (215, 7),\n    (208, 7),\n    (210, 2),\n    (207, 2),\n    (210, 7)\n]\n\nregister_df = spark.createDataFrame(register_data, schema=register_schema)\n\n# Show DataFrames\nusers_df.show()\nregister_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:20.708112Z","iopub.execute_input":"2025-01-31T17:19:20.708576Z","iopub.status.idle":"2025-01-31T17:19:21.665208Z","shell.execute_reply.started":"2025-01-31T17:19:20.708527Z","shell.execute_reply":"2025-01-31T17:19:21.664145Z"}},"outputs":[{"name":"stdout","text":"+-------+---------+\n|user_id|user_name|\n+-------+---------+\n|      6|    Alice|\n|      2|      Bob|\n|      7|     Alex|\n+-------+---------+\n\n+----------+-------+\n|contest_id|user_id|\n+----------+-------+\n|       215|      6|\n|       215|      6|\n|       209|      2|\n|       208|      2|\n|       210|      6|\n|       208|      6|\n|       209|      7|\n|       209|      6|\n|       215|      7|\n|       208|      7|\n|       210|      2|\n|       207|      2|\n|       210|      7|\n+----------+-------+\n\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"from pyspark.sql.functions import count\n\ndfu = users_df\ndfr = register_df\n\n# We are removing any duplicate registrations\ndfr = dfr.distinct()\n\nres_r = dfr.groupBy(\"contest_id\").agg(count(\"user_id\").alias(\"count_users\"))\nres_r.show()\nres_u = dfu.agg(count(col(\"*\")).alias(\"count_total_users\"))\nres_u.show()\n\nres_final = res_r.alias(\"res_r\").join(res_u.alias(\"res_u\"),how=\"cross\")\nres_final.show()\nres_final = res_final.select(\"contest_id\",expr(\"ROUND((count_users/count_total_users)*100,2) as user_percent\"))\nres_final = res_final.orderBy(col(\"user_percent\").desc(),col(\"contest_id\").asc())\nres_final.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:19:21.666897Z","iopub.execute_input":"2025-01-31T17:19:21.667309Z","iopub.status.idle":"2025-01-31T17:19:25.161670Z","shell.execute_reply.started":"2025-01-31T17:19:21.667273Z","shell.execute_reply":"2025-01-31T17:19:25.160697Z"}},"outputs":[{"name":"stdout","text":"+----------+-----------+\n|contest_id|count_users|\n+----------+-----------+\n|       210|          3|\n|       209|          3|\n|       207|          1|\n|       215|          2|\n|       208|          3|\n+----------+-----------+\n\n+-----------------+\n|count_total_users|\n+-----------------+\n|                3|\n+-----------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+-----------+-----------------+\n|contest_id|count_users|count_total_users|\n+----------+-----------+-----------------+\n|       210|          3|                3|\n|       209|          3|                3|\n|       207|          1|                3|\n|       215|          2|                3|\n|       208|          3|                3|\n+----------+-----------+-----------------+\n\n","output_type":"stream"},{"name":"stderr","text":"[Stage 187:==========================================>              (3 + 1) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+----------+------------+\n|contest_id|user_percent|\n+----------+------------+\n|       208|       100.0|\n|       209|       100.0|\n|       210|       100.0|\n|       215|       66.67|\n|       207|       33.33|\n+----------+------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Problem 19\n\nWrite an SQL query to find for each month and country, the number of transactions and their total amount, the number of approved transactions and their total amount.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\nfrom datetime import datetime\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"Create DataFrames\") \\\n    .getOrCreate()\n\n# Define schema for Transactions table\ntransactions_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"state\", StringType(), True),\n    StructField(\"amount\", IntegerType(), True),\n    StructField(\"trans_date\", DateType(), True)\n])\n\n# Sample data for Transactions table\ntransactions_data = [\n    (121, \"US\", \"approved\", 1000, datetime.strptime(\"2018-12-18\", \"%Y-%m-%d\")),\n    (122, \"US\", \"declined\", 2000, datetime.strptime(\"2018-12-19\", \"%Y-%m-%d\")),\n    (123, \"US\", \"approved\", 2000, datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\")),\n    (124, \"DE\", \"approved\", 2000, datetime.strptime(\"2019-01-07\", \"%Y-%m-%d\")),\n    (125, \"DE\", \"declined\", 2000, datetime.strptime(\"2020-01-07\", \"%Y-%m-%d\"))\n]\n\n# Create Transactions DataFrame\ntransactions_df = spark.createDataFrame(transactions_data, schema=transactions_schema)\n\n# Show the Transactions DataFrame\ntransactions_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:36:55.863155Z","iopub.execute_input":"2025-01-31T17:36:55.863559Z","iopub.status.idle":"2025-01-31T17:36:56.318704Z","shell.execute_reply.started":"2025-01-31T17:36:55.863526Z","shell.execute_reply":"2025-01-31T17:36:56.317298Z"}},"outputs":[{"name":"stdout","text":"+---+-------+--------+------+----------+\n| id|country|   state|amount|trans_date|\n+---+-------+--------+------+----------+\n|121|     US|approved|  1000|2018-12-18|\n|122|     US|declined|  2000|2018-12-19|\n|123|     US|approved|  2000|2019-01-01|\n|124|     DE|approved|  2000|2019-01-07|\n|125|     DE|declined|  2000|2020-01-07|\n+---+-------+--------+------+----------+\n\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"from pyspark.sql.functions import month, year\n\ndft = transactions_df\ndft = dft.withColumn(\"date_month\",month(col(\"trans_date\"))).withColumn(\"date_year\",year(col(\"trans_date\")))\ndft_apr = dft.filter(col(\"state\")=='approved').groupBy(\"date_month\",\"date_year\",\"country\").agg(count(\"*\").alias(\"count_apr\"),sum(\"amount\").alias(\"sum_amount_apr\"))\ndft_apr.show()\n\ndft_total = dft.groupBy(\"date_month\",\"date_year\",\"country\").agg(count(\"*\").alias(\"count_total\"),sum(\"amount\").alias(\"sum_amount_total\"))\ndft_total.show()\n\nres = dft_total.alias(\"dft_total\").join(dft_apr.alias(\"dft_apr\"),\\\n                                           ( col(\"dft_total.country\")==col(\"dft_apr.country\") ) &\\\n                                            ( col(\"dft_total.date_year\")==col(\"dft_apr.date_year\") ) &\\\n                                            ( col(\"dft_total.date_month\")==col(\"dft_apr.date_month\") ), \\\n                                        \"left\"\n                                       )\nres.show()\n\nres = res.select(\"dft_total.*\",expr(\"coalesce(dft_apr.count_apr,0) as count_approved\"),expr(\"coalesce(dft_apr.sum_amount_apr,0) as sum_amount_approved\"))\nres.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T17:47:15.871070Z","iopub.execute_input":"2025-01-31T17:47:15.871493Z","iopub.status.idle":"2025-01-31T17:47:18.571597Z","shell.execute_reply.started":"2025-01-31T17:47:15.871457Z","shell.execute_reply":"2025-01-31T17:47:18.570490Z"}},"outputs":[{"name":"stdout","text":"+----------+---------+-------+---------+--------------+\n|date_month|date_year|country|count_apr|sum_amount_apr|\n+----------+---------+-------+---------+--------------+\n|        12|     2018|     US|        1|          1000|\n|         1|     2019|     US|        1|          2000|\n|         1|     2019|     DE|        1|          2000|\n+----------+---------+-------+---------+--------------+\n\n+----------+---------+-------+-----------+----------------+\n|date_month|date_year|country|count_total|sum_amount_total|\n+----------+---------+-------+-----------+----------------+\n|        12|     2018|     US|          2|            3000|\n|         1|     2019|     US|          1|            2000|\n|         1|     2020|     DE|          1|            2000|\n|         1|     2019|     DE|          1|            2000|\n+----------+---------+-------+-----------+----------------+\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"+----------+---------+-------+-----------+----------------+----------+---------+-------+---------+--------------+\n|date_month|date_year|country|count_total|sum_amount_total|date_month|date_year|country|count_apr|sum_amount_apr|\n+----------+---------+-------+-----------+----------------+----------+---------+-------+---------+--------------+\n|        12|     2018|     US|          2|            3000|        12|     2018|     US|        1|          1000|\n|         1|     2019|     US|          1|            2000|         1|     2019|     US|        1|          2000|\n|         1|     2020|     DE|          1|            2000|      NULL|     NULL|   NULL|     NULL|          NULL|\n|         1|     2019|     DE|          1|            2000|         1|     2019|     DE|        1|          2000|\n+----------+---------+-------+-----------+----------------+----------+---------+-------+---------+--------------+\n\n+----------+---------+-------+-----------+----------------+--------------+-------------------+\n|date_month|date_year|country|count_total|sum_amount_total|count_approved|sum_amount_approved|\n+----------+---------+-------+-----------+----------------+--------------+-------------------+\n|        12|     2018|     US|          2|            3000|             1|               1000|\n|         1|     2019|     US|          1|            2000|             1|               2000|\n|         1|     2020|     DE|          1|            2000|             0|                  0|\n|         1|     2019|     DE|          1|            2000|             1|               2000|\n+----------+---------+-------+-----------+----------------+--------------+-------------------+\n\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"# Problem 20\n\nIf the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n\nThe first order of a customer is the order with the earliest order date that the customer made. It is guaranteed that a customer has precisely one first order.\n\nWrite a solution to find the percentage of immediate orders in the first orders of all customers, rounded to 2 decimal places.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\nfrom datetime import datetime\n\n\n# Define schema for Delivery table\ndelivery_schema = StructType([\n    StructField(\"delivery_id\", IntegerType(), True),\n    StructField(\"customer_id\", IntegerType(), True),\n    StructField(\"order_date\", DateType(), True),\n    StructField(\"customer_pref_delivery_date\", DateType(), True)\n])\n\n# Sample data for Delivery table\ndelivery_data = [\n    (1, 1, datetime.strptime(\"2019-08-01\", \"%Y-%m-%d\"), datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\")),\n    (2, 2, datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\"), datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\")),\n    (3, 1, datetime.strptime(\"2019-08-11\", \"%Y-%m-%d\"), datetime.strptime(\"2019-08-12\", \"%Y-%m-%d\")),\n    (4, 3, datetime.strptime(\"2019-08-24\", \"%Y-%m-%d\"), datetime.strptime(\"2019-08-24\", \"%Y-%m-%d\")),\n    (5, 3, datetime.strptime(\"2019-08-21\", \"%Y-%m-%d\"), datetime.strptime(\"2019-08-22\", \"%Y-%m-%d\")),\n    (6, 2, datetime.strptime(\"2019-08-11\", \"%Y-%m-%d\"), datetime.strptime(\"2019-08-13\", \"%Y-%m-%d\")),\n    (7, 4, datetime.strptime(\"2019-08-09\", \"%Y-%m-%d\"), datetime.strptime(\"2019-08-09\", \"%Y-%m-%d\"))\n]\n\n# Create Delivery DataFrame\ndelivery_df = spark.createDataFrame(delivery_data, schema=delivery_schema)\n\n# Show the Delivery DataFrame\ndelivery_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:08:29.248244Z","iopub.execute_input":"2025-01-31T18:08:29.249476Z","iopub.status.idle":"2025-01-31T18:08:29.770259Z","shell.execute_reply.started":"2025-01-31T18:08:29.249425Z","shell.execute_reply":"2025-01-31T18:08:29.769233Z"}},"outputs":[{"name":"stdout","text":"+-----------+-----------+----------+---------------------------+\n|delivery_id|customer_id|order_date|customer_pref_delivery_date|\n+-----------+-----------+----------+---------------------------+\n|          1|          1|2019-08-01|                 2019-08-02|\n|          2|          2|2019-08-02|                 2019-08-02|\n|          3|          1|2019-08-11|                 2019-08-12|\n|          4|          3|2019-08-24|                 2019-08-24|\n|          5|          3|2019-08-21|                 2019-08-22|\n|          6|          2|2019-08-11|                 2019-08-13|\n|          7|          4|2019-08-09|                 2019-08-09|\n+-----------+-----------+----------+---------------------------+\n\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"from pyspark.sql.functions import when,rank\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.window import Window\n\ndfd = delivery_df\ndfd = dfd.withColumn(\"is_Immediate\",when( col(\"order_date\") == col(\"customer_pref_delivery_date\"),lit(1) ).otherwise(lit(0)) )\ndfd.show()\n\nrank_window_spec = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n\ndfd = dfd.withColumn(\"rank_order_by_emp\",rank().over(rank_window_spec))\ndfd.show()\n\ndfd = dfd.filter(expr(\"rank_order_by_emp=1\"))\ndfd.show()\n\ndfd = dfd.select(expr(\"format_number(sum(is_Immediate)/count(*),2)\").alias(\"immediate_percentage \"))\ndfd.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T18:27:20.752017Z","iopub.execute_input":"2025-01-31T18:27:20.752449Z","iopub.status.idle":"2025-01-31T18:27:22.599280Z","shell.execute_reply.started":"2025-01-31T18:27:20.752414Z","shell.execute_reply":"2025-01-31T18:27:22.598412Z"}},"outputs":[{"name":"stdout","text":"+-----------+-----------+----------+---------------------------+------------+\n|delivery_id|customer_id|order_date|customer_pref_delivery_date|is_Immediate|\n+-----------+-----------+----------+---------------------------+------------+\n|          1|          1|2019-08-01|                 2019-08-02|           0|\n|          2|          2|2019-08-02|                 2019-08-02|           1|\n|          3|          1|2019-08-11|                 2019-08-12|           0|\n|          4|          3|2019-08-24|                 2019-08-24|           1|\n|          5|          3|2019-08-21|                 2019-08-22|           0|\n|          6|          2|2019-08-11|                 2019-08-13|           0|\n|          7|          4|2019-08-09|                 2019-08-09|           1|\n+-----------+-----------+----------+---------------------------+------------+\n\n+-----------+-----------+----------+---------------------------+------------+-----------------+\n|delivery_id|customer_id|order_date|customer_pref_delivery_date|is_Immediate|rank_order_by_emp|\n+-----------+-----------+----------+---------------------------+------------+-----------------+\n|          1|          1|2019-08-01|                 2019-08-02|           0|                1|\n|          3|          1|2019-08-11|                 2019-08-12|           0|                2|\n|          2|          2|2019-08-02|                 2019-08-02|           1|                1|\n|          6|          2|2019-08-11|                 2019-08-13|           0|                2|\n|          5|          3|2019-08-21|                 2019-08-22|           0|                1|\n|          4|          3|2019-08-24|                 2019-08-24|           1|                2|\n|          7|          4|2019-08-09|                 2019-08-09|           1|                1|\n+-----------+-----------+----------+---------------------------+------------+-----------------+\n\n+-----------+-----------+----------+---------------------------+------------+-----------------+\n|delivery_id|customer_id|order_date|customer_pref_delivery_date|is_Immediate|rank_order_by_emp|\n+-----------+-----------+----------+---------------------------+------------+-----------------+\n|          1|          1|2019-08-01|                 2019-08-02|           0|                1|\n|          2|          2|2019-08-02|                 2019-08-02|           1|                1|\n|          5|          3|2019-08-21|                 2019-08-22|           0|                1|\n|          7|          4|2019-08-09|                 2019-08-09|           1|                1|\n+-----------+-----------+----------+---------------------------+------------+-----------------+\n\n+---------------------+\n|immediate_percentage |\n+---------------------+\n|                 0.50|\n+---------------------+\n\n","output_type":"stream"}],"execution_count":77}]}