{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n!pip install pyspark\n\nfrom pyspark.sql import SparkSession\n# Create a SparkSession (without a specified name)\nspark = SparkSession.builder.getOrCreate()\nspark.conf.set('spark.sql.repl.eagerEval.enabled', True) #for simple calls and better display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:23.542872Z","iopub.execute_input":"2025-02-04T04:56:23.543303Z","iopub.status.idle":"2025-02-04T04:56:38.777441Z","shell.execute_reply.started":"2025-02-04T04:56:23.543265Z","shell.execute_reply":"2025-02-04T04:56:38.775843Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\nRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#  Understanding `expr()` vs. Strings in PySpark\n\nPySpark provides flexibility in how we pass expressions, but not all methods handle strings and expressions in the same way. This guide explains when you can use raw strings and when you must use `expr()`.\n\n---\n\n##  **1. `filter()` (or `where()`) Accepts Both Strings and `expr()`**\n- `filter()` automatically converts a SQL expression string into a `Column`, so using `expr()` explicitly is optional.\n\n **Both of these work the same way:**\n```python\ndfa = dfa.filter(expr(\"player_date_rank + device_id = 1\"))\ndfa = dfa.filter(\"player_date_rank + device_id = 1\")  # No need for expr()\n```\nSince filter() interprets string arguments as SQL expressions internally, both versions work.\n\n## **2. withColumn() Requires a Column Object**\nUnlike filter(), withColumn() does not automatically convert strings into SQL expressions.\nIf you pass a raw string, Spark throws an error:\n\"Argument col should be a Column, got str.\"\n```python\ndfa = dfa.withColumn(\"abc\", \"games_played + device_id\")  # ❌ Error\n\nfrom pyspark.sql.functions import expr\ndfa = dfa.withColumn(\"abc\", expr(\"games_played + device_id\"))  # ✅ Works! Use expr() to convert the string into a Column:\n\ndfa = dfa.withColumn(\"abc\", dfa[\"games_played\"] + dfa[\"device_id\"])  # ✅ Works!  Or use DataFrame column operations:\n```\n| Method          | Can Use String Directly? | Requires `expr()` or Column Object? | Notes                                                                 |\n|-----------------|--------------------------|-------------------------------------|-----------------------------------------------------------------------|\n| `filter()` / `where()` | ✅ Yes                 | ❌ No                              | Accepts SQL expressions as strings. `expr()` is optional.             |\n| `select()`      | ❌ No                    | ✅ Yes                             | Must use `expr()` or `col()`, unless selecting column names.          |\n| `withColumn()`  | ❌ No                    | ✅ Yes                             | Always requires a Column object, use `expr()` if needed.              |\n| `groupBy().agg()` | ❌ No                    | ✅ Yes                             | Aggregation functions require Column objects.                         |\n| `orderBy()` / `sort()` | ✅ Yes                 | ❌ No                              | Strings (column names) are allowed, but complex expressions need `expr()` |\n\n\n#### Summary: withColumn(param_1: `<String>`new_column_name, param_2: `<Col>` defining_col_object), while filter will accept both Column class or String with SQL style expression and auto convert it to a Column class","metadata":{}},{"cell_type":"markdown","source":"# Problem 20\n\nWrite a solution to report the fraction of players that logged in again on the day after the day they first logged in, rounded to 2 decimal places. In other words, you need to count the number of players that logged in for at least two consecutive days starting from their first login date, then divide that number by the total number of players.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\nfrom datetime import datetime\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"Create DataFrames\") \\\n    .getOrCreate()\n\n# Define schema for Activity table\nactivity_schema = StructType([\n    StructField(\"player_id\", IntegerType(), True),\n    StructField(\"device_id\", IntegerType(), True),\n    StructField(\"event_date\", DateType(), True),\n    StructField(\"games_played\", IntegerType(), True)\n])\n\n# Sample data for Activity table\nactivity_data = [\n    (1, 2, datetime.strptime(\"2016-03-01\", \"%Y-%m-%d\"), 5),\n    (1, 2, datetime.strptime(\"2016-03-02\", \"%Y-%m-%d\"), 6),\n    (2, 3, datetime.strptime(\"2017-06-25\", \"%Y-%m-%d\"), 1),\n    (3, 1, datetime.strptime(\"2016-03-02\", \"%Y-%m-%d\"), 0),\n    (3, 4, datetime.strptime(\"2018-07-03\", \"%Y-%m-%d\"), 5)\n]\n\n# Create Activity DataFrame\nactivity_df = spark.createDataFrame(activity_data, schema=activity_schema)\n\n# Show the Activity DataFrame\nactivity_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:38.779637Z","iopub.execute_input":"2025-02-04T04:56:38.780384Z","iopub.status.idle":"2025-02-04T04:56:45.556515Z","shell.execute_reply.started":"2025-02-04T04:56:38.780347Z","shell.execute_reply":"2025-02-04T04:56:45.554960Z"}},"outputs":[{"name":"stdout","text":"+---------+---------+----------+------------+\n|player_id|device_id|event_date|games_played|\n+---------+---------+----------+------------+\n|        1|        2|2016-03-01|           5|\n|        1|        2|2016-03-02|           6|\n|        2|        3|2017-06-25|           1|\n|        3|        1|2016-03-02|           0|\n|        3|        4|2018-07-03|           5|\n+---------+---------+----------+------------+\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from pyspark.sql.functions import date_sub,date_add,lead,lag,rank,expr,avg,round\nfrom pyspark.sql.window import Window\n\ndfa = activity_df\n\nwindow_spec = Window.partitionBy(\"player_id\").orderBy(\"event_date\")\n\ndfa = dfa.withColumn(\"player_date_rank\",rank().over(window_spec))\\\n    .withColumn(\"next_day_in_calendar\",date_add(\"event_date\",1))\\\n    .withColumn(\"next_day_player\",lead(\"event_date\").over(window_spec))\n\ndfa = dfa.filter(expr(\"player_date_rank = 1\"))\\\n    .withColumn(\"is_next_day_logged\",expr(\"case when next_day_in_calendar = next_day_player then 1 else 0 end \"))\n\ndfa.agg(round(avg(\"is_next_day_logged\"),2).alias(\"fraction\")).show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:45.564003Z","iopub.execute_input":"2025-02-04T04:56:45.564713Z","iopub.status.idle":"2025-02-04T04:56:48.810452Z","shell.execute_reply.started":"2025-02-04T04:56:45.564664Z","shell.execute_reply":"2025-02-04T04:56:48.809336Z"}},"outputs":[{"name":"stdout","text":"+--------+\n|fraction|\n+--------+\n|    0.33|\n+--------+\n\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Problem 21\n\nWrite a solution to report the customer ids from the Customer table that bought all the products in the Product table.\n\nReturn the result table in any order.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"Create DataFrames\") \\\n    .getOrCreate()\n\n# Define schema for Customer table\ncustomer_schema = StructType([\n    StructField(\"customer_id\", IntegerType(), True),\n    StructField(\"product_key\", IntegerType(), True)\n])\n\n# Sample data for Customer table\ncustomer_data = [\n    (1, 5),\n    (1, 5),\n    (2, 6),\n    (3, 5),\n    (3, 6),\n    (1, 6)\n]\n\n# Create Customer DataFrame\ncustomer_df = spark.createDataFrame(customer_data, schema=customer_schema)\n\n# Define schema for Product table\nproduct_schema = StructType([\n    StructField(\"product_key\", IntegerType(), True)\n])\n\n# Sample data for Product table\nproduct_data = [\n    (5,),\n    (6,)\n]\n\n# Create Product DataFrame\nproduct_df = spark.createDataFrame(product_data, schema=product_schema)\n\n# Show the Customer DataFrame\ncustomer_df.show()\n\n# Show the Product DataFrame\nproduct_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:48.814505Z","iopub.execute_input":"2025-02-04T04:56:48.814957Z","iopub.status.idle":"2025-02-04T04:56:50.327118Z","shell.execute_reply.started":"2025-02-04T04:56:48.814914Z","shell.execute_reply":"2025-02-04T04:56:50.326051Z"}},"outputs":[{"name":"stdout","text":"+-----------+-----------+\n|customer_id|product_key|\n+-----------+-----------+\n|          1|          5|\n|          1|          5|\n|          2|          6|\n|          3|          5|\n|          3|          6|\n|          1|          6|\n+-----------+-----------+\n\n+-----------+\n|product_key|\n+-----------+\n|          5|\n|          6|\n+-----------+\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pyspark.sql.functions import sum,count,col\ndfc = customer_df\ndfp = product_df\n\ndfc = dfc.distinct()\ndfc = dfc.groupBy(\"customer_id\").agg(count(\"*\").alias(\"products\"))\ndfc.show()\n\ndfp = dfp.agg(count('*').alias(\"total_prod_count\"))\ndfp.show()\n\n# Both the below give the same results, we can see how an expr() function is making it easier to construct JOIN conditions\n# res = dfp.alias(\"dfp\").join(dfc.alias(\"dfc\"),col(\"dfc.products\") == col(\"dfp.total_prod_count\"),how=\"inner\")\nres = dfp.alias(\"dfp\").join(dfc.alias(\"dfc\"),expr(\"dfc.products = dfp.total_prod_count\"),how=\"inner\")\n\nres.select(\"dfc.customer_id\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:50.328697Z","iopub.execute_input":"2025-02-04T04:56:50.329309Z","iopub.status.idle":"2025-02-04T04:56:53.679315Z","shell.execute_reply.started":"2025-02-04T04:56:50.329265Z","shell.execute_reply":"2025-02-04T04:56:53.676629Z"}},"outputs":[{"name":"stdout","text":"+-----------+--------+\n|customer_id|products|\n+-----------+--------+\n|          1|       2|\n|          3|       2|\n|          2|       1|\n+-----------+--------+\n\n+----------------+\n|total_prod_count|\n+----------------+\n|               2|\n+----------------+\n\n+-----------+\n|customer_id|\n+-----------+\n|          1|\n|          3|\n+-----------+\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Problem 22\n\nFind all numbers that appear at least three times consecutively.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"PracticePySpark\").getOrCreate()\n\n# Define schema for the Logs table\nlogs_schema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"num\", IntegerType(), True)\n])\n\n# Sample data for the Logs table\nlogs_data = [\n    (1, 1),\n    (2, 1),\n    (3, 1),\n    (4, 2),\n    (5, 1),\n    (6, 2),\n    (7, 2)\n]\n\n# Create DataFrame for Logs table\nlogs_df = spark.createDataFrame(data=logs_data, schema=logs_schema)\n\n# Show the Logs DataFrame to verify\nlogs_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:53.680341Z","iopub.execute_input":"2025-02-04T04:56:53.680762Z","iopub.status.idle":"2025-02-04T04:56:54.343420Z","shell.execute_reply.started":"2025-02-04T04:56:53.680720Z","shell.execute_reply":"2025-02-04T04:56:54.341932Z"}},"outputs":[{"name":"stdout","text":"+---+---+\n| id|num|\n+---+---+\n|  1|  1|\n|  2|  1|\n|  3|  1|\n|  4|  2|\n|  5|  1|\n|  6|  2|\n|  7|  2|\n+---+---+\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from pyspark.sql.window import Window\nfrom pyspark.sql.functions import lag,lead\n\ndfl = logs_df\nwindow_spec = Window.orderBy(\"id\")\n\nres = dfl.withColumn(\"is_next_two_cols_same\",\\\n            (lead(\"num\",1).over(window_spec) == lead(\"num\",2).over(window_spec) ) & (lead(\"num\",1).over(window_spec) == col(\"num\"))\\\n                    )\nres.filter(\"is_next_two_cols_same = TRUE\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:54.344369Z","iopub.execute_input":"2025-02-04T04:56:54.344785Z","iopub.status.idle":"2025-02-04T04:56:55.119838Z","shell.execute_reply.started":"2025-02-04T04:56:54.344746Z","shell.execute_reply":"2025-02-04T04:56:55.118857Z"}},"outputs":[{"name":"stdout","text":"+---+---+---------------------+\n| id|num|is_next_two_cols_same|\n+---+---+---------------------+\n|  1|  1|                 true|\n+---+---+---------------------+\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Problem 23\n\nWrite a solution to find the prices of all products on 2019-08-16. Assume the price of all products before any change is 10.","metadata":{}},{"cell_type":"code","source":"import pyspark.sql.functions as F\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DateType\n\n# Sample data for the SQL tables\ndata = [\n    (1, 20, \"2019-08-14\"),\n    (2, 50, \"2019-08-14\"),\n    (1, 30, \"2019-08-15\"),\n    (1, 35, \"2019-08-16\"),\n    (2, 65, \"2019-08-17\"),\n    (3, 20, \"2019-08-18\"),\n    (3, 20, \"2019-08-28\")\n]\n\n# Define the schema for the DataFrame\nschema = StructType([\n    StructField(\"product_id\", IntegerType(), True),\n    StructField(\"new_price\", IntegerType(), True),\n    StructField(\"change_date_str\", StringType(), True)\n])\n\n# Create the DataFrame\ndf = spark.createDataFrame(data, schema)\n\ndf = df.withColumn(\"change_date\", F.to_date(F.col(\"change_date_str\"),\"yyyy-MM-dd\")).drop(\"change_date_str\")\n\n# Show the DataFrame again to verify\ndf.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:55.122958Z","iopub.execute_input":"2025-02-04T04:56:55.123321Z","iopub.status.idle":"2025-02-04T04:56:55.877589Z","shell.execute_reply.started":"2025-02-04T04:56:55.123285Z","shell.execute_reply":"2025-02-04T04:56:55.874501Z"}},"outputs":[{"name":"stdout","text":"+----------+---------+-----------+\n|product_id|new_price|change_date|\n+----------+---------+-----------+\n|         1|       20| 2019-08-14|\n|         2|       50| 2019-08-14|\n|         1|       30| 2019-08-15|\n|         1|       35| 2019-08-16|\n|         2|       65| 2019-08-17|\n|         3|       20| 2019-08-18|\n|         3|       20| 2019-08-28|\n+----------+---------+-----------+\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from pyspark.sql.window import Window\nfrom pyspark.sql.functions import col,rank\n\nwindow_spec = Window.partitionBy(\"product_id\").orderBy(col(\"change_date\").desc())\n\ndf_nearest_date =  df.filter(\"change_date <= '2019-08-16'\").withColumn(\"rank_date\",rank().over(window_spec))\\\n        .filter(\"rank_date = 1\")\ndf_nearest_date.show()\n\nres = df_nearest_date.alias(\"df_nearest_date\")\\\n    .join(df.select(\"product_id\").distinct().alias(\"df\"),col(\"df.product_id\")==col(\"df_nearest_date.product_id\"),\"outer\")\n\nres.select(\"df.product_id\",expr(\"coalesce(new_price,10) as price\")).show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:55.879315Z","iopub.execute_input":"2025-02-04T04:56:55.879810Z","iopub.status.idle":"2025-02-04T04:56:58.241458Z","shell.execute_reply.started":"2025-02-04T04:56:55.879767Z","shell.execute_reply":"2025-02-04T04:56:58.239646Z"}},"outputs":[{"name":"stdout","text":"+----------+---------+-----------+---------+\n|product_id|new_price|change_date|rank_date|\n+----------+---------+-----------+---------+\n|         1|       35| 2019-08-16|        1|\n|         2|       50| 2019-08-14|        1|\n+----------+---------+-----------+---------+\n\n+----------+-----+\n|product_id|price|\n+----------+-----+\n|         1|   35|\n|         2|   50|\n|         3|   10|\n+----------+-----+\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Problem 24\n\nThere is a queue of people waiting to board a bus. However, the bus has a weight limit of 1000 kilograms, so there may be some people who cannot board.\n\nWrite a solution to find the person_name of the last person that can fit on the bus without exceeding the weight limit. The test cases are generated such that the first person does not exceed the weight limit.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"QueueTable\").getOrCreate()\n\n# Define schema\nqueue_schema = StructType([\n    StructField(\"person_id\", IntegerType(), False),\n    StructField(\"person_name\", StringType(), False),\n    StructField(\"weight\", IntegerType(), False),\n    StructField(\"turn\", IntegerType(), False)\n])\n\n# Create DataFrame\ndata = [\n    (5, \"Alice\", 250, 1),\n    (4, \"Bob\", 175, 5),\n    (3, \"Alex\", 350, 2),\n    (6, \"John Cena\", 400, 3),\n    (1, \"Winston\", 500, 6),\n    (2, \"Marie\", 200, 4)\n]\n\nqueue_df = spark.createDataFrame(data, schema=queue_schema)\n\n# Show DataFrame\nqueue_df.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:58.242419Z","iopub.execute_input":"2025-02-04T04:56:58.242833Z","iopub.status.idle":"2025-02-04T04:56:58.773231Z","shell.execute_reply.started":"2025-02-04T04:56:58.242796Z","shell.execute_reply":"2025-02-04T04:56:58.771808Z"}},"outputs":[{"name":"stdout","text":"+---------+-----------+------+----+\n|person_id|person_name|weight|turn|\n+---------+-----------+------+----+\n|        5|      Alice|   250|   1|\n|        4|        Bob|   175|   5|\n|        3|       Alex|   350|   2|\n|        6|  John Cena|   400|   3|\n|        1|    Winston|   500|   6|\n|        2|      Marie|   200|   4|\n+---------+-----------+------+----+\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pyspark.sql.window import Window\nfrom pyspark.sql.functions import sum,lit\n\ndfq = queue_df\n\nwindow_spec = Window.orderBy(\"turn\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n\ndfq = dfq.withColumn(\"cumulative_sum\",sum(\"weight\").over(window_spec))\ndfq.show()\n\ndfq = dfq.filter(\"cumulative_sum <= 1000\")\ndfq.show()\n\nmax_turn = dfq.selectExpr(\"max(turn)\")\nmax_turn = max_turn.collect()[0][0]\nprint(max_turn)\n\ndfq.filter(f\"turn = {max_turn}\").select(\"person_name\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:56:58.774059Z","iopub.execute_input":"2025-02-04T04:56:58.774403Z","iopub.status.idle":"2025-02-04T04:57:01.336383Z","shell.execute_reply.started":"2025-02-04T04:56:58.774369Z","shell.execute_reply":"2025-02-04T04:57:01.335323Z"}},"outputs":[{"name":"stdout","text":"+---------+-----------+------+----+--------------+\n|person_id|person_name|weight|turn|cumulative_sum|\n+---------+-----------+------+----+--------------+\n|        5|      Alice|   250|   1|           250|\n|        3|       Alex|   350|   2|           600|\n|        6|  John Cena|   400|   3|          1000|\n|        2|      Marie|   200|   4|          1200|\n|        4|        Bob|   175|   5|          1375|\n|        1|    Winston|   500|   6|          1875|\n+---------+-----------+------+----+--------------+\n\n+---------+-----------+------+----+--------------+\n|person_id|person_name|weight|turn|cumulative_sum|\n+---------+-----------+------+----+--------------+\n|        5|      Alice|   250|   1|           250|\n|        3|       Alex|   350|   2|           600|\n|        6|  John Cena|   400|   3|          1000|\n+---------+-----------+------+----+--------------+\n\n3\n+-----------+\n|person_name|\n+-----------+\n|  John Cena|\n+-----------+\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Problem 25\n\nWrite a solution to swap the seat id of every two consecutive students. If the number of students is odd, the id of the last student is not swapped.","metadata":{}},{"cell_type":"code","source":"# Define schema\nseat_schema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"student\", StringType(), False)\n])\n\n# Create DataFrame\nseat_data = [\n    (1, \"Abbot\"),\n    (2, \"Doris\"),\n    (3, \"Emerson\"),\n    (4, \"Green\"),\n    (5, \"Jeames\")\n]\n\nseat_df = spark.createDataFrame(seat_data, schema=seat_schema)\n\n# Show DataFrame\nseat_df.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:01.337270Z","iopub.execute_input":"2025-02-04T04:57:01.337667Z","iopub.status.idle":"2025-02-04T04:57:01.965075Z","shell.execute_reply.started":"2025-02-04T04:57:01.337632Z","shell.execute_reply":"2025-02-04T04:57:01.963927Z"}},"outputs":[{"name":"stdout","text":"+---+-------+\n| id|student|\n+---+-------+\n|  1|  Abbot|\n|  2|  Doris|\n|  3|Emerson|\n|  4|  Green|\n|  5| Jeames|\n+---+-------+\n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from pyspark.sql.window import Window\nfrom pyspark.sql.functions import col,lag,lead,when,coalesce\n\ndfs = seat_df\n\nwindow_spec = Window.orderBy(\"id\")\n\nres = dfs.withColumn(\"lag\",lag(\"student\").over(window_spec)).withColumn(\"lead\",lead(\"student\").over(window_spec))\nres = res.withColumn(\"swapped_names\",when(col(\"id\")%2==0,col(\"lag\")).otherwise(coalesce(col(\"lead\"),col(\"student\")) ))\nres.select(\"id\",\"swapped_names\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:01.965958Z","iopub.execute_input":"2025-02-04T04:57:01.966345Z","iopub.status.idle":"2025-02-04T04:57:02.641186Z","shell.execute_reply.started":"2025-02-04T04:57:01.966303Z","shell.execute_reply":"2025-02-04T04:57:02.640077Z"}},"outputs":[{"name":"stdout","text":"+---+-------------+\n| id|swapped_names|\n+---+-------------+\n|  1|        Doris|\n|  2|        Abbot|\n|  3|        Green|\n|  4|      Emerson|\n|  5|       Jeames|\n+---+-------------+\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Problem 26\n\nFind the name of the user who has rated the greatest number of movies. In case of a tie, return the lexicographically smaller user name.\n\nFind the movie name with the highest average rating in February 2020. In case of a tie, return the lexicographically smaller movie name.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\nimport datetime\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"SQLToPySpark\").getOrCreate()\n\n# Data for Movies table\nmovies_data = [\n    (1, \"Avengers\"),\n    (2, \"Frozen 2\"),\n    (3, \"Joker\")\n]\nmovies_schema = StructType([\n    StructField(\"movie_id\", IntegerType(), False),\n    StructField(\"title\", StringType(), False)\n])\nmovies_df = spark.createDataFrame(movies_data, schema=movies_schema)\n\n# Data for Users table\nusers_data = [\n    (1, \"Daniel\"),\n    (2, \"Monica\"),\n    (3, \"Maria\"),\n    (4, \"James\")\n]\nusers_schema = StructType([\n    StructField(\"user_id\", IntegerType(), False),\n    StructField(\"name\", StringType(), False)\n])\nusers_df = spark.createDataFrame(users_data, schema=users_schema)\n\n# Data for MovieRating table\nmovie_ratings_data = [\n    (1, 1, 3, datetime.datetime.strptime(\"2020-01-12\", \"%Y-%m-%d\")),\n    (1, 2, 4, datetime.datetime.strptime(\"2020-02-11\", \"%Y-%m-%d\")),\n    (1, 3, 2, datetime.datetime.strptime(\"2020-02-12\", \"%Y-%m-%d\")),\n    (1, 4, 1, datetime.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\")),\n    (2, 1, 5, datetime.datetime.strptime(\"2020-02-17\", \"%Y-%m-%d\")),\n    (2, 2, 2, datetime.datetime.strptime(\"2020-02-01\", \"%Y-%m-%d\")),\n    (2, 3, 2, datetime.datetime.strptime(\"2020-03-01\", \"%Y-%m-%d\")),\n    (3, 1, 3, datetime.datetime.strptime(\"2020-02-22\", \"%Y-%m-%d\")),\n    (3, 2, 4, datetime.datetime.strptime(\"2020-02-25\", \"%Y-%m-%d\"))\n]\nmovie_ratings_schema = StructType([\n    StructField(\"movie_id\", IntegerType(), False),\n    StructField(\"user_id\", IntegerType(), False),\n    StructField(\"rating\", IntegerType(), False),\n    StructField(\"created_at\", DateType(), False)\n])\nmovie_ratings_df = spark.createDataFrame(movie_ratings_data, schema=movie_ratings_schema)\n\n# Show dataframes (optional, can be removed if not needed)\nmovies_df.show()\nusers_df.show()\nmovie_ratings_df.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:02.642382Z","iopub.execute_input":"2025-02-04T04:57:02.643143Z","iopub.status.idle":"2025-02-04T04:57:04.228765Z","shell.execute_reply.started":"2025-02-04T04:57:02.642781Z","shell.execute_reply":"2025-02-04T04:57:04.227213Z"}},"outputs":[{"name":"stdout","text":"+--------+--------+\n|movie_id|   title|\n+--------+--------+\n|       1|Avengers|\n|       2|Frozen 2|\n|       3|   Joker|\n+--------+--------+\n\n+-------+------+\n|user_id|  name|\n+-------+------+\n|      1|Daniel|\n|      2|Monica|\n|      3| Maria|\n|      4| James|\n+-------+------+\n\n+--------+-------+------+----------+\n|movie_id|user_id|rating|created_at|\n+--------+-------+------+----------+\n|       1|      1|     3|2020-01-12|\n|       1|      2|     4|2020-02-11|\n|       1|      3|     2|2020-02-12|\n|       1|      4|     1|2020-01-01|\n|       2|      1|     5|2020-02-17|\n|       2|      2|     2|2020-02-01|\n|       2|      3|     2|2020-03-01|\n|       3|      1|     3|2020-02-22|\n|       3|      2|     4|2020-02-25|\n+--------+-------+------+----------+\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from pyspark.sql.window import Window\nfrom pyspark.sql.functions import rank,month,year,lit,avg\n\ndfm = movies_df\ndfu = users_df\ndfmr = movie_ratings_df\n\ndfmr_1 = dfmr.groupBy(\"user_id\").count()\ndfmr_res = dfmr_1.alias(\"dfmr_1\").join(dfu.alias(\"dfu\"),col(\"dfmr_1.user_id\")==col(\"dfu.user_id\"),\"inner\")\ndfmr_res.select(\"*\").show()\n\nwindow_spec = Window.orderBy(col(\"count\").desc())\ndfmr_res = dfmr_res.withColumn(\"rank\",rank().over(window_spec))\ndfmr_res = dfmr_res.filter(\"rank=1\").orderBy(\"name\").limit(1).select(\"name\")\ndfmr_res.show()\n\nwindow_spec = Window.orderBy(col(\"avg_rating\").desc())\n\n## Highest rated movie for the month of Feb, 2020\ndfmr_2 = dfmr.filter((month(\"created_at\")==2) & (year(\"created_at\")==2020) )\n# dfmr_2.show()\ndfmr_2 = dfmr_2.groupBy(\"movie_id\").agg(avg(\"rating\").alias(\"avg_rating\"))\ndfmr_2 = dfmr_2.alias(\"dfmr_2\").join(dfm.alias(\"dfm\"),col(\"dfm.movie_id\")==col(\"dfmr_2.movie_id\"),\"inner\")\ndfmr_2 = dfmr_2.withColumn(\"rank\",rank().over(window_spec))\ndfmr_2 = dfmr_2.filter(\"rank=1\").orderBy(\"title\").limit(1).select(\"title\")\ndfmr_2.show()\n\ndfmr_res.union(dfmr_2).show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:04.229686Z","iopub.execute_input":"2025-02-04T04:57:04.230091Z","iopub.status.idle":"2025-02-04T04:57:10.323801Z","shell.execute_reply.started":"2025-02-04T04:57:04.230052Z","shell.execute_reply":"2025-02-04T04:57:10.321827Z"}},"outputs":[{"name":"stdout","text":"+-------+-----+-------+------+\n|user_id|count|user_id|  name|\n+-------+-----+-------+------+\n|      1|    3|      1|Daniel|\n|      2|    3|      2|Monica|\n|      3|    2|      3| Maria|\n|      4|    1|      4| James|\n+-------+-----+-------+------+\n\n+------+\n|  name|\n+------+\n|Daniel|\n+------+\n\n+--------+\n|   title|\n+--------+\n|Frozen 2|\n+--------+\n\n+--------+\n|    name|\n+--------+\n|  Daniel|\n|Frozen 2|\n+--------+\n\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Problem 27","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DateType\nfrom datetime import datetime\n\n\n# Define schema\nschema = StructType([\n    StructField(\"requester_id\", IntegerType(), True),\n    StructField(\"accepter_id\", IntegerType(), True),\n    StructField(\"accept_date\", DateType(), True)\n])\n\n# Create data\ndata = [\n    (1, 2, datetime.strptime(\"2016/06/03\", \"%Y/%m/%d\")),\n    (1, 3, datetime.strptime(\"2016/06/08\", \"%Y/%m/%d\")),\n    (2, 3, datetime.strptime(\"2016/06/08\", \"%Y/%m/%d\")),\n    (3, 4, datetime.strptime(\"2016/06/09\", \"%Y/%m/%d\"))\n]\n\n# Create DataFrame\ndf = spark.createDataFrame(data, schema=schema)\n\n# Show DataFrame\ndf.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:10.325179Z","iopub.execute_input":"2025-02-04T04:57:10.325625Z","iopub.status.idle":"2025-02-04T04:57:10.776051Z","shell.execute_reply.started":"2025-02-04T04:57:10.325587Z","shell.execute_reply":"2025-02-04T04:57:10.774959Z"}},"outputs":[{"name":"stdout","text":"+------------+-----------+-----------+\n|requester_id|accepter_id|accept_date|\n+------------+-----------+-----------+\n|           1|          2| 2016-06-03|\n|           1|          3| 2016-06-08|\n|           2|          3| 2016-06-08|\n|           3|          4| 2016-06-09|\n+------------+-----------+-----------+\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from pyspark.sql.functions import coalesce,expr\nfrom pyspark.sql.window import Window\n\ndf_r = df.groupBy(\"requester_id\").count()\ndf_r.show()\n\ndf_a = df.groupBy(\"accepter_id\").count()\ndf_a.show()\n\ndf_res = df_r.alias(\"df_r\").join(df_a.alias(\"df_a\"),col(\"requester_id\")==col(\"accepter_id\"),\"outer\")\ndf_res.show()\n\ndf_res = df_res.withColumn(\"user_id\",coalesce(\"df_r.requester_id\",\"df_a.accepter_id\"))\\\n                .withColumn(\"total_friends\",expr(\"coalesce(df_r.count,0)+coalesce(df_a.count,0)\"))\ndf_res.show()\n\ndf_res = df_res.select(\"user_id\",\"total_friends\")\ndf_res.show()\n\nwindow_spec = Window.orderBy(col(\"total_friends\").desc())\n\ndf_res.withColumn(\"rank\",rank().over(window_spec)).filter(\"rank = 1\").show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:10.776970Z","iopub.execute_input":"2025-02-04T04:57:10.777366Z","iopub.status.idle":"2025-02-04T04:57:15.661418Z","shell.execute_reply.started":"2025-02-04T04:57:10.777331Z","shell.execute_reply":"2025-02-04T04:57:15.659639Z"}},"outputs":[{"name":"stdout","text":"+------------+-----+\n|requester_id|count|\n+------------+-----+\n|           1|    2|\n|           2|    1|\n|           3|    1|\n+------------+-----+\n\n+-----------+-----+\n|accepter_id|count|\n+-----------+-----+\n|          2|    1|\n|          3|    2|\n|          4|    1|\n+-----------+-----+\n\n+------------+-----+-----------+-----+\n|requester_id|count|accepter_id|count|\n+------------+-----+-----------+-----+\n|           1|    2|       NULL| NULL|\n|           2|    1|          2|    1|\n|           3|    1|          3|    2|\n|        NULL| NULL|          4|    1|\n+------------+-----+-----------+-----+\n\n+------------+-----+-----------+-----+-------+-------------+\n|requester_id|count|accepter_id|count|user_id|total_friends|\n+------------+-----+-----------+-----+-------+-------------+\n|           1|    2|       NULL| NULL|      1|            2|\n|           2|    1|          2|    1|      2|            2|\n|           3|    1|          3|    2|      3|            3|\n|        NULL| NULL|          4|    1|      4|            1|\n+------------+-----+-----------+-----+-------+-------------+\n\n+-------+-------------+\n|user_id|total_friends|\n+-------+-------------+\n|      1|            2|\n|      2|            2|\n|      3|            3|\n|      4|            1|\n+-------+-------------+\n\n+-------+-------------+----+\n|user_id|total_friends|rank|\n+-------+-------------+----+\n|      3|            3|   1|\n+-------+-------------+----+\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Problem 28\n\nA company's executives are interested in seeing who earns the most money in each of the company's departments. A high earner in a department is an employee who has a salary in the top three unique salaries for that department.\n\nWrite a solution to find the employees who are high earners in each of the departments.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType\n\n# Initialize Spark Session\nspark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n\n# Define schemas\nemployee_schema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"name\", StringType(), False),\n    StructField(\"salary\", IntegerType(), False),\n    StructField(\"departmentId\", IntegerType(), False)\n])\n\ndepartment_schema = StructType([\n    StructField(\"id\", IntegerType(), False),\n    StructField(\"name\", StringType(), False)\n])\n\n# Create DataFrames\nemployee_data = [\n    (1, \"Joe\", 85000, 1),\n    (2, \"Henry\", 80000, 2),\n    (3, \"Sam\", 60000, 2),\n    (4, \"Max\", 90000, 1),\n    (5, \"Janet\", 69000, 1),\n    (6, \"Randy\", 85000, 1),\n    (7, \"Will\", 70000, 1)\n]\n\ndepartment_data = [\n    (1, \"IT\"),\n    (2, \"Sales\")\n]\n\nemployee_df = spark.createDataFrame(employee_data, schema=employee_schema)\ndepartment_df = spark.createDataFrame(department_data, schema=department_schema)\n\n# Show DataFrames\nemployee_df.show()\ndepartment_df.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:15.664142Z","iopub.execute_input":"2025-02-04T04:57:15.665840Z","iopub.status.idle":"2025-02-04T04:57:16.686478Z","shell.execute_reply.started":"2025-02-04T04:57:15.665784Z","shell.execute_reply":"2025-02-04T04:57:16.685255Z"}},"outputs":[{"name":"stdout","text":"+---+-----+------+------------+\n| id| name|salary|departmentId|\n+---+-----+------+------------+\n|  1|  Joe| 85000|           1|\n|  2|Henry| 80000|           2|\n|  3|  Sam| 60000|           2|\n|  4|  Max| 90000|           1|\n|  5|Janet| 69000|           1|\n|  6|Randy| 85000|           1|\n|  7| Will| 70000|           1|\n+---+-----+------+------------+\n\n+---+-----+\n| id| name|\n+---+-----+\n|  1|   IT|\n|  2|Sales|\n+---+-----+\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from pyspark.sql.functions import dense_rank\nfrom pyspark.sql.window import Window\n\ndfe = employee_df\ndfd = department_df\n\nwindow_spec = Window.partitionBy(\"departmentId\").orderBy(col(\"salary\").desc())\n\ndfe_1 = dfe.withColumn(\"dense_rank\",dense_rank().over(window_spec))\ndfe_1.show()\n\ndfe_1  = dfe_1.filter(\"dense_rank <= 3\")\ndfe_1 = dfe_1.alias(\"dfe_1\").join(dfd.alias(\"dfd\"),col(\"dfd.id\")==col(\"dfe_1.departmentId\"),\"inner\")\ndfe_1.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:16.687341Z","iopub.execute_input":"2025-02-04T04:57:16.687781Z","iopub.status.idle":"2025-02-04T04:57:18.170662Z","shell.execute_reply.started":"2025-02-04T04:57:16.687741Z","shell.execute_reply":"2025-02-04T04:57:18.169526Z"}},"outputs":[{"name":"stdout","text":"+---+-----+------+------------+----------+\n| id| name|salary|departmentId|dense_rank|\n+---+-----+------+------------+----------+\n|  4|  Max| 90000|           1|         1|\n|  1|  Joe| 85000|           1|         2|\n|  6|Randy| 85000|           1|         2|\n|  7| Will| 70000|           1|         3|\n|  5|Janet| 69000|           1|         4|\n|  2|Henry| 80000|           2|         1|\n|  3|  Sam| 60000|           2|         2|\n+---+-----+------+------------+----------+\n\n+---+-----+------+------------+----------+---+-----+\n| id| name|salary|departmentId|dense_rank| id| name|\n+---+-----+------+------------+----------+---+-----+\n|  7| Will| 70000|           1|         3|  1|   IT|\n|  6|Randy| 85000|           1|         2|  1|   IT|\n|  1|  Joe| 85000|           1|         2|  1|   IT|\n|  4|  Max| 90000|           1|         1|  1|   IT|\n|  3|  Sam| 60000|           2|         2|  2|Sales|\n|  2|Henry| 80000|           2|         1|  2|Sales|\n+---+-----+------+------------+----------+---+-----+\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Problem 29\n\nWrite a solution to report the sum of all total investment values in 2016 tiv_2016, for all policyholders who:\n\ni. have the same tiv_2015 value as one or more other policyholders, and\n\nii. are not located in the same city as any other policyholder (i.e., the (lat, lon) attribute pairs must be unique).","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"InsuranceTable\").getOrCreate()\n\n# Define schema\nschema = StructType([\n    StructField(\"pid\", IntegerType(), True),\n    StructField(\"tiv_2015\", DoubleType(), True),\n    StructField(\"tiv_2016\", DoubleType(), True),\n    StructField(\"lat\", DoubleType(), True),\n    StructField(\"lon\", DoubleType(), True)\n])\n\n# Create data\ndata = [\n    (1, 10.0, 5.0, 10.0, 10.0),\n    (2, 20.0, 20.0, 20.0, 20.0),\n    (3, 10.0, 30.0, 20.0, 20.0),\n    (4, 10.0, 40.0, 40.0, 40.0)\n]\n\n# Create DataFrame\ndf = spark.createDataFrame(data, schema)\n\n# Show DataFrame\ndf.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T04:57:18.171475Z","iopub.execute_input":"2025-02-04T04:57:18.171834Z","iopub.status.idle":"2025-02-04T04:57:18.640286Z","shell.execute_reply.started":"2025-02-04T04:57:18.171800Z","shell.execute_reply":"2025-02-04T04:57:18.639127Z"}},"outputs":[{"name":"stdout","text":"+---+--------+--------+----+----+\n|pid|tiv_2015|tiv_2016| lat| lon|\n+---+--------+--------+----+----+\n|  1|    10.0|     5.0|10.0|10.0|\n|  2|    20.0|    20.0|20.0|20.0|\n|  3|    10.0|    30.0|20.0|20.0|\n|  4|    10.0|    40.0|40.0|40.0|\n+---+--------+--------+----+----+\n\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"df_1 = df.groupBy(\"lat\",\"lon\").agg(count(\"*\").alias(\"lat_long_unique\"))\ndf_1.show()\n\ndf_1 = df_1.filter(\"lat_long_unique = 1\")\ndf_1.show()\n\ndf_2 = df.groupBy(\"tiv_2015\").agg(count(\"*\").alias(\"tiv_2015_count\"))\ndf_2 = df_2.filter(\"tiv_2015_count>1\")\ndf_2.show()\n\ndf_3 = df.alias(\"df\").join(df_1.alias(\"df_1\")\\\n                           ,( col(\"df_1.lat\")==col(\"df.lat\")) & (col(\"df_1.lon\")==col(\"df.lon\")) \\\n                          ,\"left\")\\\n                    .join(df_2.alias(\"df_2\")\\\n                          ,(col(\"df_2.tiv_2015\")==col(\"df.tiv_2015\"))\\\n                         ,\"left\")\n\ndf_3 = df_3.filter(\"tiv_2015_count>1 AND lat_long_unique = 1\")\ndf_3 = df_3.agg(sum(\"tiv_2016\").alias(\"tiv_2016_sum\"))\ndf_3.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T05:33:23.118607Z","iopub.execute_input":"2025-02-04T05:33:23.118996Z","iopub.status.idle":"2025-02-04T05:33:25.826466Z","shell.execute_reply.started":"2025-02-04T05:33:23.118966Z","shell.execute_reply":"2025-02-04T05:33:25.825219Z"}},"outputs":[{"name":"stdout","text":"+----+----+---------------+\n| lat| lon|lat_long_unique|\n+----+----+---------------+\n|10.0|10.0|              1|\n|20.0|20.0|              2|\n|40.0|40.0|              1|\n+----+----+---------------+\n\n+----+----+---------------+\n| lat| lon|lat_long_unique|\n+----+----+---------------+\n|10.0|10.0|              1|\n|40.0|40.0|              1|\n+----+----+---------------+\n\n+--------+--------------+\n|tiv_2015|tiv_2015_count|\n+--------+--------------+\n|    10.0|             3|\n+--------+--------------+\n\n+------------+\n|tiv_2016_sum|\n+------------+\n|        45.0|\n+------------+\n\n","output_type":"stream"}],"execution_count":38}]}